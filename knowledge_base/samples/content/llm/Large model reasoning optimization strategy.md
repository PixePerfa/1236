# Large model inference optimization strategy
- 7.1 Video Memory Optimization
    -  [PagedAttention]("https://zhuanlan.zhihu.com/p/638468472")
        - KV cache, which has the following characteristics:1. The video memory occupies a large amount, and the 14b level model requires about 0.7M-1M video memory for each token;2. Dynamics: The size of the KV cache depends on the sequence length, which is highly variable and unpredictable. As a result, this is a challenge for effectively managing KV cache. The study found that existing systems wasted 60% - 80% of video memory due to fragmentation and excessive retention.
        - To solve this problem, the study introduced PagedAttention, an attention algorithm inspired by the classical ideas of virtual memory and pagination in operating systems. Unlike traditional attention algorithms, PagedAttention allows sequential keys and values to be stored in non-contiguous memory space. Specifically, PagedAttention divides the KV cache for each sequence into chunks, each containing a fixed number of tokens and a key. During attention computation, the PagedAttention kernel can efficiently identify and fetch these blocks. Because blocks don't need to be contiguous in memory, keys and values can be managed in a much more flexible way, just like in the operating system's virtual memory: blocks can be thought of as pages, tokens as bytes, and sequences as processes. The sequential logical blocks of a sequence are mapped into non-contiguous physical blocks by a block table. Physical blocks are allocated on demand when new tokens are generated. In PagedAttention, memory waste only occurs in the last block of the sequence. This makes it possible to achieve near-optimal memory usage in practice, with less than 4% wasted.
        - PagedAttention has another key advantage – efficient memory sharing. For example, in parallel sampling, multiple output sequences are generated by the same prompt. In this case, the computation and memory of the prompt can be shared across the output sequence. PagedAttention naturally initiates memory sharing through its block table. Similar to how processes share physical pages, different sequences in PagedAttention can share blocks by mapping their logical blocks to the same physical block. To ensure secure sharing, PagedAttention keeps track of the reference count of physical blocks and implements a copy-on-write mechanism. PageAttention's memory sharing dramatically reduces the memory overhead of complex sampling algorithms, such as parallel sampling and cluster search, which reduces memory usage by 55%. This translates into up to 2.2x higher throughput.
        - continuous batching
        - CUDA kernel optimization
    - Qunatized KV Cache
    - MQA/GQA
        - The core idea is to retrieve the number of kv-caches, and a small number of kv-caches correspond to multiple query ![images](./img/大模型推理优化策略-幕布图片-699343-219844.jpg)
        -  ![Image](./img/大模型推理优化策略-幕布图片-930255-616209.jpg)
    -  [FlashAttention]("https://zhuanlan.zhihu.com/p/638468472")
        - Explanation 1: I is the computational intensity I of the model, and the unit FLOP/byte represents the operands that the model can achieve by data interaction in unit bytes, then the *I-bandwidth beta is the computational performance of the model, and the unit is FLOP/s. Let I_max = computing platform computing power/computing platform bandwidth, when the computing intensity I of the model is less than the theoretical computing intensity of the platform I_max, the computing power of the model P is I*beta, when the computing intensity of the model is greater than I_max, the computing performance P of the model is equal to the computing power of the platform. Therefore, if the computational intensity of the model is small, the bottleneck is bandwidth, and if the computational intensity of the model is large, the bottleneck is computing power. To improve computing performance, you need to increase the computing intensity, that is, the number of operations per byte of data interaction. 
        - Explanation 2: Write N = FLOP required per operation, unit FLOP/OP; pi = computing power of the platform, unit: FLOP/s; beta=memory bandwidth, in bytes/s; P = actual calculation speed, in FLOP/s; The optimization goal is O=P/N, the number of operations per second, and the unit is OP/s. Since N is fixed, the optimization goal is P,P=min{beta_r*I_max=beta_r*pi/beta,pi}, so the optimization goal is changed to beta, that is, the memory access policy is changed to maximize the beta. 
        - In the attention operation, the computational space complexity of S and P is O(N^2), and scale, mask, softmax, and dropout are bandwidth constrained operations. ![Pictures](./img/大模型推理优化策略-幕布图片-380552-579242.jpg)
        -  ![Image](./img/大模型推理优化策略-幕布图片-789705-122117.jpg)
        - It can be seen that the matrix calculation of O(N^2) space complexity is the main memory bottleneck for HBM, so the main optimization points are:1. Calculate softmax without accessing the entire input; 2. Do not store large intermediate attention matrices for backpropagation. FlashAttention proposes two ways to solve the above problem step by step: tiling, recomputation.tiling - Attention computation is reconstructed to split the input into chunks and incrementally perform softmax operations by passing multiple times on the input block. recomputation - Stores the softmax normalization factor from the forward direction in order to quickly recalculate the on-chip attention in the inverse, which is faster than the standard attention method of reading the intermediate matrix from HBM. This does result in an increase in FLOPs due to the recalculation, but FlashAttention runs faster due to a massive reduction in HBM access. The main idea behind the algorithm is to split the inputs, load them from the slow HBM to the fast SRAM, and then calculate the attention output of those blocks. Before adding up the output of each block, scale it by the correct normalization factor to get the correct result.
        -  ![Image](./img/大模型推理优化策略-幕布图片-590671-36787.jpg)
        -  ![Image](./img/大模型推理优化策略-幕布图片-276446-401476.jpg)
    - References
        -  [Inference Optimization]("https://zhuanlan.zhihu.com/p/656485997") [Inference optimization]("https://zhuanlan.zhihu.com/p/656485997")
- 7.2 Operator fusion
- 7.3 Latency Optimization
    - No Padding optimization
- 7.4 Scheduling Optimization
    - Dynamic Batching
        - The batch size is fixed and cannot dynamically change with the load of computing resources, resulting in low GPU resource utilization
        - This is achieved by maintaining a job queue to dynamically insert new sequences in the batch dimension
    - Async Servering
        - The Tokenize / Detokenize process is executed on the CPU while the GPU is idle
        - Multi-threaded asynchronous and pipelined overlap to reduce latency
    - Inflight Batching/continuous batching
        - When inferring the same batch of sequences, there are "bubbles", resulting in low GPU resource utilization
        - Refinement from batch-granular scheduling to step-level scheduling, dynamically inserting new sequences in the timeline direction
- 7.5 Quantification
    - GPTQ
    - AWQ
    - SmoothQuant
    - SpQR
- 7.6 Model Parallelism
    - Tensor Paralellism Acceleration + Memory Reduction
    - Pipeline paralellism reduces video memory
- 7.7 Request Optimization
    -  [Network Communications]("https://article.juejin.cn/post/7226374741064892453")
        - rpc
        - grpc
        - http
    -  [Responsive mode]("https://blog.csdn.net/weixin_44999716/article/details/128436984")
        - SSE
        - Polling
        - Long polling comet
        - WebSocket
- 7.8 Sampling/Decoding
    - speculative decoding
        -  ![Image](./img/大模型推理优化策略-幕布图片-923924-83386.jpg)
    - Blockwise *Parallel*  *Decoding*
    - Medusa
    - SOT - Parallel Decoding
    - SpecInfer
    - StreamingLLM
- Resources
    -  [Best Practices]("https://mp.weixin.qq.com/s/nJLrE9Dzj1mmTeQDiqNdzA")