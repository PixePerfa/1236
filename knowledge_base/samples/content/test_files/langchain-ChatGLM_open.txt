{"title": "How to optimize the effect", "file": "2023-04-04.00", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/14", "detail": "As shown in the figure, after combining the README.md of the project and the project, the answer effect is not ideal, What can be optimized", "id": 0}
{"title": "How to make the model answer strictly based on the retrieved data, and reduce the nonsense answers", "file": "2023-04-04.00", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/15", "detail": "Example:", "id": 1}
{"title": "When I try to run the `python knowledge_based_chatglm.py`, I got this error in macOS(M1 Max, OS 13.2)", "file": "2023-04-07.00", "url":  "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/32", "detail": "```python", "id": 2}
{"title": "How do you change to an AMD graphics card or CPU?"} , "file": "2023-04-10.00", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/48", "detail": "Just remove .cuda()", "id": 3}
{"title": "The output of the answer is very long, can the text vectorized part be stored in advance?"} , "file": "2023-04-10.00", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/50", "detail": "GPU:4090 24G VRAM", "id": 4}
{"title": "Use 'repo_type' argument if needed.", "file": "2023-04-11.00", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/57", "detail": "Traceback (most.") recent call last):", "id": 5}
{"title": "Unable to open gradio's page", "file": "2023-04-11.00", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/58", "detail": "$python webui.py", "id": 6}
{"title": "Support word, do the pictures in word display normally?"} , "file": "2023-04-12.00", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/60", "detail": "As the title suggests, I just transferred from next door, I want to know first", "id": 7}
{"title": "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.", "file": "2023-04-12.00", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/63", "detail": "Able to run normally, When loading a file in the content folder, each file is loaded with a prompt: ", "id": 8}
{"title": "Error reported when running webui on CPU, error reported when step3 asking", "file": "2023-04-12.00", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/66", "detail": "When the web is running, the file is loading normally, an error is reported when asking", " id": 9}
{"title": "It is recommended to get a plug-in system", "file": "2023-04-13.00", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/67", "detail": "As the title says, it can be installed with a plug-in in stable-diffusion-webui, Open another repository for users or plugin development, store or download plugins. ", "id": 10}
{"title": "Error loading model!?"} , "file": "2023-04-13.00", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/75", "detail": "AttributeError: module 'transformers_ modules.chatglm-6b.configuration_chatglm' has no attribute 'ChatGLMConfig how to solve it", "id": 11}
{"title": "When retrieving content from local knowledge, is it possible to set a similarity threshold, and content less than this threshold will not be returned, even if it will be less than the set VECTOR_SEARCH_TOP_K parameters?"} Thank you, "file": "2023-04-13.00", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/76", "detail": "e.g. ask some questions that are not related to the local knowledge base", "id": 12}
{"title": "How do I change it to Doka reasoning?"} , "file": "2023-04-13.00", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/77", "detail": "+1", "id": 13}
{"title": "Can you get a lazy bag that you can try with one click?"} , "file": "2023-04-13.00", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/78", "detail": "Can you get a lazy bag and experience it with one click?" , "id": 14}
{"title": "Asking questions continuously will cause a crash", "file": "2023-04-13.00", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/79", "detail": "It doesn't look like a memory explosion problem, after asking questions continuously, the following error will appear", "id": 15}
{"title": "AttributeError: 'NoneType' object has no attribute 'as_retriever'", "file": "2023-04-14.00", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/86", " detail": "Environment: Windows 11, Anaconda/Python 3.8", "ID": 16}
{"title": "FileNotFoundError: Could not find module 'nvcuda.dll' (or one of its dependencies). Try using the full path with constructor syntax.", "file": "2023-04-14.00", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/87", "detail": " Please check if there is an installation problem with CUDA or CUDNN", "id": 17}
{"title": "Failed to load txt file?"} , "file": "2023-04-14.00", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/89", "detail": "! [JppHrGOWFa] (https://user-images.githubusercontent.com/109277248/232009383-bf7c46d1-a01e-4e0a-9de6-5b5ed3e36158.jpg)", "id": 18}
{"title": "NameError: name 'chatglm' is not defined", "file": "2023-04-14.00", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/90", "detail": "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces", "id": 19}
{"title": "Can't open an address?"} , "file": "2023-04-14.00", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/91", "detail": "The error data is as follows:", "id": 20}
{"title": "Error loading MD file", "file": "2023-04-14.00", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/98", "detail": "I can access the page after running webui.py, and there is an error in the log after uploading an MD file. After waiting, the loading can be completed, and the prompt is ready to ask a question, but the question is not answered, and there is an error in the log. The specific logs are as follows. ", "id": 21}
{"title": "Suggestion to increase the ability to acquire online knowledge", "file": "2023-04-15.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/101", "detail": "Suggestion to increase the ability to acquire online knowledge", "id": 22}
{"title": "txt failed to load", "file": "2023-04-15.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/103", "detail": "hinese. Creating a new one with MEAN pooling.", "id": 23}
{"title": "PDF load failed", "file": "2023-04-15.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/105", "detail": "e:\\a.txt loaded successfully, e:\\ a.pdf loading failed, the first few pages of the pdf file are pictures, and the back are text, and there are no more errors reported when the loading fails, how to troubleshoot? ", "id": 24}
{"title": "Stops at Text Load", "file": "2023-04-15.01", "URL": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/108", "detail": "Stops at Text Load", "id": 25}
{"title": " File \"/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/modeling_chatglm.py\", line 440, in forward     new_tensor_shape = mixed_raw_layer.size()[:-1] + ( TypeError: torch. Size() takes an iterable of 'int' (item 2 is 'float')", "file": "2023-04-17.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/113", "detail": "follow the latest code, discover", "id": 26}
{"title": "Will there be headless functionality in the future?"} , "file": "2023-04-17.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/114", "detail": " Something like this https://github.com/lm-sys/FastChat/tree/main/fastchat/serve", "id": 27}
{"title": "Installation dependency error", "file": "2023-04-17.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/115", "detail": "(test) C:\\Users\\linh\\Desktop\\ langchain-ChatGLM-master>pip install -r requirements.txt", "id": 28}
{"title": "Asking a specific question will cause the video memory to explode", "file": "2023-04-17.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/117", "detail": "It's okay to ask a question normally."} , "id": 29}
{"title": "Expecting value: line 1 column 1 (char 0)", "file": "2023-04-17.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/118", "detail": "After running The first step of loading configuration keeps getting an error:", "id": 30}
{"title": "Embedding https://huggingface.co/GanymedeNil/text2vec-large-chinese/tree/main is free, how does it compare to OpenAI?} , "file": "2023-04-17.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/119", "detail": "------------------------------------------------------------------------------- ", "id": 31}
{"title": "What is this bug, running on Colab."} , "file": "2023-04-17.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/120", "detail": "libcuda.so.1: cannot open shared object file: No such file or directory" , "id": 32}
{"title": "I just want to have a conversation with my LoRa fine-tuned model, and I don't want to load any local documents, how do I adjust that?"} , "file": "2023-04-18.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/121", "detail": "Can you make a separate tutorial", "id": 33}
{"title": "Running on local URL: http://0.0.0.0:7860 To create a public link, set 'share=True' in 'launch()'.  I can't access it on my browser??? ", "file": "2023-04-18.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/122", "detail": "(chatglm20230401) root@autodl-container-e82d11963c-10ece0d7: ~/autodl-tmp/chatglm/langchain-ChatGLM-20230418# python3.9 webui.py", "id": 34}
{"title": "Error report in local deployment", "file": "2023-04-18.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/124", "detail": "Hello, in the process of running langchain-chatGLM locally, The environment and the dependent packages have met the conditions, but the error is as follows when running webui.py (similar to the error report cli_demo.py running), what is wrong? Looking forward to your reply, thank you! ", "id": 35}
{"title": "Error message."} The dtype of attention mask (torch.int64) is not bool", "file": "2023-04-18.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/131", "detail": "The dtype of attention mask (torch.int64) is not bool", "id": 36}
{"title": "[HELP] pip install -r requirements.txt I get the following error... Is there a big guy to help see how to do it, the package in the next release", "file": "2023-04-18.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/134", "detail": "$pip install -r requirements.txt", "id": 37}
{"title": "How to improve the accuracy of searching for corresponding knowledge based on the question", "file": "2023-04-19.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/136", "detail": "The biggest problem with the external link knowledge base is that the problem is short text, and knowledge is medium and long text. How to accurately search for the corresponding knowledge according to the problem is the biggest problem. This kind of localization project is not like Baidu, which consists of countless web pages, and basically every question can find the corresponding page. ", "id": 38}
{"title": "Is it possible to increase the threshold setting of vector recall, some recall content is too relevant, causing the model to babble", "file": "2023-04-20.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/140", "detail": "as title", "id": 39}
{"title": "Input length issue", "file": "2023-04-20.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/141", "detail": "Thanks to the author for supporting the ptuning fine-tuning model."} , "id": 40}
{"title": "How do I access chatGLM-6b through an interface?"} "file": "2023-04-20.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/144", "detail": "How to access chatGLM-6b through an interface instead of reloading a model;" , "id": 41}
{"title": "After executing the web_demo.py, Killed is displayed, and it exits, is it not configured?"} , "file": "2023-04-20.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/146", "detail": "! [Picture] (https://user-images.githubusercontent.com/26102866/233256425-c7aab999-11d7-4de9-867b-23ef18d519e4.png)", "id": 42}
{"title": "Execute python cli_demo1.py", "file": "2023-04-20.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/147", "detail": "Traceback (most recent call last):" , "id": 43}
{"title": "Error: ImportError: cannot import name 'GENERATION_CONFIG_NAME' from 'transformers.utils'", "file": "2023-04-20.01", "url": " https://github.com/imClumsyPanda/langchain-ChatGLM/issues/149", "detail": "(mychatGLM) PS D:\\Users\\admin3\\zrh\\langchain-ChatGLM> python cli_demo.py", "id": 44}
{"title": "Temporary files keep appearing when uploading files and loading the knowledge base", "file": "2023-04-21.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/153", "detail": "Environment: Ubuntu 18.04", "id": 45}
{"title": "After adding a file to the knowledge base, click Upload File and Load Knowledge Base", a Segmentation fault error is reported. ", "file": "2023-04-23.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/161", "detail": "The prompt after running the service is as follows:", "id": 46}
{"title": "langchain-serve integration", "file": "2023-04-24.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/162", "detail": "Hey I'm from [langchain-serve] (https://github.com/jina-ai/langchain-serve) dev! ", "id": 47}
{"title": "Guys, how to configure UBUNTU of WSL to accelerate with CUDA, and found that the CPU is running after installing and running", "file": "2023-04-24.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/164", "detail": "Bigwigs, How to configure UBUNTU of WSL to accelerate with CUDA, I found that the CPU was running after installing it", "ID": 48}
{"title": "Docker ran error in github codespaces", "file": "2023-04-24.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/165", "detail": "docker run -d --restart=."} always --name chatglm -p 7860:7860 -v /www/wwwroot/code/langchain-ChatGLM:/chatGLM  chatglm", "id": 49}
{"title": "There are plans to access the Moss model", "file": "2023-04-24.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/166", "detail": "Tests will be carried out in the future, and some effects of langchain are currently being optimized. If you are interested, please feel free to submit a PR", "id": 50}
{"title": "How do I deploy an API?"} , "file": "2023-04-24.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/168", "detail": "Use fastapi to implement API deployment methods, how to implement it, is there any way to explain it?" , "id": 51}
{"title": " 'NoneType' object has no attribute 'message_types_by_name' error", "file": "2023-04-24.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/169", " detail": "_HISTOGRAMPROTO = DESCRIPTOR.message_types_by_name['HistogramProto']", "id": 52}
{"title": "Can I specify the text2vector model I trained?"} , "file": "2023-04-25.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/172", "detail": "Excuse me:", "id": 53}
{"title": "Questions about the models supported by the project and the potential impact of quantization_bit", "file": "2023-04-26.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/176", "detail": "Hello Author~", "id": 54}
{"title": "Run python3.9 api.py WARNING: You must pass the application as an import string to enable 'reload' or 'workers'.", "file": "2023-04-26.01", "url":  "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/179", "detail": "api.py try to change the bottom of the file to this:", "id": 55}
{"title": "ValidationError: 1 validation error for HuggingFaceEmbeddings model_kwargs   extra fields not permitted (type=value_error.extra)", "file": "2023-04-26.01", "url":  "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/180", "detail": "ValidationError: 1 validation error for HuggingFaceEmbeddings", "id": 56}
{"title": "If no high relevance is retrieved, answer "I don't know"", "file": "2023-04-26.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/181", "detail": "If you system_template by design, Have the model answer "I don't know" if none of the documents searched are relevant", "id": 57}
{"title": "If I can't connect to the Internet, where do I put files like 6B from the local upload", "file": "2023-04-26.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/182", "detail": "Thank you for the project, it's very inspiring~", "id": 58}
{"title": "Q&A of the knowledge base--If the new name of the knowledge base is Chinese, an error will be reported", "file": "2023-04-27.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/184", "detail": "Q&A of the knowledge base--Enter the new knowledge base name is Chinese, An error will be reported, and the previously added knowledge base will not be displayed if you select the knowledge base to be loaded", "id": 59}
{"title": "Is it now possible to return a paragraph directly from a document without going through the model by the similarity value that the question matches?"} Because some answers are in the document, the model answers by itself, and cannot answer the answers in the document", "file": "2023-04-27.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/186", "detail": "You can now directly return the text in the document by the similarity value matched by the question, And without going through the model? Because some of the answers are in the document, the model answers on its own, and cannot answer the answers in the document. That is to say, a strategy combining vector retrieval answers + model answers is provided. If the similarity value is higher than a certain value, the text in the document will be returned directly, and the answer to the model or not known if it is not higher than the value", "id": 60}
{"title": "TypeError: The type of ChatGLM.callback_manager differs from the new default value; if you wish to change the type of this field, please use a type annotation", "file": "2023-04-27.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/188", "detail": " Mac running python3 ./webui.py reports TypeError: The type of ChatGLM.callback_manager differs from the new default value; if you wish to change the type of this field, please use a type annotation", "id": 61}
{"title": "Not Enough Memory", "file": "2023-04-27.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/190", "detail": "Run the command line program python cli_demo.py, the pdf file has been loaded successfully."} , "DefaultCPUAllocator: not enough memory: you tried to allocate 458288380900 bytes" error is displayed, where can I configure default memory", "id": 62}
{"title": "Participate in the development issue", "file": "2023-04-27.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/191", "detail": "1.Do you need to join a special development group", "id": 63}
{"title": "The format of the code snippet in the dialog box needs to be improved", "file": "2023-04-27.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/192", "detail": "It would be better to improve the format of the output code snippet, the current output format is not friendly."} , "id": 64}
{"title": "Is it possible to support Belle in the future", "file": "2023-04-28.01", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/195", "detail": "As the title suggests, thank you", "id": 65}
{"title": "TypeError: cannot unpack non-iterable NoneType object", "file": "2023-04-28.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/200", "detail": " When i tried to change the knowledge vector store through `init_knowledge_vector_store`, the error `TypeError: cannot unpack non-iterable NoneType object` came out.", "id": 66}
{"title": "Generate result", "file": "2023-04-28.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/202", "detail": "Hello, I want to ask langchain+chatglm-6B to find a similar matching prompt, Does it directly return the answer information corresponding to the prompt, or does chatglm-6B optimize the answer by itself on this basis? ", "id": 67}
{"title": "I get this error under win, ubuntu: attributeerror: 't5forconditionalgeneration' object has no attribute 'stream_chat'", "file": "2023-04-29.02", "url":  "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/207", "detail": "In win, ubuntu. After downloading the model, there is no way to modify the code to execute the local model, and the path has to be re-entered every time; LLM model and Embedding model support are also under the official website, and can be used under other projects (wenda)", "id": 68}
{"title": "[FEATURE] knowledge_based_chatglm.py: renamed or missing?", "file": "2023-04-30.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/210", "detail" : "Not found. Was it renamed? Or, is it missing? How can I get it?", "id": 69}
{"title": "sudo apt-get install -y nvidia-container-toolkit-base execution error", "file": "2023-05-01.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/211", "detail" : "**Problem Description**", "id": 70}
{"title": "Poor effect is almost impossible to answer", "file": "2023-05-01.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/212", "detail": "docx file with 50 questions and answers", "id": 71}
{"title": "Is it possible to add a new way to build langchain based on chatglm api calls", "file": "2023-05-02.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/218", "detail": "I have two 8G GPUs/ 40G memory server, one by one made into chatglm API; I want to deploy LangChain on another server; There doesn't seem to be a similar code online. ", "id": 72}
{"title": "The computer is an Intel integrated graphics card;  The runtime tells me that I can't find nvcuda.dll, the model can't run", "file": "2023-05-02.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/219", "detail": "Hello, my computer is an intel integrated graphics card, but the CPU is i5-11400 @ 2.60GHz, 64G memory; ", "id": 73}
{"title": "According to langchain's official documentation and usage patterns, is it possible to change Faiss to Elasticsearch?"} What additional adjustments will be required? solve", "file": "2023-05-03.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/221", "detail": "I am a novice, due to the business model (some of my own scenarios and optimizations), I hope to use Elasticsearch as the internal retrieval mechanism of this systemI don't know if it can be replaced, and at the same time, what changes will be involved? Or what other influences may be, I hope the author and the bigwigs will not hesitate to give advice! ", "id": 74}
{"title": "Is it possible to support T5 in the future", "file": "2023-05-04.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/224", "detail": "Is it possible to support T5-based models?", "id": 75}
{"title": "[BUG] Memory Overflow / torch.cuda.OutOfMemoryError:", "file": "2023-05-04.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/229", "detail": "**Problem / Problem Description**", "id": 76}
{"title": "Error No module named 'chatglm_llm'", "file": "2023-05-04.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/230", "detail": "I have installed the package, but I can't hang it in python" , "id": 77}
{"title": "Can you produce a description of the API deployment", "file": "2023-05-04.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/233", "detail": "**Feature Description**", "id": 78."} }
{"title": "Error using docs/API.md", "file": "2023-05-04.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/234", "detail": "Error using API.md document 2 methods", "id": 79}
{"title": "Error loading pdf document?"} , "file": "2023-05-05.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/238", "detail": "ew one with MEAN pooling.", "id": 80}
{"title": "The uploaded local knowledge file cannot be displayed after being uploaded again, only one is displayed successfully, and the other ones are refreshed again after the upload is successful, and it is gone", "file": "2023-05-05.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/239", "detail": "Hello, The project is very inspiring, thanks to ~", "id": 81}
{"title": "A new virtual environment was created, the package was installed, and the model was automatically downloaded, but it still appeared: OSError: Unable to load weights from pytorch checkpoint file for", "file": "2023-05-05.02", "url":  "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/240", "detail": "! [78ac8e663fdc312d0e9d78da95925c4] (https://user-images.githubusercontent.com/34124260/236378728-9ea4424f-0f7f-4013-9d33-820b723de321.png)", "id": 82}
{"title": "[BUG] Data cannot be loaded", "file": "2023-05-05.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/243", "detail": "The following error is reported in the .txt format used, utf-8 encoding", "id": 83}
{"title": "Cannot read PDF", "file": "2023-05-05.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/244", "detail": "Is it a webui or a cli_demo", "id": 84}
{"title": "The local txt file has 500M, and it is slow to load, how can I improve the speed?"} , "file": "2023-05-06.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/251", "detail": "! [yayRzxSYHP] (https://user-images.githubusercontent.com/109277248/236592902-f5ab338d-c1e9-43dc-ae16-9df2cd3c1378.jpg)", "id": 85}
{"title": "[BUG] After gradio uploads the knowledge base and refreshes it, the knowledge base is gone, and you can only see the previously uploaded knowledge base when you restart it", "file": "2023-05-06.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/253", "detail": " After gradio uploads the knowledge base and refreshes it, the knowledge base is gone, and you can only see the previously uploaded knowledge base when you restart it", "id": 86}
{"title": "[FEATURE] Can OpenAI's models be supported? For example, GPT-3, GPT-3.5, GPT-4; embedding added text-embedding-ada-002", "file": "2023-05-06.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/254", "detail": "**Feature Description**" , "id": 87}
{"title": "[FEATURE] Can you add support for milvus vector database / Concise description of the feature", "file": "2023-05-06.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/256", " detail": "**Feature Description**", "id": 88}
{"title": "Is there a difference in accuracy between CPU and GPU in addition to speed?"} , "file": "2023-05-06.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/259", "detail": "theoretically no difference", "id": 89}
{"title": "M1, how do you see if you use MPS or CPU when generating your answer?" , "file": "2023-05-06.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/260", "detail": "m1, how do you see if MPS or CPU is used when generating the answer?" , "id": 90}
{"title": "The knowledge base is gone as soon as it is refreshed", "file": "2023-05-07.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/263", "detail": "The knowledge base is no longer refreshed after uploading", "id": 91}
{"title": "No model reported for local deployment", "file": "2023-05-07.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/267", "detail": "It is recommended to download the LLM and embedding models to the local computer in configs/model_ config and then run after writing the local storage path of the model", "id": 92}
{"title": "[BUG] python3: can't open file 'webui.py': [Errno 2] No such file or directory", "file": "2023-05-08.02", "url": " https://github.com/imClumsyPanda/langchain-ChatGLM/issues/269", "detail": "Problem Description", "id": 93}
{"title": "Missing Module Prompt", "file": "2023-05-08.02", "URL": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/271", "detail": "Because I already have my own docker environment, start directly webui.py, prompt", "id": 94}
{"title": "After running api.py, curl -X POST \"http://127.0.0.1:7861\" error is reported? ", "file": "2023-05-08.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/272", "detail": "Execute curl -X POST \"http://127.0.0.1:7861\" \\ -H 'Content-Type: application/json' \\ -d '{\"prompt\": \"hello\", \"history\": []}',how to solve the error", "id": 95}
{"title": "[BUG] colab installation requirements prompts protobuf version issues?"} , "file": "2023-05-08.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/273", "detail": "pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.", "id": 96}
{"title": "What method is used to calculate the similarity of vectors in the project?"} , "file": "2023-05-08.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/275", "detail": "Basically according to the FAISS.similarity_search_with_score_by_vector in langchain" , "id": 97}
{"title": "[BUG] pdf fails to load after installing detectron2", "file": "2023-05-08.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/276", "detail": "**Problem Description** ", "id": 98}
{"title": "[BUG]Unable to stream output using ChatYuan-V2 model, error will be reported", "file": "2023-05-08.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/277", "detail": " On the one hand, it seems that ChatYuan itself does not support stream_chat, and someone raised an issue on clueai's side, and they said that it has not been developed yet, so it is estimated that this attribute cannot be adjusted; But on the other hand, it seems that the T5 model itself is not a decoder-only model, so it can't be streamed (personal understanding)", "id": 99}
{"title": "[BUG] Failed to load text2vec model", "file": "2023-05-08.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/278", "detail": "**Problem Description**", "id."} ": 100}
{"title": "Can I add a web search function", "file": "2023-05-08.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/281", "detail": "Can I add a web search function", "id": 101}
{"title": "[FEATURE] Structured data sql, excel, csv, when will it be supported.} , "file": "2023-05-08.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/283", "detail": "**Feature Description**", "id": 102}
{"title": "TypeError: ChatGLM._call() got an unexpected keyword argument 'stop'", "file": "2023-05-08.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/284", "detail": "No sentence-transformers model found with name D:\\DevProject\\langchain-ChatGLM\\GanymedeNil\\text2vec-large-Chinese. Creating a new one with MEAN pooling.", "id": 103}
{"title": "Some bugs and design logic questions about api.py?"} " file": "2023-05-09.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/285", "detail": "First of all, I would like to ask, is this api.py really okay after the developers test it on their own computers?" , "id": 104}
{"title": "Is there a rented computing platform, after running api.py, the browser http://localhost:7861/ reports an error", "file": "2023-05-09.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/287", "detail": " Does this problem occur on rented GPU platforms??? ", "id": 105}
{"title": "Is there a way to cut paragraphs in your project?"} , "file": "2023-05-09.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/288", "detail": "Do you use the document cutting method in text_load? It doesn't seem to be used in the code? ", "id": 106}
{"title": "Error raise ValueError(f\"Knowledge base {knowledge_base_id} not found\") ValueError: Knowledge base ./vector_store not found", "file": "2023-05-09.02", "url":  "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/289", "detail": "File \"/root/autodl-tmp/chatglm/langchain-ChatGLM-master/api.py\", line 183, in chat", "id": 107}
{"title": "Can I access the Vikuna model", "file": "2023-05-09.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/290", "detail": "Can I directly access the Vicuna model if I already have a local model?"} , "id": 108}
{"title": "[BUG] Questions related to the question formula have a high probability of exploding the video memory", "file": "2023-05-09.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/291", "detail": "**Problem Description**", "id."] ": 109}
{"title": "Failed to install pycocotools, I couldn't solve it after many methods."} , "file": "2023-05-10.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/292", "detail": "**Problem Description**", "id": 110}
{"title": "Install with requirements, PyTorch is installed with CPU version", "file": "2023-05-10.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/294", "detail": "If title, Use requirements to install, PyTorch installs the CPU version, and when running the program, it also uses the CPU to work. ", "id": 111}
{"title": "Can you give a rough server deployment tutorial", "file": "2023-05-10.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/298", "detail": ""Development and deployment" You can use it as a server deployment tutorial. , "id": 112}
{"title": " Error(s) in loading state_dict for ChatGLMForConditionalGeneration:", "file": "2023-05-10.02", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/299" , "detail": "If there is a problem in operation, the port page of 7860 cannot be displayed, please ask for help." , "id": 113}
{"title": "ChatYuan-large-v2 model failed to load", "file": "2023-05-10.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/300", "detail": "**Actual Result**", "id": 114}
{"title": "New summary function", "file": "2023-05-10.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/303", "detail": "Hello, will you consider adding inference and speech understanding functions for long text messages in the future?" e.g. generate abstract", "id": 115}
{"title": "[BUG] pip install -r requirements.txt error", "file": "2023-05-10.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/304", "detail": "pip install."} langchain -i https://pypi.org/simple", "id": 116}
{"title": "[BUG] Error message for uploading knowledge base file", "file": "2023-05-10.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/305", "detail": "! [19621e29eaa547d01213bee53d81e6a] (https://github.com/imClumsyPanda/langchain-ChatGLM/assets/84606552/7f6ceb46-e494-4b0e-939c-23b585a6d9d8)", "id": 117}
{"title": "[BUG] AssertionError: <class 'gradio.layouts.Accordion'> Component with id 41 not a valid input component.", "file": "2023-05-10.03", "url":  "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/306", "detail": "Problem Description", "id": 118}
{"title": "[BUG] CUDA out of memory with container deployment", "file": "2023-05-10.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/310", "detail": "**Problem / Problem Description**", "id": 119}
{"title": "[FEATURE] Add fine-tuning training function", "file": "2023-05-11.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/311", "detail": "**Feature Description**", "id": 120}
{"title": "How to use multi-card deployment, multiple GPUs", "file": "2023-05-11.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/315", "detail": "How to use all of them on multiple GPUs on a machine", "id": 121}
{"title": "What is the relationship between this knowledge base Q&A and chatglm", "file": "2023-05-11.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/319", "detail": "Which part of this knowledge base Q&A is associated with chatglm, Isn't there no chatglm, the knowledge base Q&A can also be brought out alone", "id": 122}
{"title": "[BUG] ImportError: libcudnn.so.8: cannot open shared object file: No such file or directory", "file": "2023-05-12.03", "url": " https://github.com/imClumsyPanda/langchain-ChatGLM/issues/324", "detail": "Problem Description**raceback (most recent call last):", "id": 123}
{"title": "WebUI started successfully, but an error is reported", "file": "2023-05-12.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/325", "detail": "**Problem Description**", "id": 124."} }
{"title": "Error message when switching MOSS", "file": "2023-05-12.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/327", "detail": "danshi but in the published source code,", "id": 125}
{"title": "Can the Vicuna model be plugged in?"} , "file": "2023-05-12.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/328", "detail": "Hello! For both the MOSS model and the vicuna model, AutoModelForCausalLM loads the model, but a slight change (model paths) will result in this error. What is the cause of this error", "id": 126}
{"title": "Hello, can I run on an Alibaba Cloud CPU server?" What is the ideal CPU configuration if so? ", "file": "2023-05-12.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/330", "detail": "Hello, can I run on Alibaba Cloud CPU server?" What is the ideal CPU configuration if so? ", "id": 127}
{"title": "Hello, can an 8-core, 32g CPU run multiple rounds of dialogue?"} , "file": "2023-05-12.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/331", "detail": "What kind of CPU configuration is better? I currently want to deploy multiple rounds of conversations under the CPU? ", "id": 128}
{"title": "[BUG] System error when entering more than 10,000 characters in chat", "file": "2023-05-12.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/332", "detail": "System error when entering more than 10,000 characters in chat, As shown in the figure below:", "id": 129}
{"title": "Can I increase the deployment of multi-user access interfaces for APIs?"} , "file": "2023-05-12.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/333", "detail": "The default deployer only supports single-user access, and multi-user access needs to be queued. I've tested several related Github multi-user projects, but some of them still don't meet the requirements. This section describes how to implement the deployment interface of ChatGLM for multiple users to access ChatGLM at the same time, including http, websocket (stream), and web page. ", "id": 130}
{"title": "Multi-SIM deployment", "file": "2023-05-12.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/334", "detail": "How to improve concurrency with single-node multi-SIM or multi-server multi-SIM, fastapi deployment model", "id": 131}
{"title": "Can the WEBUI specify a knowledge base directory?"} , "file": "2023-05-12.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/335", "detail": "**Feature Description**", "id": 132}
{"title": "[BUG] Cannot read properties of undefined (reading 'error')", "file": "2023-05-12.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/336", "detail": "**Problem Description**", "id": 133}
{"title": "[BUG] 1 validation error for HuggingFaceEmbeddings model_kwargs extra fields not permitted.", "file": "2023-05-12.03", "url": " https://github.com/imClumsyPanda/langchain-ChatGLM/issues/337", "detail": "There was a problem with the model loading to 100%:", "id": 134}
{"title": "Can I fix the need to restart the uploaded knowledge base", "file": "2023-05-12.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/338", "detail": "Serious problem", "id": 135}
{"title": "[BUG] 4 v100 cards burst out of video memory, the same is true in LLM session mode", "file": "2023-05-12.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/339", "detail": "**Problem Description."] Description**", "id": 136}
{"title": "Configure different TextSpliters for uploaded files", "file": "2023-05-12.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/341", "detail": "1. At present, ChineseTextSpliter sharding is not friendly to English, especially code files, and limits the fixed length; Resulting in unsatisfactory results in conversations", "id": 137}
{"title": "[FEATURE] Can I add a Bloom series of models in the future?} According to Oracle's test, this series of Chinese reviews are good", "file": "2023-05-13.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/346", "detail": "**Feature Description**", "id": 138}
{"title": "[BUG] v0.1.12 Boot after packaging the image webui.py Failed / Concise description of the issue", "file": "2023-05-13.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/347", " detail": "Problem Description", "id": 139}
{"title": "Error when switching MOSS models", "file": "2023-05-13.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/349", "detail": "I asked yesterday, saying that the transformers version is wrong, Need 4.30.0, found that there is no such version, today updated to 4.29.1, still the error is reported, the error is as follows", "id": 140}
{"title": "[BUG] pdf failed to load", "file": "2023-05-13.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/350", "detail": "**Problem Description**", "id": 141."} }
{"title": "Suggestion can enhance a wave of comments at a later stage, which will also help more people follow up with PR", "file": "2023-05-13.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/351", "detail": "Know that the author and the team are frantically updating the review code, It is only suggested that the core code can be supplemented with some comments after subsequent stabilization, so as to help more people understand the ideas of the authors of each module and propose better optimizations. ", "id": 142}
{"title": "[FEATURE] MOSS Quantization Edition Support", "file": "2023-05-13.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/353", "detail": "**Feature Description**", "id" : 143}
{"title": "[BUG] moss model failed to load", "file": "2023-05-13.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/356", "detail": "**Problem Description**", "id": 144}
{"title": "[BUG] The load_file function in load_doc_qa.py has a bug", "file": "2023-05-14.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/358", "detail": "The original function is:", "id": 145."} }
{"title": "[FEATURE] API mode, knowledge base loading optimization", "file": "2023-05-14.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/359", "detail": "As the title suggests, the current version, the knowledge base will be loaded once every time the local knowledge base interface is calledIs there a better way? ", "id": 146}
{"title": "How do I use the curl command to call a Python api.py script after the backend is deployed?"} , "file": "2023-05-15.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/361", "detail": "In other words, I want to make a conversational bot now, and I want to co-debug with the company's front-end and back-end? How to call each other with the front and back ends? Private message, paid answer!! ", "id": 147}
{"title": "Can I fix the need to restart the uploaded knowledge base", "file": "2023-05-15.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/363", "detail": "Can I fix the need to restart the uploaded knowledge base", "id": 148}
{"title": "[BUG] pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple", "file": "2023-05-15.03", "url": " https://github.com/imClumsyPanda/langchain-ChatGLM/issues/364", "detail": "My python is 3.8.5", "id": 149}
{"title": "pip install gradio error", "file": "2023-05-15.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/367", "detail": "help me", "id": 150}
{"title": "[BUG] pip install gradio keeps getting stuck", "file": "2023-05-15.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/369", "detail": "! [aba82742dd9d4d242181662eb5027a7] (https://github.com/imClumsyPanda/langchain-ChatGLM/assets/84606552/cd9600d9-f6e7-46b7-b1be-30ed8b99f76b)", "id": 151}
{"title": "[BUG] Concise description of the issue", "file": "2023-05-16.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/370", "detail": "The initial loading of the local knowledge base was successful, but after asking the question, it is not possible to rewrite and load the local knowledge base", "id": 152}
{"title": "[FEATURE] Concise description of the feature", "file": "2023-05-16.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/371", "detail": "** Feature Description**", "id": 153}
{"title": "On Windows, where will the model files be installed by default", "file": "2023-05-16.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/372", "detail":  "-------------------------------------------------------------------------------", "id": 154}
{"title": "[FEATURE] Conversation Management", "file": "2023-05-16.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/374", "detail": "How to manage conversations in the case of knowledge base retrieval?" , "id": 155}
{"title": "llm device: cpu embedding device: cpu", "file": "2023-05-16.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/376", "detail": "**Problem Description."} Description**", "id": 156}
{"title": "[FEATURE] What separator can be used to detach knowledge points from a text file?} , "file": "2023-05-16.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/377", "detail": "**Feature Description**", "id": 157}
{"title": "[BUG] Failed to upload file: PermissionError: [WinError 32] Another program is using this file and the process is inaccessible." , "file": "2023-05-16.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/379", "detail": "**Problem Description**", "id": 158}
{"title": "[BUG] Execute python api.py error", "file": "2023-05-16.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/383", "detail": "error message", "id": 159}
{"title": "model_kwargs extra fields not permitted (type=value_error.extra)", "file": "2023-05-16.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/384", " detail": "Hello everyone, did you encounter this?" , "id": 160}
{"title": "[BUG] Concise description of the issue", "file": "2023-05-17.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/385", "detail": "ls1 = appears during execution."} [ls[0]]", "id": 161}
{"title": "[FEATURE] Performance Optimization", "file": "2023-05-17.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/388", "detail": "**Feature Description**", "id": 162}
{"title": "[BUG] Moss Model Q&A, RuntimeError: probability tensor contains either inf, nan or element < 0", "file": "2023-05-17.03", "url": " https://github.com/imClumsyPanda/langchain-ChatGLM/issues/390", "detail": "Problem Description", "id": 163}
{"title": "Does anyone know about the v100GPU's 32G video memory, will it report an error?"} Is it V100GPU supported? ", "file": "2023-05-17.03", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/392", "detail": "**Problem Description**", "id": 164}
{"title": "Rough workaround for coding problems such as 'gbk' codec can't encode character '\\xab' in position 14: illegal multibyte sequence", "file": "2023-05-17.03", "url":  "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/397", "detail": "Feature Description", "id": 165}
{"title": "Could not import sentence_transformers python package. Please install it with 'pip install sentence_transformers'.", "file": "2023-05-18.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/400", "detail": "**Problem / Problem Description**", "id": 166}
{"title": "Support model Q&A and retrieval Q&A", "file": "2023-05-18.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/401", "detail": "Different queries should have different answers based on inconsistent intent and intent."} , "id": 167}
{"title": "When splitting text, can it be split according to each line of the txt file, that is, according to the line break symbol \\n???" , "file": "2023-05-18.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/403", "detail": "How should the following code be modified?" , "id": 168}
{"title": "local_doc_qa/local_doc_chat interface response is serial", "file": "2023-05-18.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/405", "detail": "**Problem."} Description**", "id": 169}
{"title": "Why can't I find the source but still can't answer the question?", "file": "2023-05-18.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/406", "detail": "! [Picture] (https://github.com/imClumsyPanda/langchain-ChatGLM/assets/3349611/1fc81d61-2409-4330-9065-fdda1a27c86a)", "id": 170}
{"title": "What is the format of adding a single piece of content in the knowledge base test, if it is replaced with a text import?"} I found that adding a single piece of content to the test works well.", "file": "2023-05-18.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/412", "detail": "I found that in the knowledge base test, 'add a single piece of content', and check 'Prohibit content from clause storage', even if 'Do not enable contextual association' The results are very good.", "id": 171}
{"title": "[BUG] Unable to configure knowledge base", "file": "2023-05-18.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/413", "detail": "**Problem Description**", "id": 172}
{"title": "[BUG] The page is white when deployed on the EAS of the Alibaba PAI platform", "file": "2023-05-19.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/414", "detail": "**Problem Description."] Description**", "id": 173}
{"title": "Call /local_doc_qa/local_doc_chat after API deployment Return Knowledge base samples not found", "file": "2023-05-19.04", "url": " https://github.com/imClumsyPanda/langchain-ChatGLM/issues/416", "detail": "input parameters", "id": 174}
{"title": "[FEATURE] Upload a txt file saved as 'ascii' codec can't decode byte 0xb9 in position 6: ordinal not in range(128)", "file": "2023-05-20.04", "url":  "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/421", "detail": "Upload Word save as txt file", "id": 175}
{"title": "Is the knowledge base that is created and saved permanently saved does not come out after being refreshed?"} Is it possible to connect to an external vector knowledge base? ", "file": "2023-05-21.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/422", "detail": "Is the saved knowledge base permanently saved if it does not come out after refreshing?" Is it possible to connect to an external vector knowledge base? ", "id": 176}
{"title": "[BUG] Running with colab, unable to load model, error: 'NoneType' object has no attribute 'message_types_by_name'", "file": "2023-05-21.04", "url": " https://github.com/imClumsyPanda/langchain-ChatGLM/issues/423", "detail": "Problem Description", "id": 177}
{"title": "Do I need a vector database?"} And when do you need to use a vector database? ", "file": "2023-05-21.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/424", "detail": "I am currently using text2vec, do I need to use a vector database? And when do you need to use a vector database? ", "id": 178}
{"title": "huggingface model reference issue", "file": "2023-05-22.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/427", "detail": "It seems to have turned into an error recently?"} , "id": 179}
{"title": "Hello, I get this killed error when loading the local txt file, the TXT file is about 100M in size."} The reason? Thank you. ", "file": "2023-05-22.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/429", "detail": "<img width=\"677\" alt=\"929aca3b22b8cd74e997a87b61d241b\"  src=\"https://github.com/imClumsyPanda/langchain-ChatGLM/assets/109277248/24024522-c884-4170-b5cf-a498491bd8bc\">", "id": 180}
{"title": ""Простовання п For example: Add data or delete a piece of data through the http API", "file": "2023-05-22.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/430", "detail": "For example: add, delete, or modify a piece of data through the http API." , "id": 181}
{"title": "[FEATURE] Double-column pdf recognition problem", "file": "2023-05-22.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/432", "detail": "I tried the model, and I feel that the accuracy of single-column pdf recognition is high, However, due to the basic OCR technology used, there are many problems in the identification of some double-column PDF papers, is there any way to improve it? ", "id": 182}
{"title": "Deployment startup problem, little brother beginner to ask for a big guy to answer", "file": "2023-05-22.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/433", "detail": "1.python loader/image_ loader.py, ModuleNotFoundError: No module named 'configs' is displayed, but python webui.py can still run", "id": 183}
{"title": "Can it support detecting the increase of documents in the directory and loading documents incrementally, without affecting the foreground conversation, in fact, it supports read/write splitting."} It would be better if I could support querying which documents have been vectorized, delete obsolete documents, etc., thank you. ", "file": "2023-05-22.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/434", "detail": "**Feature Description**", "id": 184}
{"title": "[BUG] Concisely describe the problem / cuda error under windows, please use https://github.com/Keith-Hon/bitsandbytes-windows.git", "file": "2023-05-22.04", "url": " https://github.com/imClumsyPanda/langchain-ChatGLM/issues/435", "detail": "pip install git+https://github.com/Keith-Hon/bitsandbytes-windows.git", "id": 185}
{"title": "[BUG] from commit 33bbb47,  Required library version not found: libbitsandbytes_cuda121_nocublaslt.so. Maybe you need to compile it from source?", "file": "2023-05-23.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/438", "detail": "**Problem Description** ", "id": 186}
{"title": "[BUG] Concise description of the issue Upload a 60m txt file with an error, showing a timeout, is there a limit to the size of the file that can be uploaded", "file": "2023-05-23.04", "url": " https://github.com/imClumsyPanda/langchain-ChatGLM/issues/439", "detail": "ERROR 2023-05-23 11:13:09,627-1d: Timeout reached while detecting encoding for ./docs/GLM model format data.txt", " id": 187}
{"title": "[BUG] TypeError: issubclass() arg 1 must be a class", "file": "2023-05-23.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/440", "detail": "**Issue** ", "id": 188}
{"title": "After executing python3 webui.py, it keeps saying "The model is not loaded successfully, please go to the upper left corner of the page to re-select the "Model Configuration" tab and click the "Load Model" button", "file": "2023-05-23.04", "url": " https://github.com/imClumsyPanda/langchain-ChatGLM/issues/441", "detail": "Problem Description", "id": 189}
{"title": "Can you provide import support for web documents", "file": "2023-05-23.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/444", "detail": "Nowadays, many online documents are used as collaboration tools, so there is a greater demand for importing online documents through URLs", "id": 190}
{"title": "[BUG] history index issue", "file": "2023-05-23.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/445", "detail": "history and model chat function in comparison dialog."} When passing in llm._call, the index used for history is a bit of a problem, resulting in the content of the previous round of dialogue not being fed into the model. ", "id": 191}
{"title": "[BUG] moss_llm Not implemented", "file": "2023-05-23.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/447", "detail": "Some methods are not supported, such as history_len", "id": 192}
{"title": "How does langchain-ChatGLM delete a piece of data from a local knowledge base?"} , "file": "2023-05-23.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/448", "detail": "For example, a user has just submitted an incorrect piece of data to the local knowledge base, and now how can I find it from the local knowledge base and delete it." , "id": 193}
{"title": "[BUG] Concisely elaborate on the problem / UnboundLocalError: local variable 'resp' referenced before assignment", "file": "2023-05-24.04", "url": " https://github.com/imClumsyPanda/langchain-ChatGLM/issues/450", "detail": "In the latest version of the code, running api.py has the above error (UnboundLocalError: local variable 'resp' referenced before assignment), It has been observed by debug that local_doc_qa.llm.generatorAnswer(prompt=question, history=history,streaming=True) may not return any value. ", "id": 194}
{"title": "Is there a PROMPT_TEMPLATE that allows the model not to answer sensitive questions", "file": "2023-05-24.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/452", "detail": "## PROMPT_TEMPLATE questions" , "id": 195}
{"title": "[BUG] The test environment has an incorrect Python version", "file": "2023-05-24.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/456", "detail": "**Problem Description**", "id."} ": 196}
{"title": "[BUG] The style of the webui is incorrect after deployment", "file": "2023-05-24.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/458", "detail": "**Problem Description**", "id" : 197}
{"title": "Problem with configuring the default LLM model", "file": "2023-05-24.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/459", "detail": "Problem Description**", "id": 198}
{"title": "[FEATURE]It's time to update the autoDL image", "file": "2023-05-24.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/460", "detail": "As titled, I ran the autoDL image, Found to be 4.27, git pull the new version of the code function + old dependency environment, all kinds of strange problems. ", "id": 199}
{"title": "[BUG] tag:0.1.13 In cpu mode, I can't run it if I want to use the local model, various path parameter problems", "file": "2023-05-24.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/462", "detail":  "-------------------------------------------------------------------------------", "id": 200}
{"title": "[BUG] Have any of your classmates encountered this mistake!!} This killed error occurs when loading the local txt file, and the TXT file is about 100M in size. ", "file": "2023-05-25.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/463", "detail": "Run cli_demo.py. Is the local txt file too big? ABOUT 100M. ", "id": 201}
{"title": "Can the API version provide a streaming interface for WEBSOCKETS", "file": "2023-05-25.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/464", "detail": "In the webui version, the streaming output of WS is used, and the overall perception response is very fast", "id": 202}
{"title": "[BUG] Installation bug log", "file": "2023-05-25.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/465", "detail":  "Installed according to [install document](https://github.com/imClumsyPanda/langchain-ChatGLM/blob/master/docs/INSTALL.md),", "id": 203}
{"title": "VUE's pnmp i execution failed to fix - just run the npm i command", "file": "2023-05-25.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/466", "detail": "Thanks to the author!"} Great app, a lot of fun to use. ", "id": 204}
{"title": "Excuse me, does anyone know if cuda11.4 supports ???" "file": "2023-05-25.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/467", "detail": "Please ask a question, does anyone know if cuda11.4 supports ???" , "id": 205}
{"title": "Is there any contextual correlation for question-based search in multiple rounds of Q&A", "file": "2023-05-25.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/468", "detail": "In multiple rounds of Q&A based on knowledge base, the first question describes a topic, Subsequent issue descriptions do not contain keywords for this topic, but there is a contextual context. If you use follow-up questions to search the knowledge base, you may find irrelevant information, resulting in the large model not being able to answer the questions correctly. Does this project need to take this situation into account? ", "id": 206}
{"title": "[BUG] Out of memory problem", "file": "2023-05-26.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/470", "detail": "I used the local chatglm-6b-int4 model, and then it shows out of memory (win10+."). 32G memory + 1080ti11G), how much memory is generally needed? How should this bug be fixed? ", "id": 207}
{"title": "[BUG]pycocotools installation failed in a pure intranet environment", "file": "2023-05-26.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/472", "detail": "**Problem Description** ", "id": 208}
{"title": "[BUG] webui.py Reloading the model results in a KeyError", "file": "2023-05-26.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/473", "detail": "**Problem Description."} Description**", "id": 209}
{"title": "chatyuan unavailable", "file": "2023-05-26.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/475", "detail": "**Problem Description**", "id": 210}
{"title": "[BUG] There is a bug in the text segmentation model AliTextSplitter, which will put "." As a splitter", "file": "2023-05-26.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/476", "detail": "There is a bug in the semantic segmentation model of Ali Damo Academy, and \"" will be set by default." Split as a splitter regardless of contextual semantics. Whether there are any other splitters is unknown. Proposed amendment: Add "." Replace them with other characters uniformly, split them and replace them back. Or add another segmentation model. ", "id": 211}
{"title": "[BUG] RuntimeError: Error in faiss::FileIOReader::FileIOReader(const char*) a", "file": "2023-05-27.04", "url": " https://github.com/imClumsyPanda/langchain-ChatGLM/issues/479", "detail": "Problem Description", "id": 212}
{"title": "[FEATURE] Installation, why does conda create specify an additional path with -p instead of the default /envs below", "file": "2023-05-28.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/481", "detail": "##** Feature Description**", "id": 213}
{"title": "Unable to open web link after executing webui.py via Anaconda", "file": "2023-05-28.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/485", "detail": "After executing the webui.py command."} , http://0.0.0.0:7860 cannot be opened after copying to the browser, and the message "This website cannot be accessed" is displayed. ", "id": 214}
{"title": "[BUG] Reload error after using p-tuningv2", "file": "2023-05-29.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/486", "detail": " Put the relevant files after p-tunningv2 training into the p-tunningv2 folder, check the use p-tuningv2 point to reload the model, and the console input error message: ", "id": 215}
{"title": "After executing webui.py on the server, the web link cannot be opened locally", "file": "2023-05-29.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/487", "detail": " This project is executed on the xxx.xx.xxx.xxx server and my code on webui.py is (demo", "id": 216}
{"title": "[FEATURE] Does VisualGLM-6B support", "file": "2023-05-29.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/488", "detail": "**Feature Description** ", "id": 217}
{"title": "Hello, let me ask you, when the backend API is deployed, do you support multi-user Q&A at the same time???" , "file": "2023-05-29.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/489", "detail": "If multiple users are supported, how many users can answer questions and answers?" It depends on the hardware, right? ", "id": 218}
{"title": "Why is V100GPU full of video memory and 0 utilization?"} , "file": "2023-05-29.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/491", "detail": "<img width=\"731\" alt=\"de45fe2b6cb76fa091b6e8f76a3de60\"  src=\"https://github.com/imClumsyPanda/langchain-ChatGLM/assets/109277248/c32efd52-7dbf-4e9b-bd4d-0944d73d0b8b\">", "id": 219}
{"title": "[HELP] If I build a product knowledge base in-house and use the INT-4 model, how many memory servers do I need to configure for 200 people?" , "file": "2023-05-29.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/492", "detail": "As the title suggests, I plan to build an online knowledge base for the company." , "id": 220}
{"title": "Hello, may I ask a question, it takes about 20 seconds to reply to the Q&A at the moment, how to improve the speed?"} V10032G server. ", "file": "2023-05-29.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/493", "detail": "**Problem Description**", "id": 221}
{"title": "[FEATURE] How to achieve results that only match the following and not the above", "file": "2023-05-29.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/494", "detail": "When building your own knowledge base, it mainly takes the form of Q&A pairs, So what I need is the answer to my question, but the current value of the chunk_size matches the context of the context, but I don't actually need it. I've had to turn up the chunk_size value for a more complete presentation of the following answers, but I don't really need half of the above content. That is, I threw half of the useless things to the prompt, and I didn't find some descriptions of this piece in faiss.py, how can I modify it? ", "id": 222}
{"title": "Hello, I call api.py deployment, why can I use postman to call ip plus port, but change the domain name to use postman and can't call it?" , "file": "2023-05-30.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/497", "detail": "! [5ufBSWxLyF] (https://github.com/imClumsyPanda/langchain-ChatGLM/assets/109277248/70e2fbac-5699-48d0-b0d1-3dc84fd042c2)", "id": 223}
{"title": "Call stream_chat in api.py and return Chinese garbled characters in source_documents."} , "file": "2023-05-30.04", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/498", "detail": "------------------------------------------------------------------------------- ", "id": 224}
{"title": "[BUG] Catch a bug, stream_chat parsing json problem in api.py", "file": "2023-05-30.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/501", "detail": "**Problem Description."} Description**", "id": 225}
{"title": "Windows On-Premise Encountered OMP Error", "file": "2023-05-31.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/502", "detail": "**Problem Description**", "id."} ": 226}
{"title": "[BUG] bug14 ,\"POST /local_doc_qa/upload_file HTTP/1.1\" 422 Unprocessable Entity", "file": "2023-05-31.05", "url": " https://github.com/imClumsyPanda/langchain-ChatGLM/issues/503", "detail": "Error message for uploaded file,Error returned,api.py", "id": 227}
{"title": "Hello, may I ask a question,api.py How do I change to a multi-threaded call when deploying?} Thank you", "file": "2023-05-31.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/505", "detail": "The current api.py script does not support multithreading", "id": 228}
{"title": "Hello, excuse me."} api.py When deploying, can you provide the backend churn return result? ", "file": "2023-05-31.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/507", "detail": "curl -X 'POST' \\", "id": 229}
{"title": "Streaming output, streaming interface, using server-sent events technology."} , "file": "2023-05-31.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/508", "detail": "Think like this, https://blog.csdn.net/weixin_43228814/article/details/130063010", "id": 230}
{"title": "Do you plan to add streaming output capabilities?"} How to break the slow response time of the ChatGLM model through the api call, the Fastapi streaming interface can quickly improve the response speed", "file": "2023-05-31.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/509", "detail": "**Problem / Problem Description**", "id": 231}
{"title": "[BUG] An error occurred while uploading the knowledge base (could not open xxx for reading: No such file or directory)", "file": "2023-05-31.05", "url": " https://github.com/imClumsyPanda/langchain-ChatGLM/issues/510", "detail": "Problem Description**", "id": 232}
{"title": "api.py Is the script going to add SSE streaming output?" , "file": "2023-05-31.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/511", "detail": "The first word can be detected when curl is called, so as to improve the experience of replying", "id": 233}
{"title": "[BUG] Using tornado to implement webSocket, multiple clients can connect at the same time, and stream reply can be realized, but multiple clients are used at the same time, the answer is very messy, is the model not supporting multithreading", "file": "2023-05-31.05", "url": " https://github.com/imClumsyPanda/langchain-ChatGLM/issues/512", "detail": "import asyncio", "id": 234}
{"title": "Does Chinese_alpaca_plus_lora support llama-based", "file": "2023-06-01.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/514", "detail": "support Chinese_alpaca_ plus_lora LLAMA-based, https://github.com/ymcui/Chinese-LLaMA-Alpaca this project", "id": 235}
{"title": "[BUG] Now I can read the pdf of the picture, but the pdf of the text can't be read, what ???" , "file": "2023-06-01.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/515", "detail": "**Problem Description**", "id": 236}
{"title": "The process cannot be terminated properly due to the stuck process in the process of inference", "file": "2023-06-01.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/516", "detail": "**Problem Description**", " id": 237}
{"title": "When curl is called, how can I pass parameters to the curl to achieve multiple rounds of dialogue from the second round?"} , "file": "2023-06-01.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/517", "detail": "first round of call:", "id": 238}
{"title": "Recommended addition of post-api.py log management capabilities?"} , "file": "2023-06-01.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/518", "detail": "------------------------------------------------------------------------------- ", "id": 239}
{"title": "Does anyone know how to deploy api.py scripts with multiple threads?"} "file": "2023-06-01.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/519", "detail": "api.py After deployment, with the following request, the time is slower, it seems to be single-threaded, how to change to multi-threaded deployment api.py:", "id": 240}
{"title": "[BUG] Upload a file to the knowledge base Any format and content will always fail", "file": "2023-06-01.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/520", "detail": "When uploading the knowledge base, the txt cannot be parsed, Even if you wear the sample txt in content/sample, you can't parse it, and you can't load it when you upload MD, PDF, etc., and you will continue to wait for more than 30 minutes. ", "id": 241}
{"title": "Question about prompt_template", "file": "2023-06-01.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/521", "detail": "What does this prompt_template mean and how do I use it?"). Can you give a specific template for reference? ", "id": 242}
{"title": "[BUG] Concise description of the issue", "file": "2023-06-01.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/522", "detail": "**Problem description / Problem Description**", "id": 243}
{"title": "Chinese participle full stop processing (about expressing the amount between \".\")", "file": "2023-06-02.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/523", "detail": "It is recommended to deal with 1.26 billion yuan of such participles, and it is best not to divide them into 12 and 600 million of these, need to be put together", "id": 244}
{"title": "ImportError: cannot import name 'inference' from 'paddle'", "file": "2023-06-02.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/526", "detail": " I looked around on the Internet, some said to upgrade paddle, I did it or it didn't work, some said to install paddlepaddle, I found the image source of Douban, but the installation error cannot detect archive format", "id": 245}
{"title": "[BUG] webscoket interface serial issue (/local_doc_qa/stream-chat/{knowledge_base_id})", "file": "2023-06-02.05", "url": " https://github.com/imClumsyPanda/langchain-ChatGLM/issues/527", "detail": "Problem Description", "id": 246}
{"title": "[FEATURE] Refresh the page to update the list of knowledge bases", "file": "2023-06-02.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/528", "detail": "**Feature description and improvements**", "id": 247}
{"title": "[BUG] After using ptuning to fine-tune the model, the Q&A effect is not good", "file": "2023-06-02.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/530", "detail": "### ptuning is not called", "id": 248}
{"title": "[BUG] Multi-round dialogue does not work well", "file": "2023-06-02.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/532", "detail": "When having multiple rounds of dialogue, no matter what history_len is set, the effect is not good."} In fact, I set it to a maximum value of 10, but in the dialog, it is still not possible to implement multiple rounds of dialogue:", "id": 249}
{"title": "RuntimeError: MPS backend out of memory (MPS allocated: 18.00 GB, other allocations: 4.87 MB, max allowed: 18.13 GB)", "file": "2023-06-02.05", "url":  "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/533", "detail": "**Problem Description**", "id": 250}
{"title": " Please pay attention to this issue! } The real use is definitely multi-user concurrent Q&A, hope to add this feature!! ", "file": "2023-06-02.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/534", "detail": "It depends on how many graphics cards you have", "id": 251}
{"title": "How many GPUs do you use to start a project?"} , "file": "2023-06-02.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/535", "detail": "**How many GPUs do I use when starting a project? **", "id": 252}
{"title": "What is the format of the curl call when using streaming output?"} , "file": "2023-06-02.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/536", "detail": "app. What should I fill in the knowledge_base_id in websocket(\"/local_doc_qa/stream-chat/{knowledge_base_id}\")(stream_chat)??? ", "id": 253}
{"title": "Startup error with local vicuna-7b model", "file": "2023-06-02.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/538", "detail": "Environment: Ubuntu 22.04 CUDA 12.1 does not have NCCL installed, Parallel Computing with RTX2080 and M60 Graphics Card", "ID": 254}
{"title": "Why don't you call the GPU and call the CPU directly", "file": "2023-06-02.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/539", "detail": "My Alibaba Cloud configuration is 16G video memory, prompt when running webui.py with the default code", "id": 255}
{"title": "Multiple files will be overwritten when uploaded", "file": "2023-06-03.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/541", "detail": "1. When multiple files are uploaded in the same knowledge base, they will overwrite each other, and the knowledge of multiple documents cannot be combined. Is there a big guy who knows how to solve it? ", "id": 256}
{"title": "[BUG] 'gcc' is not an internal or external command/LLM conversation that can only last one round", "file": "2023-06-03.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/542", "detail": "No compiled kernel."} found.", "id": 257}
{"title": "Start a project in API mode without a list of interfaces with a knowledge base?"} , "file": "2023-06-04.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/544", "detail": "How do I get the list of interfaces in the knowledge base?" If you don't need to write it yourself, can you provide relevant access methods, thank you", "id": 258}
{"title": "How can I get the interface to be called in stream mode when the program is started in API mode?"} , "file": "2023-06-05.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/546", "detail": "Hello author, after I start the program in API mode, I find that the interface response time is very long, how can I make the interface be called in stream mode? I want to implement the answer like the webui pattern", "id": 259}
{"title": "About the relevance of the data after the table is converted to text in the original text."} , "file": "2023-06-06.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/547", "detail": "Convert tabular data in the original text to text (X-Y: value; ... ), but after doing so, I found that the relevance was low and the effect was poor, so what was the best solution? ", "id": 260}
{"title": "After starting, only the last round of records will be recorded in both the LLM and the knowledge base Q&A mode", "file": "2023-06-06.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/548", "detail": "When pulling the latest code, when the Q&A is pulled, only the last Q&A record will be displayed on each page, What parameters need to be modified to keep the history? ", "id": 261}
{"title": "Provide system message configuration so that answers don't go beyond the knowledge base", "file": "2023-06-06.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/549", "detail": "**Feature Description."} Description**", "id": 262}
{"title": "[BUG] Error reported with p-tunningv2", "file": "2023-06-06.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/551", "detail": " According to the instructions of the README, put the p-tunningv2 training file into the p-tunningv2 folder, check the use p-tuningv2 point to reload the model, and the console prompts an error message: ", "id": 263}
{"title": "[BUG] mentally retarded, so many problems, I'm embarrassed to put them out, wasting time", "file": "2023-06-06.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/553", "detail": ".。." , "id": 264}
{"title": "[FEATURE] I see there's a ali_text_splitter.py in the code file, why not use it as a text splitter?" " file": "2023-06-06.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/554", "detail": "I see there is a ali_text_splitter.py in the code file, why not use him as a text splitter?" , "id": 265}
{"title": "Error reported when loading document function", "file": "2023-06-06.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/557", "detail": "def load_file(filepath, sentence_size=.") SENTENCE_SIZE):", "id": 266}
{"title": "After installing docker with reference to the guide, run the cli_demo.py prompt killed", "file": "2023-06-06.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/558", "detail": " root@b3d1bd08095c:/chatGLM# python3 cli_demo.py", "id": 267}
{"title": "Note: If the installation is wrong, note the version of these two packages wandb==0.11.0 protobuf==3.18.3", "file": "2023-06-06.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/559", "detail": " Error1: If the startup error 'protobuf' is reported, you need to update to 'protobuf==3.18.3 '", "id": 268}
{"title": "What is the optimization direction of the knowledge base for the less ideal matching of knowledge relevance for long texts", "file": "2023-06-07.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/563", "detail": "We may enter an article with 1W words, There are a lot of angle questions related to the topic of this article, and we asked him what to do, what if the content of his relevance match is very different from the answer we actually need. ", "id": 269}
{"title": "Can I use curl calls when using the stream-chat function for streaming output?"} , "file": "2023-06-07.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/565", "detail": "Why does the following call get an error???" , "id": 270}
{"title": "Has any of the bigwigs practiced parallel or multi-threaded deployments?"} , "file": "2023-06-07.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/566", "detail": "+1", "id": 271}
{"title": "Having trouble with a multithreaded deployment?"} , "file": "2023-06-07.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/567", "detail": "<img width=\"615\" alt=\"3d87bf74f0cf1a4820cc9e46b245859\"  src=\"https://github.com/imClumsyPanda/langchain-ChatGLM/assets/109277248/8787570d-88bd-434e-aaa4-cb9276d1aa50\">", "id": 272}
{"title": "[BUG]Loading vicuna-13b model with fastchat for Q&A of knowledge base has token restriction error", "file": "2023-06-07.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/569", "detail": " When I opened the api service of Fastchat's vicuna-13b, and then the config was configured there (the API was tested locally and the results could be returned), and then the knowledge base was loaded (the knowledge base probably has more than 1000 documents, and chatGLM can be reasoned normally), when the token exceeded the limit during the Q&A, I asked hello; ", "id": 273}
{"title": ""When I add a knowledge base, I always get an error when there are too many files, and I don't know which files I loaded, and I don't know if all of them fail or some of them succeed after the error is reported; I hope there is a function to load a specified folder as a knowledge base", "file": "2023-06-07.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/574", "detail": "**Feature Description**", "id": 274}
{"title": "[BUG] Error reported when the moss model is loaded locally", "file": "2023-06-08.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/577", "detail": "Error reported when the moss model is loaded locally:", "id": 275}
{"title": "Can't instantiate abstract class MOSSLLM with abstract methods _history_len", "file": "2023-06-08.05", "url": " https://github.com/imClumsyPanda/langchain-ChatGLM/issues/578", "detail": "(vicuna) ps@ps[13:56:20]:/data/chat/langchain-ChatGLM2/langchain-ChatGLM-0.1.13$ python webui.py  -- model-dir local_models --model moss --no-remote-model", "id": 276}
{"title": "[FEATURE] Can I increase the prompt_template control on the front-end page?} Or can you support the front-end page to choose which prompt to use? ", "file": "2023-06-08.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/579", "detail": "At present, you can only modify one prompt in config, and it is troublesome to switch between multiple different scenarios", "id": 277}
{"title": "[BUG] Streamlit ui bug, error will be reported when adding knowledge base", "file": "2023-06-08.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/580", "detail": "**Problem Description."} Description**", "id": 278}
{"title": "[FEATURE] webui/webui_st can support history?} At present, you can only have one conversation", "file": "2023-06-08.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/581", "detail": "I tried that the webui and webui_st do not support history dialogue, you can only talk once, can't you turn on all history by default?" , "id": 279}
{"title": "Error message when starting python cli_demo.py --model chatglm-6b-int4-qe", "file": "2023-06-09.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/585", "detail": "Download the model."} , and related dependencies, run 'python cli_demo.py --model chatglm-6b-int4-qe' and get an error:", "id": 280}
{"title": "Error message for rebuilding knowledge base", "file": "2023-06-09.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/586", "detail": "**Problem Description**", "id": 281}
{"title": "[FEATURE] Can I block paddle, I don't need OCR, the effect is poor, and the dependency environment is complicated", "file": "2023-06-09.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/587", "detail": "I hope I don't need to rely on paddle."} ", "id": 282}
{"title": "question: Can document vectorization be done manually?"} , "file": "2023-06-09.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/589", "detail": "Existing company-level data 500G+, need to use this function, how to manually implement this vectorization, and then load it", "id": 283}
{"title": "Can the view frontend perform streaming returns?"} , "file": "2023-06-09.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/590", "detail": "Can the view frontend perform streaming returns??" , "id": 284}
{"title": "[BUG]  Load parallel cpu kernel failed, using default cpu kernel code", "file": "2023-06-11.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/594", " detail": "Problem Description**", "id": 285}
{"title": "[BUG] Concise description of the issue", "file": "2023-06-11.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/595", "detail": "**Problem description / Problem Description**", "id": 286}
{"title": "I get a KeyError: 'name' error when uploading a local knowledge base, and the local knowledge base is all .txt files, and the number of files is about 2000+."} , "file": "2023-06-12.05", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/597", "detail": "<img width=\"649\" alt=\"KError\"  src=\"https://github.com/imClumsyPanda/langchain-ChatGLM/assets/59411575/1ecc8182-aeee-4a0a-bbc3-74c2f1373f2d\">", "id": 287}
{"title": "model_config.py has configuration information for the Vikuna-13B-HF model, but it still doesn't seem to be available?"} , "file": "2023-06-12.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/600", "detail": "@dongyihua543", "id": 288}
{"title": "ImportError: Using SOCKS proxy, but the 'socksio' package is not installed. Make sure to install httpx using 'pip install httpx[socks]'.", "file": "2023-06-12.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/605", "detail": "Should proxy the issue, But I tried many methods but couldn't solve it,", "id": 289}
{"title": "[BUG] similarity_search_with_score_by_vector Error in case no match found", "file": "2023-06-12.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/607", " detail": "When the matching threshold VECTOR_SEARCH_SCORE_THRESHOLD is set, the vectorstore returns empty, at which point the above handler will error", "id": 290}
{"title": "[FEATURE] How to build an English knowledge base", "file": "2023-06-12.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/609", "detail": "**Feature Description**", " id": 291}
{"title": "Who has the vicuna weight?"} "file": "2023-06-13.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/611", "detail": "**Problem Description**", "id": 292}
{"title": "Does the [FEATURE] API enable you to upload folders?"} , "file": "2023-06-13.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/612", "detail": "The user is too lazy to select all the files and wants to upload a folder, can the API implement this function?" , "id": 293}
{"title": "After the multi-card deployment, upload a single file as a knowledge base, is the single card used to generate the vector or the multi-card?"} , "file": "2023-06-13.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/614", "detail": "At present, I check my local multi-card deployment, it seems that I still use a single card when generating knowledge base vectors", "id": 294}
{"title": "[BUG] python webui.py Prompts Illegal Instructions", "file": "2023-06-13.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/615", "detail": "(/data/conda-langchain [ root@chatglm langchain-ChatGLM]# python webui.py", "id": 295}
{"title": "Knowledge base file cross-row segmentation problem", "file": "2023-06-13.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/616", "detail": "My knowledge base file txt file is a line by line knowledge, lined with \\n."}"}} , "id": 296}
{"title": "[FEATURE] Bing search Q&A has a streaming API?} , "file": "2023-06-13.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/617", "detail": "There is this Bing search answer on the web side, but the API interface is not found, can the big guy give a hint?" , "id": 297}
{"title": "I want to get a macOS M2 installation tutorial", "file": "2023-06-14.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/620", "detail": "MAC M2 is installed, the model is loaded successfully, and the knowledge base file is also uploaded successfully, However, an error will be reported when the question is answered, and the error content is as follows", "id": 298}
{"title": "Provide highlighting for [Source]", "file": "2023-06-14.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/621", "detail": "In the specific source, the relevant content is highlighted, without context." , "id": 299}
{"title": "[BUG] CPU runs cli_demo.py, does not answer, hangs", "file": "2023-06-14.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/622", "detail": "No GPU; ubuntu machine with 32G RAM. ", "id": 300}
{"title": "After deleting a document in the knowledge base, the content of the deleted document will still be returned when the LLM knowledge base is in conversation", "file": "2023-06-14.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/623", "detail": "As the title suggests, After the vue front-end successfully deletes the A.txt of the document in the knowledge base, and fails to delete the document in the faiss index, the LLM will still return the content of the A.txt, and the A.txt is used as the source, and the deletion effect cannot be achieved", "id": 301}
{"title": "[BUG] Call the knowledge base for Q&A, the video memory will always be superimposed", "file": "2023-06-14.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/625", "detail": "14G video memory, the chatglm-6b-int8 model called, When the knowledge base is Q&A, the video memory will explode after a maximum of four questions, and the video memory will be increased every time you observe the usage of the video memory, and the video memory will be increased every time you use it, is this normal? Is there any configuration that needs to be turned on to fix this? For example, a knowledge base Q&A is performed to clear the memory of the last question?", "id": 302}
{"title": "[BUG] Web page rebuild database failed, resulting in the original uploaded database is gone", "file": "2023-06-14.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/626", "detail": "web page rebuild database failed, resulting in The original uploaded database is gone", "id": 303}
{"title": "Running webui.py on CPU Error message Tensor on device cpu is not on the expected device meta!", "file": "2023-06-14.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/627."} ", "detail": "Running python on CPU webui.py can start, but at the end there is: RuntimeError: Tensor on device cpu is not on the expected device meta!", "id": 304}
{"title": "OSError: [WinError 1114] Dynamic Link Library (DLL) initialization routine failed. Error loading \"E:\\xxx\\envs\\langchain\\lib\\site-packages\\torch\\lib\\caffe2_nvrtc.dll\" or one of its dependencies. ", "file": "2023-06-14.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/629", "detail": "**Problem Description**", "id": 305}
{"title": "[BUG] If the WEBUI deletes a knowledge base document, it will cause the knowledge base Q&A to fail", "file": "2023-06-15.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/632", "detail": "As questioned, select the file you want to delete from the existing files in the knowledge base, After clicking Delete, enter the content in the Q&A box and press Enter to report an error", "id": 306}
{"title": "In the updated version, delete the files in the knowledge base, and then ask the question with an error error", "file": "2023-06-15.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/634", "detail": "For the updated version, a problem is identified, the process is as follows:" , "id": 307}
{"title": "I've configured my environment and want to implement Q&A for my local knowledge base?"} But it returned to me", "file": "2023-06-15.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/637", "detail": "There is no summary, only the reply of relevance, but I look at the presentation in the demo, the reply can be summarized, I go to query the code", "id": 308}
{"title": "[BUG] NPM run dev can not successfully start the VUE frontend", "file": "2023-06-15.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/638", "detail": "** Problem Description**", "id": 309}
{"title": "[BUG] Concise description of the issue", "file": "2023-06-15.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/639", "detail": "**Problem description / Problem Description**", "id": 310}
{"title": ""I mentioned a bug in model loading, I fixed it in the screenshot, you can take a look at it when you have time."} , "file": "2023-06-15.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/642", "detail": "! [model_load_bug] (https://github.com/imClumsyPanda/langchain-ChatGLM/assets/59411575/4432adc4-ccdd-45d9-aafc-5f2d1963403b)", "id": 311}
{"title": "[Help] about setting the embedding model path", "file": "2023-06-16.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/643", "detail": "As the title suggests, I ran successfully once before, However, due to the loss of the environment and reconfiguration, the re-running of WebUI always yields an error", "id": 312}
{"title": "Can the model after Lora fine-tuning be used directly", "file": "2023-06-16.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/646", "detail": "Look model_config.py there is a USE_LORA this parameter, However, it is not used in cli_demo.py and webui.py, and the model has no fine-tuning effect after actual testing, I want to ask if this function is implemented now", "id": 313}
{"title": "Does the load_file.txt write_check_file generated in the tmp_files directory need to be retained all the time, it takes up a lot of space, and can it be deleted after the index is created", "file": "2023-06-16.06", "url": " https://github.com/imClumsyPanda/langchain-ChatGLM/issues/647", "detail": "Feature Description", "id": 314}
{"title": "[BUG] /local_doc_qa/list_files?knowledge_base_id=testDelete KB bug", "file": "2023-06-16.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/649", " detail": "1. Create a new test knowledge base and upload the file (done on the vue front-end and check the back-end and found that the test folder and the following content and vec_store were indeed generated", "id": 315}
{"title": "[BUG] vue webui failed to load the knowledge base", "file": "2023-06-16.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/650", "detail": "Pulled the latest code, ran the backend api and frontend web respectively, click the knowledge base."} , always only simple can be displayed, unable to load knowledge base", "id": 316}
{"title": "Can't I load a MOSS model locally?"} "file": "2023-06-16.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/652", "detail": "How to manually download the model settings local_model_path path still prompts missing files?" , "id": 317}
{"title": "macOS M2 Pro Docker installation failed", "file": "2023-06-17.06", "URL": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/654", "detail": "macOS M2 Pro Docker installation failed", "ID": 318}
{"title": " [BUG] mac m1 pro running tips zsh: segmentation fault", "file": "2023-06-17.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/655", "detail": "Run: python."} webui.py", "id": 319}
{"title": "Installation requirements error", "file": "2023-06-17.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/656", "detail": "(langchainchatglm) D:\\github\\ langchain-ChatGLM>pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple/", "id": 320}
{"title": "[BUG] AssertionError", "file": "2023-06-17.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/658", "detail": "**Problem Description**", "id."} ": 321}
{"title": "[FEATURE] Does AMD win10 on-premise support?"} , "file": "2023-06-18.06", "url": "https://github.com/imClumsyPanda/langchain-ChatGLM/issues/660", "detail": "**Feature Description**", "id": 322}
