,title,file,url,detail,id
0,How the performance is optimized,2023-04-04.00,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/14,"As shown in the figure, after combining the README.md of the project with the project, the answer effect is not ideal, so what can be optimized?",0
1,How to make the model answer strictly based on the retrieved data and reduce nonsense answers?,2023-04-04.00,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/15,For example:,1
2,"When I try to run the `python knowledge_based_chatglm.py`, I got this error in macOS(M1 Max, OS 13.2)",2023-04-07.00,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/32,```python,2
3,How to change the new boss to AMD graphics card or CPU?,2023-04-10.00,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/48,Just remove .cuda().,3
4,"The output of the answer takes a long time, is it possible to store the text vectorization part in advance?",2023-04-10.00,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/50,GPU: 4090 24G VRAM,4
5,Use 'repo_type' argument if needed.,2023-04-11.00,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/57,Traceback (most recent call last):,5
6,Unable to open gradio's page,2023-04-11.00,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/58,$ python webui.py,6
7,"Support word,Is the picture in word displayed normally?",2023-04-12.00,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/60,"As the title suggests, I just turned from next door, and I want to know about it first",7
8,detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.,2023-04-12.00,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/63,"To be able to run normally, when loading a file in the content folder, each file will be prompted:",8
9,"When WebUI is running on the CPU, an error is reported at STEP3 ASKING",2023-04-12.00,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/66,"The web is running, the files are loading normally, and an error is reported when asking",9
10,It is advisable to get a plug-in system,2023-04-13.00,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/67,"For example, the stable-diffusion-webui can install plug-ins, and then open a repository for users or plug-in development, store or download plug-ins.",10
11,Excuse me about the error loading the model!?,2023-04-13.00,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/75,AttributeError: module 'transformers_modules.chatglm-6b.configuration_chatglm' has no attribute 'ChatGLMConfig,11
12,"When retrieving content from local knowledge, is it possible to set a similarity threshold, and content less than this threshold will not be returned, even if it will be less than the set VECTOR_SEARCH_TOP_K parameters? Thank you, big guy",2023-04-13.00,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/76,"For example, ask questions that are not related to the local knowledge base, such as "hello/who are you".",12
13,How do I change it to Doka reasoning?,2023-04-13.00,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/77,+1,13
14,Can you get a lazy bag that you can experience with one click?,2023-04-13.00,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/78,Can you get a lazy bag that you can experience with one click?,14
15,Asking questions continuously will cause crashes,2023-04-13.00,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/79,"It does not seem to be a problem with the explosion of memory, but after asking the question continuously, the following error will appear",15
16,AttributeError: 'NoneType' object has no attribute 'as_retriever',2023-04-14.00,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/86,"Environment: Windows 11, Anaconda/Python 3.8",16
17,FileNotFoundError: Could not find module 'nvcuda.dll' (or one of its dependencies). Try using the full path with constructor syntax.,2023-04-14.00,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/87,Please check if there is an installation issue with CUDA or CUDNN,17
18,Failed to load txt file?,2023-04-14.00,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/89,! [JppHrGOWFa] (https://user-images.githubusercontent.com/109277248/232009383-bf7c46d1-a01e-4e0a-9de6-5b5ed3e36158.jpg),18
19,NameError: name 'chatglm' is not defined,2023-04-14.00,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/90,"This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces",19
20,Can't open the address?,2023-04-14.00,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/91,The error data is as follows:,20
21,There was an error loading the md file,2023-04-14.00,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/98,"After running webui.py, I can access the page, and after uploading an MD file, there is an error in the log. After waiting, the loading can be completed, and the prompt is ready to ask a question, but the question is not answered, and there is an error in the log. The specific logs are as follows.",21
22,It is advisable to increase the ability to acquire online knowledge,2023-04-15.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/101,It is advisable to increase the ability to acquire online knowledge,22
23,txt failed to load successfully,2023-04-15.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/103,hinese. Creating a new one with MEAN pooling.,23
24,The PDF failed to load,2023-04-15.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/105,"e:\a.txt loading successfully,e:\a.pdf loading failed,The first few pages of the pdf file are pictures,The back is text,Loading failure did not report more errors,How to troubleshoot?",24
25,All the time at the place where the text loads,2023-04-15.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/108,All the time at the place where the text loads,25
26,"File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/modeling_chatglm.py", line 440, in forward     new_tensor_shape = mixed_raw_layer.size()[:-1] + ( TypeError: torch. Size() takes an iterable of 'int' (item 2 is 'float')",2023-04-17.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/113,Follow the latest code and discover,26
27,Will the front-end separation feature be provided in the future?,2023-04-17.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/114,Something like this https://github.com/lm-sys/FastChat/tree/main/fastchat/serve,27
28,An error is reported during the installation of dependencies,2023-04-17.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/115,(test) C:\Users\linh\Desktop\langchain-ChatGLM-master>pip install -r requirements.txt,28
29,Asking specific questions will explode the video memory,2023-04-17.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/117,It's okay to ask questions normally.,29
30,Expecting value: line 1 column 1 (char 0),2023-04-17.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/118,"After running, the first step of loading the configuration keeps getting an error:",30
31,"Embedding https://huggingface.co/GanymedeNil/text2vec-large-chinese/tree/main is free, how does it compare to OpenAI's?",2023-04-17.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/119,-------------------------------------------------------------------------------,31
32,What is the error that runs on Colab.,2023-04-17.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/120,libcuda.so.1: cannot open shared object file: No such file or directory,32
33,"I just want to have a conversation with my own LoRa fine-tuned model, and I don't want to load any local documents, how can I adjust it?",2023-04-18.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/121,Can you come up with a separate tutorial?,33
34,"Running on local URL: http://0.0.0.0:7860 To create a public link, set 'share=True' in 'launch()'.  I can't access it on my browser???",2023-04-18.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/122,(chatglm20230401) root@autodl-container-e82d11963c-10ece0d7:~/autodl-tmp/chatglm/langchain-ChatGLM-20230418# python3.9 webui.py,34
35,Consult with you about the error message in the on-premises deployment,2023-04-18.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/124,"Hello, in the process of running langchain-chatGLM locally, the environment and the dependent packages have met the conditions, but running webui.py, the error is as follows (the error is similar to cli_demo.py running), what is wrong? Looking forward to your reply, thank you!",35
36,Error. The dtype of attention mask (torch.int64) is not bool,2023-04-18.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/131,The dtype of attention mask (torch.int64) is not bool,36
37,"[HELP] pip install -r requirements.txt when I get the following error... There's a big guy to help see how to do it.,The package in the next release.",2023-04-18.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/134,$ pip install -r requirements.txt,37
38,How to improve the accuracy of searching for corresponding knowledge based on questions,2023-04-19.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/136,"The biggest problem with the backlink knowledge base is that the problem is short text, and the knowledge is medium and long text. How to accurately search for the corresponding knowledge according to the problem is the biggest problem. This kind of localization project is not like Baidu, which consists of countless web pages, and basically every question can find the corresponding page.",38
39,"Is it possible to increase the threshold setting of vector recall, some recall content is too low, causing the model to babble",2023-04-20.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/140,Title,39
40,Input length issues,2023-04-20.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/141,Thanks to the authors for supporting the ptuning fine-tuning model.,40
41,How do I access chatGLM-6b through an interface?,2023-04-20.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/144,How to access the existing deployed chatGLM-6b through the interface instead of reloading a model;,41
42,"After the web_demo.py is executed, Killed is displayed, and it exits, is it not configured?",2023-04-20.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/146,! [Picture] (https://user-images.githubusercontent.com/26102866/233256425-c7aab999-11d7-4de9-867b-23ef18d519e4.png),42
43,Execute a Python cli_demo1.py,2023-04-20.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/147,Traceback (most recent call last):,43
44,Error: ImportError: cannot import name 'GENERATION_CONFIG_NAME' from 'transformers.utils',2023-04-20.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/149,(mychatGLM) PS D:\Users\admin3\zrh\langchain-ChatGLM> python cli_demo.py,44
45,"When you upload a file and load the knowledge base, temporary files keep appearing",2023-04-21.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/153,Environment: Ubuntu 18.04,45
46,"After adding a file to the knowledge base, clicking Upload File and Load Knowledge Base, a Segmentation fault error is reported.",2023-04-23.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/161,The prompt after running the service is as follows:,46
47,langchain-serve integration,2023-04-24.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/162,"Hey, I'm dev from [langchain-serve](https://github.com/jina-ai/langchain-serve)!",47
48,"Guys,How to configure UBUNTU of WSL to accelerate with CUDA,After installing and running, I found that the CPU was running",2023-04-24.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/164,"Guys,How to configure UBUNTU of WSL to accelerate with CUDA,After installing and running, I found that the CPU was running",48
49,Docker ran error in github codespaces,2023-04-24.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/165,docker run -d --restart=always --name chatglm -p 7860:7860 -v /www/wwwroot/code/langchain-ChatGLM:/chatGLM  chatglm,49
50,There are plans to integrate the Moss model,2023-04-24.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/166,"The follow-up test will be carried out, and at present, it is mainly optimizing part of the effect of langchain, if you are interested, you are also welcome to submit a PR",50
51,How do I deploy APIs?,2023-04-24.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/168,"Is there a way to use fastAPI to implement API deployment methods, and how to implement them?",51
52,'NoneType' object has no attribute 'message_types_by_name' error,2023-04-24.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/169,_HISTOGRAMPROTO = DESCRIPTOR.message_types_by_name['HistogramProto'],52
53,Can I specify the text2vector model I trained?,2023-04-25.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/172,"Excuse me, big guy:",53
54,Questions about the model supported by the project and the potential impact of quantization_bit,2023-04-26.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/176,Hello author~,54
55,Run python3.9 api.py WARNING: You must pass the application as an import string to enable 'reload' or 'workers'.,2023-04-26.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/179,"api.py At the bottom of the file, try it like this:",55
56,ValidationError: 1 validation error for HuggingFaceEmbeddings model_kwargs   extra fields not permitted (type=value_error.extra),2023-04-26.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/180,ValidationError: 1 validation error for HuggingFaceEmbeddings,56
57,"If no high correlation is found, answer "I don't know"",2023-04-26.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/181,If you use the design system_template to get the model to answer "I don't know" when the documents searched are not very relevant,57
58,"If I can't connect to the Internet, where do I need to put files like 6B uploaded from the local computer?",2023-04-26.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/182,"Thanks to the big guy for the project, it's very inspiring~",58
59,"Knowledge base Q&A--If you enter a new knowledge base name in Chinese, an error will be reported",2023-04-27.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/184,"Knowledge base Q&A--If the name of the new knowledge base is Chinese, an error will be reported, and the previously added knowledge base will not be displayed if you select the knowledge base to load",59
60,"Is it now possible to return a paragraph directly from a document with the similarity value that the question matches, without going through the model? Because some of the answers are in the document, the model answers on its own, and cannot answer the answers in the document",2023-04-27.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/186,"Is it now possible to return a paragraph directly from a document with the similarity value that the question matches, without going through the model? Because some of the answers are in the document, the model answers on its own, and cannot answer the answers in the document. That is to say, a strategy combining vector retrieval answers + model answers is provided. If the similarity value is higher than a certain value, the text in the document is returned directly, and the answer to the model or not known is returned if it is not higher",60
61,"TypeError: The type of ChatGLM.callback_manager differs from the new default value; if you wish to change the type of this field, please use a type annotation",2023-04-27.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/188,"Mac running python3 ./webui.py reports TypeError: The type of ChatGLM.callback_manager differs from the new default value; if you wish to change the type of this field, please use a type annotation",61
62,Not Enough Memory,2023-04-27.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/190,"Running the command line program python cli_demo.py, the pdf file has been loaded successfully, and the error "DefaultCPUAllocator: not enough memory: you tried to allocate 458288380900 bytes" is displayed, where can I configure default memory?",62
63,Participate in development issues,2023-04-27.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/191,1. Whether you need to join a special development group,63
64,The format of the code snippet in the dialog needs to be improved,2023-04-27.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/192,"It would be nice to improve the format of the output code snippet, which is not friendly at the moment.",64
65,Is it possible to support Belle in the future?,2023-04-28.01,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/195,"As the title suggests, thank you, big guy",65
66,TypeError: cannot unpack non-iterable NoneType object,2023-04-28.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/200,"When i tried to change the knowledge vector store through `init_knowledge_vector_store`, the error `TypeError: cannot unpack non-iterable NoneType object` came out.",66
67,Generate results,2023-04-28.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/202,"Hello, I would like to ask langchain+chatglm-6B to find a similar matching prompt, does it directly return the answer information corresponding to the prompt, or does chatglm-6B optimize the answer by itself on this basis?",67
68,"I get this error under win, ubuntu: attributeerror: 't5forconditionalgeneration' object has no attribute 'stream_chat'",2023-04-29.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/207,"In win, ubuntu. After downloading the model, there is no way to modify the code to execute the local model, and the path has to be re-entered every time; The LLM model and Embedding model support are also under the official website, and can be used under other projects (wenda).",68
69,[FEATURE] knowledge_based_chatglm.py: renamed or missing?,2023-04-30.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/210,"Not found. Was it renamed? Or, is it missing? How can I get it?",69
70,sudo apt-get install -y nvidia-container-toolkit-base is executed,2023-05-01.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/211,**Problem Description**,70
71,"The effect is not good, and it is almost impossible to answer",2023-05-01.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/212,A docx file of 50 questions and answers is provided,71
72,Is it possible to add a new way to build LangChain based on ChatGPT API calls?,2023-05-02.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/218,"I have two servers with 8G GPU/40G memory, one of which is made into chatglm api; I want to deploy LangChain on another server; There doesn't seem to be a similar code online.",72
73,The computer is Intel's integrated graphics card;  The runtime tells me that I can't find nvcuda.dll and the model can't run,2023-05-02.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/219,"Hello, my computer is an intel integrated graphics card, but the CPU is i5-11400 @ 2.60GHz and the memory is 64G;",73
74,"According to langchain's official documentation and usage patterns, is it possible to change Faiss to Elasticsearch? What additional adjustments will be required? Solving",2023-05-03.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/221,"I am a novice, due to the business model (I have some of my own scenarios and optimizations), I want to use Elasticsearch to do the internal search mechanism of this system, I don't know if it can be replaced, and at the same time, what changes will be involved? Or what other influences may be, I hope the author and the bigwigs will not hesitate to give advice!",74
75,Is it possible to support T5 in the future?,2023-05-04.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/224,Is it possible to support a T5-based model?,75
76,[BUG] memory overflow / torch.cuda.OutOfMemoryError:,2023-05-04.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/229,**Problem Description**,76
77,Error No module named 'chatglm_llm',2023-05-04.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/230,"Obviously the package has been installed, but it can't be hung in python",77
78,Can you produce a description of the API deployment?,2023-05-04.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/233,**Feature Description**,78
79,Error using docs/API.md,2023-05-04.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/234,"Using the API.md document 2 method, an error occurred",79
80,Error loading PDF document?,2023-05-05.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/238,ew one with MEAN pooling.,80
81,"The uploaded local knowledge file cannot be displayed after being uploaded again, only one is displayed successfully, and the other ones are refreshed again after the upload is successful",2023-05-05.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/239,"Hello, the project is very inspiring, thank you~",81
82,"A new virtual environment is created, the package is installed, and the model is automatically downloaded, but it still appears: OSError: Unable to load weights from pytorch checkpoint file for",2023-05-05.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/240,! [78ac8e663fdc312d0e9d78da95925c4] (https://user-images.githubusercontent.com/34124260/236378728-9ea4424f-0f7f-4013-9d33-820b723de321.png),82
83,[BUG] The data does not load,2023-05-05.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/243,The following error is reported in the .txt format used and encoded in UTF-8:,83
84,Cannot read pdf,2023-05-05.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/244,Pls is it webui or cli_demo,84
85,"The local txt file has 500M, and it is slow to load, how to improve the speed?",2023-05-06.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/251,! [yayRzxSYHP] (https://user-images.githubusercontent.com/109277248/236592902-f5ab338d-c1e9-43dc-ae16-9df2cd3c1378.jpg),85
86,"[BUG] gradioAfter uploading the knowledge base and refreshing, the knowledge base is gone, and the previously uploaded knowledge base can only be seen when restarted",2023-05-06.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/253,"After gradio uploads the knowledge base and refreshes it, the knowledge base is gone, and only by restarting can you see the previously uploaded knowledge base",86
87,"[FEATURE] Can OpenAI's models be supported? For example, GPT-3, GPT-3.5, GPT-4; embedding adds text-embedding-ada-002",2023-05-06.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/254,**Feature Description**,87
88,[FEATURE] Could it be possible to add support for milvus vector database / Concise description of the feature,2023-05-06.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/256,**Feature Description**,88
89,Is there a difference in accuracy between CPU and GPU in addition to speed?,2023-05-06.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/259,There is no difference in theory,89
90,"M1, how do you see if MPS or CPU is used when generating the answer?",2023-05-06.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/260,"M1, how do you see if MPS or CPU is used when generating the answer?",90
91,The knowledge base is gone as soon as it is refreshed,2023-05-07.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/263,"After the knowledge base is uploaded, the refresh is gone",91
92,On-premises deployment reports no model,2023-05-07.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/267,It is recommended that you download the LLM and Embedding models to the local computer and write the local storage path of the model in Configs/model_config before running the model,92
93,[BUG] python3: can't open file 'webui.py': [Errno 2] No such file or directory,2023-05-08.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/269,**Problem Description**,93
94,Module missing hints,2023-05-08.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/271,"Because I already have my own docker environment, I can start webui.py directly, prompting",94
95,"After running api.py, curl -X POST "http://127.0.0.1:7861" error is reported?",2023-05-08.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/272,"Run the curl -X POST "http://127.0.0.1:7861" \ -H 'Content-Type: application/json' \ -d '{"prompt": "Hello", "history": []}' command to resolve the error",95
96,[BUG] colab installation requirements prompts protobuf version problem?,2023-05-08.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/273,pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.,96
97,What method is used to calculate the vector similarity in the project?,2023-05-08.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/275,Basically according to the FAISS.similarity_search_with_score_by_vector in langchain,97
98,"[BUG] After installing detectron2, the pdf cannot be loaded",2023-05-08.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/276,**Problem Description**,98
99,[BUG] The ChatYuan-V2 model cannot be streamed and an error will be reported,2023-05-08.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/277,"On the one hand, it seems that ChatYuan itself does not support stream_chat, and someone raised an issue on clueai's side, and they said that it has not been developed yet, so it is estimated that this attribute cannot be adjusted; But on the other hand, it seems that the T5 model itself is not a decoder-only model, so it can't be streamed (personal understanding)",99
100,[BUG] Unable to load text2vec model,2023-05-08.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/278,**Problem Description**,100
101,Can I add a web search function?,2023-05-08.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/281,Can I add a web search function?,101
102,"[FEATURE] structured data sql, excel, csv, when will it be supported.",2023-05-08.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/283,**Feature Description**,102
103,TypeError: ChatGLM._call() got an unexpected keyword argument 'stop',2023-05-08.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/284,No sentence-transformers model found with name D:\DevProject\langchain-ChatGLM\GanymedeNil\text2vec-large-chinese. Creating a new one with MEAN pooling.,103
104,Some bugs and design logic questions about api.py?,2023-05-09.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/285,"First of all, I would like to ask, is this api.py really okay after the developers test it on their own computers?",104
105,"Is there a rented computing power platform, after running api.py, the browser http://localhost:7861/ reports an error.",2023-05-09.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/287,Does this problem occur on rented GPU platforms???,105
106,Is there a way to cut paragraphs in a document?,2023-05-09.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/288,Is the document cutting method in text_load used? It doesn't seem to be used in the code?,106
107,Error raise ValueError(f"Knowledge base {knowledge_base_id} not found") ValueError: Knowledge base ./vector_store not found,2023-05-09.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/289,"File "/root/autodl-tmp/chatglm/langchain-ChatGLM-master/api.py", line 183, in chat",107
108,Can I access the Vikuna model?,2023-05-09.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/290,"At present, there is already a local Vikuna model, can I directly access it?",108
109,[BUG] Questions related to the question formula have a high probability of exploding the video memory,2023-05-09.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/291,**Problem Description**,109
110,"Failed to install pycocotools, and I couldn't solve it after looking for many methods.",2023-05-10.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/292,**Problem Description**,110
111,PyTorch is installed with the CPU version using requirements,2023-05-10.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/294,"For example, use requirements to install, PyTorch is installed with the CPU version, and when running the program, it also uses the CPU to work.",111
112,Can you give a tutorial on the deployment of a blank server,2023-05-10.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/298,"Development and Deployment" can be used as a server deployment tutorial.,112
113,Error(s) in loading state_dict for ChatGLMForConditionalGeneration:,2023-05-10.02,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/299,"If there is a problem during operation, the port page of 7860 cannot be displayed, please ask for help.",113
114,The ChatYuan-large-v2 model failed to load,2023-05-10.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/300,**Actual Result**,114
115,The summary feature is added,2023-05-10.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/303,"Hello, will you consider adding the functions of reasoning and speech understanding for long text messages in the future? For example, generating summaries",115
116,[BUG] pip install -r requirements.txt error,2023-05-10.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/304,pip install langchain -i https://pypi.org/simple,116
117,[BUG] An error is reported when uploading a knowledge base file,2023-05-10.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/305,! [19621e29eaa547d01213bee53d81e6a] (https://github.com/imClumsyPanda/langchain-ChatGLM/assets/84606552/7f6ceb46-e494-4b0e-939c-23b585a6d9d8),117
118,[BUG] AssertionError: <class 'gradio.layouts.Accordion'> Component with id 41 not a valid input component.,2023-05-10.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/306,**Problem Description**,118
119,[BUG] CUDA out of memory with container deployment,2023-05-10.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/310,**Problem Description**,119
120,[FEATURE] Added the function of fine-tuning training,2023-05-11.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/311,**Feature Description**,120
121,"How to use multi-card deployment, multiple GPUs",2023-05-11.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/315,"There are multiple GPUs on the machine, how to use them all",121
122,What is the relationship between this knowledge base Q&A and chatglm?,2023-05-11.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/319,"This knowledge base Q&A, which part is associated with ChatGPT, is there no ChatGPT, and the knowledge base Q&A can also be brought out alone",122
123,[BUG] Error message is reported when running ImportError: libcudnn.so.8: cannot open shared object file: No such file or directory,2023-05-12.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/324,Problem Description: raceback (most recent call last):,123
124,"The webui is successfully started, but an error is reported",2023-05-12.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/325,**Problem Description**,124
125,An error message is reported when switching MOSS,2023-05-12.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/327,"Danshi but in the published source code,",125
126,Can the Vicuna model be accessed?,2023-05-12.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/328,"Hello! For both the MOSS model and the vicuna model, AutoModelForCausalLM loads the model, but a slight change (model paths) will result in this error. What is the cause of this error",126
127,"Hello, is it okay to run on Alibaba Cloud CPU server? What is the ideal CPU configuration if so?",2023-05-12.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/330,"Hello, is it okay to run on Alibaba Cloud CPU server? What is the ideal CPU configuration if so?",127
128,"Hello, can an 8-core 32g CPU run multiple rounds of conversations?",2023-05-12.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/331,What kind of CPU configuration is better? I currently want to deploy multiple rounds of conversations under the CPU?,128
129,"[BUG] There is an error in the system when entering more than 10,000 characters in the chat",2023-05-12.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/332,"The system error occurs when entering more than 10,000 characters in the chat, as shown in the following figure:",129
130,Can I increase the deployment of multi-user access interfaces for APIs?,2023-05-12.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/333,"The default deployer only supports single-user access, and multi-user access needs to be queued. I've tested several related Github multi-user projects, but some of them still don't meet the requirements. This section describes how to implement the deployment interface of ChatGLM for multiple users to access ChatGLM at the same time, including http, websocket (stream), and web page.",130
131,Multi-card deployment,2023-05-12.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/334,How to improve concurrency with a single-node multi-card or multi-machine multi-card deployment model with FastAPI,131
132,CAN WEBUI SPECIFY A KNOWLEDGE BASE DIRECTORY?,2023-05-12.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/335,**Feature Description**,132
133,[BUG] Cannot read properties of undefined (reading 'error'),2023-05-12.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/336,**Problem Description**,133
134,[BUG] 1 validation error for HuggingFaceEmbeddings model_kwargs extra fields not permitted.,2023-05-12.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/337,Problems with the model after loading to 100%:,134
135,"The upload knowledge base needs to be restarted, can it be fixed?",2023-05-12.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/338,Quite a serious problem,135
136,"[BUG] 4 v100 cards burst the video memory, and the same is true in LLM session mode",2023-05-12.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/339,**Problem Description**,136
137,Configure different TextSpliters for uploaded files,2023-05-12.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/341,"1. The current ChineseTextSpliter sharding is not friendly to English, especially code files, and limits the fixed length; Leads to unsatisfactory results in conversations",137
138,"[FEATURE] Can I add Bloom models in the future? According to Oracle Bone's test, this series of Chinese reviews works well",2023-05-13.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/346,**Feature Description**,138
139,[BUG] v0.1.12 Boot after packing image webui.py Failed / Concise description of the issue,2023-05-13.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/347,**Problem Description**,139
140,An error is reported when switching MOSS models,2023-05-13.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/349,"I asked yesterday, saying that the transformers version is wrong, and I need 4.30.0, and I found that there is no such version, and today I updated it to 4.29.1, and it still reports an error, and the error is as follows",140
141,[BUG] Failed to load pdf document,2023-05-13.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/350,**Problem Description**,141
142,"It is suggested that a wave of comments can be enhanced at a later stage, which will also help more people follow up with the PR",2023-05-13.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/351,"Knowing that the author and the team are frantically updating and reviewing the code, it is only suggested that the core code can be supplemented with some comments after the subsequent stabilization, so as to help more people understand the ideas of the authors of each module and propose better optimizations.",142
143,[FEATURE] MOSS Quant Edition support,2023-05-13.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/353,**Feature Description**,143
144,[BUG] moss model fails to load,2023-05-13.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/356,**Problem Description**,144
145,[BUG] There is a bug in the load_file function in load_doc_qa.py,2023-05-14.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/358,The original function is:,145
146,"[FEATURE] API mode, knowledge base loading optimization",2023-05-14.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/359,"As the title suggests, in the current version, every time the local knowledge base interface is called, the knowledge base will be loaded once, is there a better way?",146
147,"After running the Python api.py script backend deployment, how do I use the curl command to invoke it?",2023-05-15.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/361,"In other words, I want to make a conversational bot now, and I want to co-debug with the company's front-end and back-end? How to call each other with the front and back ends? Private message, paid answer!!",147
148,"The upload knowledge base needs to be restarted, can it be fixed?",2023-05-15.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/363,"The upload knowledge base needs to be restarted, can it be fixed?",148
149,[BUG] pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple,2023-05-15.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/364,My python is 3.8.5,149
150,pip install gradio error,2023-05-15.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/367,The big guy help me,150
151,[BUG] pip install gradio keeps getting stuck,2023-05-15.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/369,! [aba82742dd9d4d242181662eb5027a7] (https://github.com/imClumsyPanda/langchain-ChatGLM/assets/84606552/cd9600d9-f6e7-46b7-b1be-30ed8b99f76b),151
152,[BUG] Concise description of the issue,2023-05-16.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/370,"The initial load of the local knowledge base is successful, but after the question is asked, the local knowledge base cannot be rewritten and loaded",152
153,[FEATURE] Concise description of the feature,2023-05-16.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/371,**Feature Description**,153
154,"On Windows, the model file is installed by default",2023-05-16.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/372,-------------------------------------------------------------------------------,154
155,[FEATURE] takes into account conversation management,2023-05-16.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/374,How to take into account conversation management in the case of knowledge base search?,155
156,llm device: cpu embedding device: cpu,2023-05-16.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/376,**Problem Description**,156
157,[FEATURE] What separator can be used to separate the knowledge points of the function / text file?,2023-05-16.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/377,**Feature Description**,157
158,[BUG] Failed to upload file: PermissionError: [WinError 32] Another program is using this file and the process is inaccessible.,2023-05-16.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/379,**Problem Description**,158
159,[BUG] Python api.py error is reported,2023-05-16.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/383,error message,159
160,model_kwargs extra fields not permitted (type=value_error.extra),2023-05-16.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/384,"Hello everyone, have you encountered this?",160
161,[BUG] Concise description of the issue,2023-05-17.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/385,ls1 = [ls[0]],161
162,[FEATURE] performance optimization,2023-05-17.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/388,**Feature Description**,162
163,"[BUG] Moss model Q&A, RuntimeError: probability tensor contains either inf, nan or element < 0",2023-05-17.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/390,**Problem Description**,163
164,Does anyone know that the v100GPU's 32G video memory will report an error? Is it V100GPU supported?,2023-05-17.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/392,**Problem Description**,164
165,A superficial solution to encoding problems such as 'gbk' codec can't encode character '\xab' in position 14: illegal multibyte sequence,2023-05-17.03,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/397,**Feature Description**,165
166,Could not import sentence_transformers python package. Please install it with `pip install sentence_transformers`.,2023-05-18.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/400,**Problem Description**,166
167,Model Q&A and retrieval Q&A are supported,2023-05-18.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/401,Different queries should have different answers based on intent.,167
168,"When splitting text, can it be split according to each line of the txt file, that is, according to the line break symbol\n???",2023-05-18.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/403,How should the following code be modified?,168
169,The local_doc_qa/local_doc_chat interface response is serial,2023-05-18.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/405,**Problem Description**,169
170,Why can't I find the source but still can't answer the question?,2023-05-18.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/406,! [Picture] (https://github.com/imClumsyPanda/langchain-ChatGLM/assets/3349611/1fc81d61-2409-4330-9065-fdda1a27c86a),170
171,"Excuse me: In the knowledge base test: What is the format of adding a single piece of content, if it is replaced with text import? I've found that adding a single piece of content tests works well.",2023-05-18.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/412,"I've found that 'add a single piece of content' in the knowledge base test, and check 'disable content clause storage', even if 'don't enable contextual association' works very well.",171
172,[BUG] Unable to configure knowledge base,2023-05-18.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/413,**Problem Description**,172
173,[BUG] The access page on the EAS deployed on the Alibaba PAI platform is white,2023-05-19.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/414,**Problem Description**,173
174,"After the API is deployed, call /local_doc_qa/local_doc_chat and return Knowledge base samples not found",2023-05-19.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/416,into the parameters,174
175,[FEATURE] Upload word save as txt file report 'ascii' codec can't decode byte 0xb9 in position 6: ordinal not in range(128),2023-05-20.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/421,Upload a txt file to save as word report,175
176,"If the saved knowledge base does not come out after refreshing, is this knowledge base permanently saved? Is it possible to connect to an external vector knowledge base?",2023-05-21.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/422,"If the saved knowledge base does not come out after refreshing, is this knowledge base permanently saved? Is it possible to connect to an external vector knowledge base?",176
177,"[BUG] Running with colab, unable to load model, error: 'NoneType' object has no attribute 'message_types_by_name'",2023-05-21.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/423,**Problem Description**,177
178,Do I need a vector database? And when do you need to use a vector database?,2023-05-21.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/424,"I am currently using text2vec, do I need to use a vector database? And when do you need to use a vector database?",178
179,HuggingFace model reference issue,2023-05-22.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/427,It seems to have turned into an Error lately?,179
180,"Hello, loading the local txt file has this killed error, the TXT file has a size of about 100M. The reason? Thank you.",2023-05-22.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/429,<img width="677" alt="929aca3b22b8cd74e997a87b61d241b" src="https://github.com/imClumsyPanda/langchain-ChatGLM/assets/109277248/24024522-c884-4170-b5cf-a498491bd8bc" >,180
181,"I would like to ask you how the management of local knowledge is managed. For example, add data through the http API or delete a piece of data",2023-05-22.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/430,"For example, you can add, delete, or modify a piece of data through the http API.",181
182,[FEATURE] Double-column pdf recognition issue,2023-05-22.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/432,"I tried the model, and I feel that the accuracy of single-column PDF recognition is high, but because of the basic OCR technology used, there are many problems in the recognition of some double-column PDF papers, is there any way to improve it?",182
183,"Small problems in the deployment start, the little brother is learning to ask the big guy to answer",2023-05-22.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/433,"1. When python loader/image_loader.py, ModuleNotFoundError: No module named 'configs' is displayed, but python webui.py can still run",183
184,"Whether it can detect the increase of documents in the directory and load documents incrementally, without affecting the foreground conversation, is actually to support read/write splitting. It would be better if I could support querying which documents have been vectorized, delete obsolete documents, etc., thank you.",2023-05-22.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/434,**Feature Description**,184
185,"[BUG] Concisely explain the problem / CUDA error under windows, please use https://github.com/Keith-Hon/bitsandbytes-windows.git",2023-05-22.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/435,pip install git+https://github.com/Keith-Hon/bitsandbytes-windows.git,185
186,"[BUG] from commit 33bbb47,  Required library version not found: libbitsandbytes_cuda121_nocublaslt.so. Maybe you need to compile it from source?",2023-05-23.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/438,**Problem Description**,186
187,"[BUG] Concise description of the issue Upload a 60m txt file with an error, showing a timeout, is there a limit to the size of the file that can be uploaded",2023-05-23.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/439,"ERROR 2023-05-23 11:13:09,627-1d: Timeout reached while detecting encoding for ./docs/GLM model format data .txt",187
188,[BUG] TypeError: issubclass() arg 1 must be a class,2023-05-23.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/440,**Problem Description**,188
189,"After running Python3 webui.py, it keeps saying "The model is not loaded successfully, please go to the "Model Configuration" tab in the upper left corner of the page and click the "Load Model" button.",2023-05-23.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/441,**Problem Description**,189
190,Whether it can provide import support for web documents,2023-05-23.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/444,"Nowadays, many online documents are used as collaboration tools, so there is a greater need to import online documents through URLs",190
191,[BUG] history index issue,2023-05-23.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/445,"When comparing the history of the dialog box with the history in the chat function of the model, it is found that there is a mismatch, and when passing in llm._call, the index used for history is a bit problematic, resulting in the content of the previous round of dialogue not being input to the model.",191
192,[BUG] moss_llm not implemented,2023-05-23.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/447,"Some methods are not supported, such as history_len",192
193,How does langchain-ChatGLM delete the data of a local knowledge base?,2023-05-23.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/448,"For example, a user has just submitted an incorrect piece of data to the local knowledge base, and now how can it be found from the local knowledge base and delete it.",193
194,[BUG] Concisely elaborate the problem / UnboundLocalError: local variable 'resp' referenced before assignment,2023-05-24.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/450,"In the latest version of the code, the above error (UnboundLocalError: local variable 'resp' referenced before assignment) is observed in api.py, and the local_doc_qa.llm.generatorAnswer(prompt=question, history=) is observed by debughistory,streaming=True) may not return any value.",194
195,Is there a PROMPT_TEMPLATE that allows the model not to answer sensitive questions?,2023-05-24.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/452,## PROMPT_TEMPLATE issue,195
196,[BUG] The test environment has an incorrect version of Python,2023-05-24.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/456,**Problem Description**,196
197,[BUG] The webui is not styled correctly after deployment,2023-05-24.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/458,**Problem Description**,197
198,Issues with configuring the default LLM model,2023-05-24.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/459,**Problem Description**,198
199,It's time to update the autoDL image,2023-05-24.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/460,"As the title suggests, I ran down the image of autoDL and found that it was 4.27, git pull the new version of the code function + the old dependency environment, all kinds of strange problems.",199
200,"[BUG] tag:0.1.13 In cpu mode, I can't run it if I want to use the local model, and there are various path parameter problems",2023-05-24.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/462,-------------------------------------------------------------------------------,200
201,"[BUG] Have any classmates ever encountered this mistake!! This killed error occurs when loading the local txt file, and the TXT file is about 100M in size.",2023-05-25.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/463,Run cli_demo.py. Is the local txt file too big? ABOUT 100M.,201
202,WHETHER THE API VERSION PROVIDES A STREAMING INTERFACE FOR WEBSOCKETS,2023-05-25.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/464,"In the webui version, the streaming output of WS is adopted, and the overall perception response is very fast",202
203,[BUG] Install a bug log,2023-05-25.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/465,"Installed according to the [install document] (https://github.com/imClumsyPanda/langchain-ChatGLM/blob/master/docs/INSTALL.md),",203
204,VUE's pnmp i performs a failed fix - just use the npm i command,2023-05-25.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/466,"Thanks to the author! Great app, a lot of fun to use.",204
205,"Excuse me, does anyone know if cuda11.4 supports it???",2023-05-25.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/467,"Excuse me, does anyone know if cuda11.4 supports it???",205
206,Do you have contextual correlation for question-based search in multiple rounds of Q&A?,2023-05-25.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/468,"In multiple rounds of knowledge base-based Q&A, the first question describes a topic, and the subsequent question description does not contain the keywords of that topic, but there is a contextual association. If you use follow-up questions to search the knowledge base, you may find irrelevant information, resulting in the large model not being able to answer the questions correctly. Does this project need to take this situation into account?",206
207,[BUG] Out of memory issue,2023-05-26.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/470,"I used the local chatglm-6b-int4 model, and then it shows that the memory is insufficient (win10+32G memory+1080ti11G), how much memory is generally needed? How should this bug be fixed?",207
208,[BUG] Failed to install pycocotools in a pure intranet environment,2023-05-26.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/472,**Problem Description**,208
209,[BUG] webui.py Reloading the model results in a KeyError,2023-05-26.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/473,**Problem Description**,209
210,chatyuan is not available,2023-05-26.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/475,**Problem Description**,210
211,"[BUG] There is a bug in the text segmentation model AliTextSplitter, which will put "." as a splitter",2023-05-26.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/476,"There is a bug in the semantic segmentation model of Ali Dharma Academy, which will be set to "." Split as a splitter regardless of contextual semantics. Whether there are any other splitters is unknown. Proposed amendment: Add "." Replace them with other characters uniformly, split them and replace them back. Or add another segmentation model.",211
212,[BUG] RuntimeError: Error in faiss::FileIOReader::FileIOReader(const char*) a,2023-05-27.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/479,**Problem Description**,212
213,"[FEATURE] installation, why does conda create have to specify the path additionally with -p, instead of the default /envs below",2023-05-28.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/481,##**Feature Description**,213
214,"After performing webui.py via Anaconda, the web link cannot be opened",2023-05-28.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/485,"After executing the webui.py command, http://0.0.0.0:7860 does not open after copying to the browser, showing "This website cannot be accessed".",214
215,[BUG] Reloading error after using p-tuningv2,2023-05-29.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/486,"Put the relevant files after p-tunningv2 training into the p-tunningv2 folder, check the use p-tuningv2 point to reload the model, and the console will input an error message:",215
216,"After running webui.py on the server, the web link cannot be opened locally",2023-05-29.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/487,"This project executes on a xxx.xx.xxx.xxx server, and my code on webui.py is (demo",216
217,[FEATURE] Does VisualGLM-6B support?,2023-05-29.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/488,**Feature Description**,217
218,"Hello, let me ask you, when the backend API is deployed, does it support multi-user Q&A at the same time???",2023-05-29.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/489,"If multiple users are supported, how many users can answer questions and answers? It depends on the hardware, right?",218
219,Why is V100GPU video memory is full but the utilization rate is 0?,2023-05-29.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/491,<img width="731" alt="de45fe2b6cb76fa091b6e8f76a3de60" src="https://github.com/imClumsyPanda/langchain-ChatGLM/assets/109277248/c32efd52-7dbf-4e9b-bd4d-0944d73d0b8b" >,219
220,"If I build a product knowledge base within the company and use the INT-4 model, how many video memory servers do I need to configure for a scale of 200 people?",2023-05-29.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/492,"As the title suggests, plan to build an online knowledge base for the company.",220
221,"Hello, may I ask a question, it takes about 20 seconds to reply to the Q&A at present, how to improve the speed? V10032G server.",2023-05-29.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/493,**Problem Description**,221
222,"[FEATURE] HOW TO ACHIEVE A RESULT THAT ONLY MATCHES THE FOLLOWING, NOT THE ABOVE",2023-05-29.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/494,"When building my own knowledge base, it is mainly in the form of question and answer pairs, so the answer I need is the content below my question, but at present, after setting the value of the chunk_size, it matches the content of the context, but I don't actually need the above. I've had to turn up the chunk_size value for a more complete presentation of the following answers, but I don't really need half of the above content. That is, I threw half of the useless things to the prompt, and I didn't find some descriptions of this piece in faiss.py, how can I modify it?",222
223,"Hello, let me ask, I call api.py deployment, why can I use postman to call with ip plus port, but change to change the domain name to use postman can not call?",2023-05-30.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/497,! [5ufBSWxLyF] (https://github.com/imClumsyPanda/langchain-ChatGLM/assets/109277248/70e2fbac-5699-48d0-b0d1-3dc84fd042c2),223
224,Calling the stream_chat in api.py returns Chinese garbled characters in the source_documents.,2023-05-30.04,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/498,-------------------------------------------------------------------------------,224
225,"[BUG] Catch a bug, stream_chat parsing json problem in api.py",2023-05-30.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/501,**Problem Description**,225
226,Windows on-premise deployment encountered an OMP error,2023-05-31.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/502,**Problem Description**,226
227,"[BUG] bug14 ,"POST /local_doc_qa/upload_file HTTP/1.1" 422 Unprocessable Entity",2023-05-31.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/503,"An error is reported in the uploaded file, and an error is returned, api.py",227
228,"Hello, may I ask a question, how to change to multi-threaded call when api.py deploying? Thank you",2023-05-31.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/505,Current api.py scripts don't support multithreading,228
229,"Hello, please advise. api.py When deploying, can you provide the backend churn return result?",2023-05-31.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/507,curl -X 'POST' \,229
230,"Streaming output, streaming interface, using server-sent events technology.",2023-05-31.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/508,"Think like that, https://blog.csdn.net/weixin_43228814/article/details/130063010",230
231,"Planning to add streaming output capabilities? The ChatGLM model calls the slow response time through the api method, and the Fastapi streaming interface solves the puzzles, which can quickly improve the response speed",2023-05-31.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/509,**Problem Description**,231
232,[BUG] ERROR occurred while uploading knowledge base (could not open xxx for reading: No such file or directory),2023-05-31.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/510,**Problem Description**,232
233,api.py Is the script going to increase the SSE streaming output?,2023-05-31.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/511,"curl can detect the first word when called, so as to improve the experience of replying",233
234,"[BUG] Using tornado to implement webSocket, multiple clients can connect at the same time, and stream reply can be realized, but multiple clients are used at the same time, and the answer is very messy, is the model not supporting multi-threading",2023-05-31.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/512,import asyncio,234
235,Does Chinese_alpaca_plus_lora support llama-based,2023-06-01.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/514,"Does Chinese_alpaca_plus_lora support llama-based, https://github.com/ymcui/Chinese-LLaMA-Alpaca this project",235
236,"[BUG] I can read the pdf of the picture now, but the pdf of the text can't be read, what is the situation???",2023-06-01.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/515,**Problem Description**,236
237,"Stuck in the process of reasoning, the process cannot end normally",2023-06-01.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/516,**Problem Description**,237
238,"When curl is called, how can curl pass parameters to achieve multiple rounds of dialogue from the second round?",2023-06-01.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/517,First round of calls:,238
239,Recommended addition of log management capabilities after api.py deployment?,2023-06-01.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/518,-------------------------------------------------------------------------------,239
240,Do any bigwigs know how to deploy api.py scripts with multiple threads?,2023-06-01.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/519,"api.py After deployment, use the following request, the time is slower, it seems to be single-threaded, how to change to multi-threaded deployment api.py:",240
241,[BUG] Uploading files to the knowledge base Any format and content will always fail,2023-06-01.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/520,"When uploading the knowledge base, the txt cannot be parsed, even if it is the sample txt in content/sample, it cannot be parsed, and the uploaded md, pdf, etc. cannot be loaded, and it will continue to wait, and it will not work until it lasts for more than 30 minutes.",241
242,Questions about prompt_template,2023-06-01.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/521,What does this prompt_template mean and how do I use it? Can you give a specific template for reference?,242
243,[BUG] Concise description of the issue,2023-06-01.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/522,**Problem Description**,243
244,Chinese participle full stop processing (about expressing the "." ）,2023-06-02.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/523,"It is recommended to deal with such a participle of 1.26 billion yuan, and it is best not to divide it into 12 and 600 million, which need to be put together",244
245,ImportError: cannot import name 'inference' from 'paddle',2023-06-02.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/526,"I looked around on the Internet, some said to upgrade paddle, I did it or it didn't work, some said to install paddlepaddle, I found the image source of Douban, but the installation error cannot detect archive format",245
246,[BUG] webscoket interface serial issue (/local_doc_qa/stream-chat/{knowledge_base_id}),2023-06-02.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/527,**Problem Description**,246
247,[FEATURE] REFRESHES THE PAGE TO UPDATE THE LIST OF KNOWLEDGE BASES,2023-06-02.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/528,**Feature description and improvements**,247
248,"[BUG] After using ptuning to fine-tune the model, the Q&A effect is not good",2023-06-02.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/530,### ptuning is not called,248
249,[BUG] Multiple rounds of dialogue do not work well,2023-06-02.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/532,"When having multiple rounds of conversations, no matter what history_len is set, it doesn't work well. In fact I set it to a maximum of 10, but in the conversation, I still can't implement multiple rounds of dialogue:",249
250,"RuntimeError: MPS backend out of memory (MPS allocated: 18.00 GB, other allocations: 4.87 MB, max allowed: 18.13 GB)",2023-06-02.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/533,**Problem Description**,250
251,"Please pay attention to this issue! The real use is definitely multi-user concurrent Q&A, hope to add this feature!!",2023-06-02.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/534,It depends on how many graphics cards you have,251
252,How do you use multiple GPUs when starting a project?,2023-06-02.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/535,**How many GPUs do I use to start a project? **,252
253,What is the format of the curl call when using streaming output?,2023-06-02.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/536,What should I fill in the knowledge_base_id in app.websocket("/local_doc_qa/stream-chat/{knowledge_base_id}")(stream_chat)???,253
254,Startup error with local Vicuna-7B model,2023-06-02.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/538,"Environment: Ubuntu 22.04 CUDA 12.1 without NCCL installed, using RTX2080 to compute in parallel with M60 graphics card",254
255,Why don't you call the GPU and call the CPU directly?,2023-06-02.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/539,"My Alibaba Cloud configuration is 16G video memory, and it prompts when I run webui.py with the default code",255
256,Multiple files are overwritten on top of each other when uploaded,2023-06-03.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/541,"1. When uploading multiple files in the same knowledge base, they will overwrite each other, and the knowledge of multiple documents cannot be combined.",256
257,"[BUG] 'gcc' is not an internal or external command/LLM conversation, which can only last one round",2023-06-03.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/542,No compiled kernel found.,257
258,Starting a project in API mode without a list of interfaces with a knowledge base?,2023-06-04.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/544,"How can I get the list of interfaces in the knowledge base? If you don't need to write it yourself, can you provide relevant access methods, thanks",258
259,"When a program is started in API mode, how can I make the interface be called in stream mode?",2023-06-05.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/546,"Hello author, after I started the program in API mode, I found that the interface response time is very long, how can I make the interface be called in stream mode? I'd like to implement something like the answer to the webui pattern",259
260,Regarding the relevance of the data after the table is converted to text in the original text.,2023-06-06.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/547,"Tabular data in the original text is converted to text with (X-Y: value; ... ), but after doing so, I found that the relevance was low and the effect was poor, so what was the best solution?",260
261,"After starting, only the last round of records will be recorded in both the LLM and the knowledge base Q&A mode",2023-06-06.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/548,"When pulling the latest code, when answering a question, only the last Q&A record is displayed on each page, what parameters need to be modified to keep the history?",261
262,Provide a system message configuration so that answers don't go outside the knowledge base,2023-06-06.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/549,**Feature Description**,262
263,[BUG] Error message is reported using p-tunningv2,2023-06-06.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/551,"Follow the instructions in the README to put the p-tunningv2 training file into the p-tunningv2 folder, check the use p-tuningv2 point to reload the model, and the console will prompt an error message:",263
264,"[BUG] mentally retarded, so many problems, I am embarrassed to put it out, a waste of time",2023-06-06.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/553,。。。,264
265,"[FEATURE] I see there's a ali_text_splitter.py in the code file, why not use him as a text splitter?",2023-06-06.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/554,"I see there's a ali_text_splitter.py in the code file, why not use it as a text splitter?",265
266,An error is reported when the document function is loaded,2023-06-06.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/557,"def load_file(filepath, sentence_size=SENTENCE_SIZE):",266
267,"After installing docker by referring to the guide, run the cli_demo.py and prompt killed",2023-06-06.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/558,root@b3d1bd08095c:/chatGLM# python3 cli_demo.py,267
268,"Note: If the installation is wrong, note the version of both packages wandb==0.11.0 protobuf==3.18.3",2023-06-06.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/559,"Error1: If the startup error 'protobuf' is reported, it needs to be updated to 'protobuf==3.18.3 '",268
269,What is the optimization direction of the knowledge base for the less ideal matching of knowledge relevance of long texts?,2023-06-07.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/563,"We may type in an article with 1W words, which involves a lot of angle questions about the topic of the article, and we ask him what to do if the content of his relevance matches is very different from the answer we actually need.",269
270,Can I use curl when using the stream-chat function for streaming output?,2023-06-07.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/565,Why does the following call give an error???,270
271,Have any bigwigs practiced parallel or multi-threaded deployment schemes?,2023-06-07.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/566,+1,271
272,Having trouble with a multithreaded deployment?,2023-06-07.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/567,<img width="615" alt="3d87bf74f0cf1a4820cc9e46b245859" src="https://github.com/imClumsyPanda/langchain-ChatGLM/assets/109277248/8787570d-88bd-434e-aaa4-cb9276d1aa50" >,272
273,[BUG] Loading the vicuna-13b model with fastchat for Q&A of the knowledge base has a token restriction error,2023-06-07.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/569,"When I opened the api service of Fastchat's vicuna-13b, and then the config was configured there (the API was tested locally and the results could be returned), and then the knowledge base was loaded (the knowledge base probably has more than 1000 documents, and chatGLM can be reasoned normally), when the token exceeded the limit during the Q&A, I asked hello;",273
274,"Now when I add a knowledge base, I always report an error when there are too many files, and I don't know which files I loaded, and I don't know whether all of them fail or some of them succeed after the error is reported; I would like to see a function to load a specific folder as a knowledge base",2023-06-07.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/574,**Feature Description**,274
275,[BUG] An error is reported when the moss model is loaded locally,2023-06-08.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/577,The local loading error of the MOSS model is as follows:,275
276,The error message Can't instantiate abstract class MOSSLLM with abstract methods _history_len is reported when loading a local moss model,2023-06-08.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/578,(vicuna) ps@ps[13:56:20]:/data/chat/langchain-ChatGLM2/langchain-ChatGLM-0.1.13$ python webui.py  --model-dir local_models --model moss --no-remote-model,276
277,[FEATURE] Can you increase the control prompt_template on the front-end page? Or can you support the front-end page to choose which prompt to use?,2023-06-08.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/579,"At present, you can only modify one prompt in config, which is troublesome to switch between multiple different scenarios",277
278,"[BUG] Streamlit ui bug, an error will be reported when adding the knowledge base",2023-06-08.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/580,**Problem Description**,278
279,"[FEATURE] Can webui/webui_st support history? Currently, only one conversation is possible",2023-06-08.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/581,"Try it, webui and webui_st don't support history conversations.,You can only talk once.,Can't you turn on all history by default?",279
280,An error occurs when starting python cli_demo.py --model chatglm-6b-int4-qe,2023-06-09.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/585,"After downloading the model and the related dependencies, running 'python cli_demo.py --model chatglm-6b-int4-qe' gives an error:",280
281,An error is reported when rebuilding the knowledge base,2023-06-09.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/586,**Problem Description**,281
282,"[FEATURE] can I block paddle, I don't need OCR, the effect is poor, and the dependency environment is complicated",2023-06-09.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/587,I hope I don't rely on paddle,282
283,question: Can document vectorization be done manually?,2023-06-09.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/589,"The existing company-level data is 500G+, and you need to use this function, how can I manually implement this vectorization, and then load it?",283
284,Can the view front-end perform streaming returns?,2023-06-09.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/590,Can the view front-end perform streaming returns?,284
285,"[BUG]  Load parallel cpu kernel failed, using default cpu kernel code",2023-06-11.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/594,**Problem Description**,285
286,[BUG] Concise description of the issue,2023-06-11.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/595,**Problem Description**,286
287,"I get a KeyError: 'name' error when uploading a local knowledge base, and the local knowledge base is all . txt file, the number of files is about 2000+.",2023-06-12.05,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/597,<img width="649" alt="KError" src="https://github.com/imClumsyPanda/langchain-ChatGLM/assets/59411575/1ecc8182-aeee-4a0a-bbc3-74c2f1373f2d">,287
288,"There is configuration information for the Vikuna-13B-HF model in the model_config.py, but it still doesn't seem to be available?",2023-06-12.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/600,@dongyihua543,288
289,"ImportError: Using SOCKS proxy, but the 'socksio' package is not installed. Make sure to install httpx using `pip install httpx[socks]`.",2023-06-12.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/605,"It should be a proxy problem, but I can't solve it after trying many methods.",289
290,[BUG] similarity_search_with_score_by_vector error in case no match can be found,2023-06-12.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/607,"In the case of setting the match threshold of VECTOR_SEARCH_SCORE_THRESHOLD, the vectorstore will return empty, at which point the above handler will error",290
291,[FEATURE] How do I build an English knowledge base?,2023-06-12.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/609,**Feature Description**,291
292,Who has vicuna weighting? LLAMA after the conversion,2023-06-13.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/611,**Problem Description**,292
293,[FEATURE] Can the API be used to upload folders?,2023-06-13.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/612,"Users are too lazy to select all the files and want to upload a folder, can the API implement this function?",293
294,"After the multi-card deployment, upload a single file as the knowledge base, is the single card used to generate vectors or multi-cards?",2023-06-13.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/614,"At the moment I tested my local multi-card deployment, it seems that the knowledge base vector is still used for single card",294
295,[BUG] python webui.py prompts an illegal instruction,2023-06-13.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/615,(/data/conda-langchain [root@chatglm langchain-ChatGLM]# python webui.py,295
296,Cross-row segmentation of knowledge base files,2023-06-13.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/616,"My knowledge base file txt file is a line of knowledge, with \nbranches.",296
297,[FEATURE] Is there a streaming API for bing?,2023-06-13.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/617,"The web side has this Bing search answer, but the API interface is not found, can the big guy give a hint?",297
298,I hope to come up with a tutorial for the installation of macOS M2,2023-06-14.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/620,"Mac M2 installation, the model is loaded successfully, and the knowledge base file is also uploaded successfully, but an error will be reported when the question and answer is answered, and the error content is as follows",298
299,Provide a highlight for the Source,2023-06-14.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/621,"In the specific source, the relevant content is highlighted, and the text is not included.",299
300,"[BUG] The CPU runs cli_demo.py, does not answer, hangs",2023-06-14.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/622,No GPU; ubuntu machine with 32G RAM.,300
301,"After deleting a document in the knowledge base, the LLM knowledge base will still return the content of the deleted document when it is in conversation",2023-06-14.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/623,"As mentioned in the question, after the vue front-end successfully deletes the A.txt of the document in the knowledge base, but fails to delete the document in the faiss index, the LLM will still return the content of this A.txt, and the A.txt is used as the source, which fails to achieve the deletion effect",301
302,"[BUG] Call the knowledge base for Q&A, and the video memory will always be stacked",2023-06-14.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/625,"14G video memory,Called chatglm-6b-int8 model,When the knowledge base questions and answers,Up to four questions and answers, the video memory will explode,Observe the video memory usage,Video memory will increase once every time you use it,Is this normal? Is there any configuration that needs to be turned on to fix this? For example, conduct a knowledge base Q&A to clear the memory of the last question?",302
303,"[BUG] The web page failed to rebuild the database, resulting in the original uploaded database being gone",2023-06-14.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/626,"The web page failed to rebuild the database, resulting in the original uploaded database being gone",303
304,Running webui.py on CPU Error Message Tensor on device cpu is not on the expected device meta!,2023-06-14.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/627,"Running python on the CPU webui.py starts, but at the end there is: RuntimeError: Tensor on device cpu is not on the expected device meta!",304
305,OSError: [WinError 1114] Dynamic Link Library (DLL) initialization routine failed. Error loading "E:\xxx\envs\langchain\lib\site-packages\torch\lib\caffe2_nvrtc.dll" or one of its dependencies.,2023-06-14.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/629,**Problem Description**,305
306,"[BUG] IF THE WEBUI DELETES A KNOWLEDGE BASE DOCUMENT, THE KNOWLEDGE BASE QUESTION AND ANSWER FAILS",2023-06-15.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/632,"If you want to delete the file from the existing files in the knowledge base, click Delete, and enter the content in the Q&A box to report an error",306
307,"In the updated version, an error error occurs when the file in the knowledge base is deleted and the question is asked again",2023-06-15.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/634,"For the updated version, an issue is identified as follows:",307
308,I've configured my environment and want to implement Q&A for my local knowledge base? But it came back to me,2023-06-15.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/637,"There is no summary, only a relevant reply, but I look at the demo, the reply can be summarized, I go to query the code",308
309,[BUG] NPM run dev can not successfully start the VUE frontend,2023-06-15.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/638,**Problem Description**,309
310,[BUG] Concise description of the issue,2023-06-15.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/639,**Problem Description**,310
311,"Mention a bug in model loading, I fixed it in the screenshot, you can take a look at it when you have time.",2023-06-15.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/642,! [model_load_bug] (https://github.com/imClumsyPanda/langchain-ChatGLM/assets/59411575/4432adc4-ccdd-45d9-aafc-5f2d1963403b),311
312,[Help] about setting the embedding model path,2023-06-16.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/643,"As mentioned in the question, I have successfully run once before, but due to the loss of the environment, I always get an error when I reconfigure the webui again",312
313,Can the Lora fine-tuned model be used directly?,2023-06-16.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/646,"I see that there is a USE_LORA of this parameter in the model_config.py, but it is not used in the cli_demo.py and webui.py, and the model has no fine-tuning effect after the actual test, I want to ask if this function is implemented now",313
314,"write_check_file whether the load_file.txt generated in the tmp_files directory needs to be retained all the time, which takes up a lot of space, and whether it can be deleted after the index is created",2023-06-16.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/647,**Feature Description**,314
315,[BUG] /local_doc_qa/list_files?knowledge_base_id=test,2023-06-16.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/649,1. Create a new test knowledge base and upload the file (done on the vue front-end and check the back-end and find that the test folder and the content and vec_store below are indeed generated,315
316,[BUG] vue webui fails to load knowledge base,2023-06-16.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/650,"I pulled the latest code, ran the back-end API and the front-end web respectively, and clicked on the knowledge base, but only simple could be displayed, and the knowledge base could not be loaded",316
317,Can't load the MOSS model locally?,2023-06-16.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/652,How do I correctly configure local_model_path path of manually downloading model settings and still prompting missing files?,317
318,macOS M2 Pro Docker installation failed,2023-06-17.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/654,macOS M2 Pro Docker installation failed,318
319,[BUG] mac m1 pro runs the prompt zsh: segmentation fault,2023-06-17.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/655,Run: python webui.py,319
320,An error is reported for installing requirements,2023-06-17.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/656,(langchainchatglm) D:\github\langchain-ChatGLM>pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple/,320
321,[BUG] AssertionError,2023-06-17.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/658,**Problem Description**,321
322,[FEATURE] Does AMD win10 support on-premise deployment?,2023-06-18.06,https://github.com/imClumsyPanda/langchain-ChatGLM/issues/660,**Feature Description**,322