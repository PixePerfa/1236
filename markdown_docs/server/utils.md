## FunctionDef wrap_done(fn, event)
**wrap_done**: The function of this function wraps an Awaitable with an Event to notify the event when the awaitable object completes or throws an exception. 

**Parameters**:
- `fn`: An Awaitable, usually an asynchronous function or task.
- `event`: An `asyncio.Event`event that is used to `fn`notify when an exception is completed or thrown. 

**Code Description**:
`wrap_done`Functions are designed to be used in an asynchronous programming environment and accept an awaitable object and an event as arguments. The function first tries to wait for`fn` the completion of the awaitable object. If the execution is successful in the waiting process`fn`, it goes directly to the`finally` block; If `fn`an exception is thrown during execution, it is blocked and`except` captured. In the exception capture block, the `logging.exception`exception information is first recorded, then a message containing the exception information is constructed, and`logger.error` the error message is logged, where whether or not the detailed exception information is logged depends on`log_verbose` the value of the variable. Regardless `fn`of the outcome of the execution, the block will eventually be executed`finally`, where the `event.set()`event is notified by calling the method, indicating that `fn`the execution has completed or that an exception has occurred. 

In a project, `wrap_done`functions are called by multiple modules and are primarily used to handle the execution of asynchronous tasks and to notify them when a task completes or an exception occurs. For example,`agent_chat_iterator` in asynchronous iterators such as`chat_iterator` ,`completion_iterator` ,`knowledge_base_chat_iterator` `search_engine_chat_iterator`,`wrap_done` This pattern allows the asynchronous process in the project to be more flexible in how asynchronous tasks are completed, especially when follow-up operations need to be performed based on the results of the execution of the asynchronous task. 

**Note**:
- When using `wrap_done`functions, you need to make sure that you're passing `fn`in a correct awaitable object, such as an asynchronous function call or`asyncio` task. 
- `event`The parameter should be an `asyncio.Event`instance that is used to notify when `fn`it completes or when an exception occurs. 
- In exception handling, `log_verbose`you decide whether to record detailed exception information based on the variable settings, which needs to be configured according to the actual logging requirements. 
- `wrap_done`The use cases of functions are mainly focused on situations where asynchronous execution is required and notification is made when it is completed, so how to make reasonable use of this function to optimize the processing logic of asynchronous tasks should be considered when designing asynchronous processes.
## FunctionDef get_ChatOpenAI(model_name, temperature, max_tokens, streaming, callbacks, verbose)
**get_ChatOpenAI**: The function of this function is to initialize and return a ChatOpenAI instance. 

**Parameters**:
- model_name: String type, specifying the name of the model to use.
- temperature: A floating-point number that controls the variety of generated text.
- max_tokens: Integer or None, which is None by default, specifies the maximum number of tokens to generate text.
- streaming: Boolean type, defaults to True, specifies whether to run the model in streaming mode.
- callbacks: A list of callback functions, which is empty by default, and is used to process the text generated by the model.
- verbose: Boolean type, default is True, specifies whether to output details in the console.
- **kwargs: Accepts any additional keyword arguments.

**Code Description**:
The function first calls`get_model_worker_config` the function to `model_name`obtain the configuration information of the model according to the provided. If`model_name` it is "openai-api", the variable will be updated according to the configuration`model_name``model_name`. Next, the function sets`ChatOpenAI` the class's `_get_encoding_model`method to the`MinxChatOpenAI` class's `get_encoding_model`method to ensure that the `ChatOpenAI`instance gets the correct coding model. After that, initialize the instance with the provided parameters and configuration information`ChatOpenAI`. Eventually, the function returns the initialized`ChatOpenAI` instance. 

**Note**:
- Before using this function, make sure that you have properly configured the information about the model, including the API key and the underlying URL.
- If `model_name`the specified model is not supported or configured incorrectly, initialization may fail. 
- `callbacks`Parameters allow the user to pass in custom callback functions that can be called when the model generates text, to process the generated text or to execute other custom logic.

**Example output**:
`get_ChatOpenAI`An example output that might be returned by calling a function is:
```python
ChatOpenAI(streaming=True, verbose=True, callbacks=[], openai_api_key="YOUR_API_KEY", openai_api_base="https://api.example.com", model_name="gpt-3.5-turbo", temperature=0.7, max_tokens=1024, openai_proxy=None)
```
This indicates that the function returns a configured `ChatOpenAI`instance that uses the "gpt-3.5-turbo" model, with a temperature set to 0.7, a maximum number of tokens of 1024, and streaming and verbose output enabled. 
## FunctionDef get_OpenAI(model_name, temperature, max_tokens, streaming, echo, callbacks, verbose)
**get_OpenAI**: The function of this function is to initialize and return a configured instance of the OpenAI model. 

**Parameters**:
- model_name: String type, specifying the name of the model to use.
- temperature: A floating-point number that controls the creativity of the generated text.
- max_tokens: Integer or None, defaults to None, specifies the maximum number of tokens to generate text.
- streaming: Boolean type, which defaults to True, indicates whether streaming is enabled.
- echo: Boolean type, which defaults to True, indicates whether the input is echoed.
- callbacks: A list of callback functions, which is empty by default, and is used to process the output of the model.
- verbose: Boolean type, which defaults to True, controls whether or not details are output.
- **kwargs: Accepts any additional keyword arguments.

**Code Description**:
`get_OpenAI` The function first `get_model_worker_config`obtains the configuration information of the specified model by calling the function. If the model name is "openai-api", the model name will be updated according to the "model_name" in the configuration information. Next, the function creates an `OpenAI`instance and configures whether to enable streaming, whether to output details, a list of callback functions, OpenAI API key, OpenAI API base URL, model name, temperature, maximum number of tokens, OpenAI agent, and whether to echo input. Finally, the function returns the configured`OpenAI` model instance. 

**Note**:
- Make sure that the OpenAI API key and base URL have been correctly configured before calling this function.
- If the provided model name is "openai-api", the model name will be automatically updated according to the configuration, so you need to make sure that the relevant configuration is correct.
- The list of callback functions allows the user to customize the logic for processing the output of the model, adding callback functions as needed.

**Example output**:
Since `get_OpenAI`the function returns an `OpenAI`instance, the output example will depend on `OpenAI`the implementation of the class. Let's say `OpenAI`the instantiation result of the class is as follows:
```python
OpenAI(
    streaming=True,
    verbose=True,
    callbacks=[],
    openai_api_key="YOUR_API_KEY",
    openai_api_base="https://api.openai.com",
    model_name="text-davinci-003",
    temperature=0.7,
    max_tokens=100,
    openai_proxy=None,
    echo=True
)
```
This example shows an example of a model configured with streaming, verbose output, no callback function, specifying the API key and underlying URL, using the "text-davinci-003" model, a temperature of 0.7, a maximum number of tokens of 100, no agent, and input echo enabled`OpenAI`. 
## ClassDef BaseResponse
**BaseResponse**: The BaseResponse class is used to build a unified API response format. 

**Properties**:
- `code`: API status code, which indicates the result of the request processing, for example, 200 indicates success.
- `msg`: API status messages that provide more detailed information about the processing results, such as "success" indicates that the operation was successful.
- `data`: API data, which is used to store the data returned by the request, which can be of any type.

**Code Description**:
The BaseResponse class inherits from BaseModel and leverages the Pydantic library for data validation and serialization. It defines three main fields:`code` , `msg`, and `data`which represent the API's status code, status message, and returned data content, respectively. In addition, Pydantic's Field function provides default values and descriptions for each field to provide richer information when generating OpenAPI documentation. For example, if the`code` field defaults to 200, the`msg` field defaults to "success", and the `data`field defaults to None, indicating that no data is returned. 

The BaseResponse class also defines an internal class, Config, where properties `schema_extra`are used to provide sample data, which helps to demonstrate how to use the response format in the automatically generated API documentation. 

In the project, the BaseResponse class is widely used for the response model of various API interfaces. For example, in`mount_app_routes` a function, the `response_model=BaseResponse`response model for the root route ("/") is specified as BaseResponse, which means that the response for that route will follow the format defined by BaseResponse. Similarly,`chat_feedback``upload_temp_docs` `file_chat`BaseResponse is also used as the response model in many other API interfaces, such as , , , to ensure the consistency and standardization of API responses. 

**Note**:
- When using BaseResponse as the response model, the fields should be populated according to the actual situation`code``msg` `data`to ensure that the information returned to the client is accurate. 
- When defining an API interface, you `response_model=BaseResponse`can have FastAPI automatically convert the return value of the function to the format specified by BaseResponse by specifying parameters, which helps reduce code duplication and improve development efficiency. 
- In practice, BaseResponse can be extended as needed to add additional fields or methods to meet specific business needs.
### ClassDef Config
**Config**: The function of the Config class is to define a structure and example of configuration information. 

**Properties**:
- `schema_extra`: An example of an additional schema used to define configuration information.

**Code Description**:
A class property named is defined in the Config class `schema_extra` . This property is a dictionary that provides examples of configuration information. In this example, there are two key-value pairs: "code" and "msg". The value of "code" is set to 200, which indicates a successful status code; The value of "msg" is set to "success", which indicates a message that the operation was successful. This structure is often used in the standard format of API responses to help developers understand the expected response structure and content. 

**Note**:
- `schema_extra` Properties are primarily used in documentation and testing to provide developers with a concrete example of how to use the configuration. It doesn't directly affect the logical functionality of the code, but it can be very helpful in understanding the structure and expected behavior of the code.
- When modifying `schema_extra` the values in , you need to make sure that they are consistent with the configuration information in the actual application to avoid confusion. 
- This class can be extended with more configuration items and examples according to actual needs to adapt to different scenarios and requirements.
***
## ClassDef ListResponse
**ListResponse**: The ListResponse class is used to encapsulate the list data response returned to the client. 

**Properties**:
- `data`: Holds the property of the list of names, of type String List.

**Code Description**: The ListResponse class inherits from the BaseResponse class and is specifically designed to handle API responses that need to return list data. By using Pydantic's Field function,`data` the property is defined as a field that must be provided, with a description "List of names" indicating that this field is used to store the list of names. In addition, the ListResponse class defines an example of how to use the inner class Config to construct a response body that contains a list of status codes, status messages, and data. The data in the example includes a success status code of 200, a success message "success", and a list of three document names. 

In projects, the ListResponse class is used for API interfaces that need to return a list of file names or other strings. For example, in the knowledge base management function, the API interface for obtaining the list of knowledge bases and the list of files in the knowledge base uses ListResponse as the response model. This not only ensures a consistent format of the API response, but also allows the API user to clearly understand what data will be included in the response.

**Note**:
- When using the ListResponse class, you need to make sure that `data`the properties are populated with the correct list data. While the ListResponse class provides basic validation of data types and structures, the accuracy and relevance of the populated data is up to the developer. 
- The ListResponse class inherits from the BaseResponse class, so it automatically has`code` `msg`two properties, a status code and a status message for the API response. In practice, developers should set the values of these two properties according to the actual situation to ensure the accuracy and usefulness of the response information. 
- By defining properties in the Config class`schema_extra`, the ListResponse class provides a rich set of examples for auto-generated API documentation, helping developers and API consumers better understand how to use the response format. Developers should consider updating these sample data when extending or modifying the ListResponse class to keep the documentation accurate and useful. 
### ClassDef Config
**Config**: The function of the Config class is to define schema extras. 

**Properties**:
- `schema_extra`: Used to define additional schema information.

**Code Description**:
The Config class is a simple configuration class whose primary purpose is  to `schema_extra` provide an example configuration via a property. This example configuration is a dictionary that contains `code` the keys , , `msg` and `data` . `code` The value corresponding to the key is an integer, which represents the status code. `msg` The value corresponding to the key is a string that represents the content of the message; `data` The value corresponding to the error is a list of strings that represents the content of the data. This configuration is primarily used in documentation or API response examples to help developers understand the expected response format. 

In this example,`code` set to 200, the request was successful; `msg` Set to "success", which indicates the message that the operation was successful; `data` It contains three strings, "doc1.docx", "doc2.pdf", and "doc3.txt", simulating the return data of a list of files. 

**Note**:
- `schema_extra` It exists here as a class property, which means that it is associated with a class and not with an instance of the class. Therefore, `schema_extra` any modification to will affect all places where this class is used. 
- This class is primarily used to provide sample configurations of API responses for use when automatically generating documentation or for API testing. Developers should adapt `schema_extra` the content to their actual needs to ensure that it correctly reflects the expected response of the API. 
***
## ClassDef ChatMessage
**ChatMessage**: The function of ChatMessage is to define the data structure of chat messages. 

**Properties**:
- `question`: Question text.
- `response`: Response text.
- `history`: A list of historical texts, each element is also a list of strings, representing a Q&A of a conversation.
- `source_documents`: A list of source documents, containing information about the related documents and their scores.

**Code Description**:
The ChatMessage class inherits from the BaseModel and is used to represent a message in a conversation. The class is defined through the Pydantic library to ensure data validity and type security. Each property is defined in detail using Pydantic's Field method, including type information and description text. 

- `question` The property defines the text content of the question and is a string type.
- `response` The property defines the answer to the question and is also a string type.
- `history` A property records the historical conversations related to the current message and is a list, where each element is also a list containing a series of question and answer pairs.
- `source_documents` Attributes provide a list of source documents that are referenced when generating answers. These documents may be retrieved from external databases, files, or other sources of information to support the generation of answers.

In addition, there is a Config subclass defined in the ChatMessage class, where the  field `schema_extra` provides an example of how to populate an instance of the ChatMessage class. This example includes a typical question, corresponding answers, relevant historical conversations, and reference to source documentation. 

**Note**:
- When you use the ChatMessage class, you need to ensure that all fields meet the defined type and structure requirements. In particular `history` `source_documents` , the and attributes, which are both list types, need to be constructed correctly. 
- `schema_extra` The examples in are only meant to illustrate how to use the ChatMessage class, and don't mean that the data in the real world must be exactly the same as the examples. Developers should populate these fields on a case-by-case basis.
- Since the ChatMessage class uses the Pydantic library, you can take advantage of the data validation features provided by Pydantic to ensure the correctness and integrity of the data. In practice, the security and reliability of data can be further enhanced by defining more validation rules.
### ClassDef Config
**Config**: The function of the Config class is to provide an example configuration that illustrates how to handle and respond to workers' compensation related queries. 

**Properties**:
- `schema_extra`: A dictionary with a sample configuration that shows how to answer a question about workers' compensation insurance.

**Code Description**:
The Config class defines a `schema_extra` class property called , which is a dictionary. This dictionary has a key nested inside called "example", and its value is also a dictionary that provides a concrete example of how to answer a question about workers' compensation insurance. This example consists of four sections: "question", "response", "history", and "source_documents". 

- The value corresponding to the "question" key is a string that represents the question asked.
- The value of the "response" key is a string that describes the answer to the question in detail, including the process and benefits of work-related injury insurance.
- The value of the "history" key is a list, and each element is also a list, representing the previous related Q&A.
- The value of the "source_documents" key is a list of the source of the document on which the answer is based, and each element is a string that describes the source and content of the document.

**Note**:
- The Config class is primarily used to provide sample configurations to help developers understand how to build and use similar configuration structures.
- In practice, developers can modify or extend `schema_extra` the content according to specific needs to adapt to different scenarios and needs. 
- The questions, answers, historical Q&A, and source files in the examples are for demonstration purposes only and should be populated with appropriate content as appropriate.
***
## FunctionDef torch_gc
**torch_gc**: The function of this function is to clean up the cache memory of PyTorch on the CUDA or MPS backend. 

****Arguments: This function does not accept any arguments. 

**Code Description**: The `torch_gc`function first tries to import the PyTorch library. If successful, it checks if CUDA is available. If CUDA is available, the function calls`torch.cuda.empty_cache()` SUM `torch.cuda.ipc_collect()`to empty CUDA's cache memory, as well as collect shared memory allocated across processes. This helps to free up unused memory when using CUDA for a lot of computations, thus avoiding issues such as memory overflows or performance degradation. 

If CUDA is not available, the function checks if MPS (Apple Metal Performance Shaders) is available, which is for macOS systems. If MPS is available, it tries to`torch.mps` import from the module and call `empty_cache()`a function to clean up the MPS's cache memory. If there are any exceptions during an attempt to import or call, the function catches these exceptions and logs an error message recommending that the user upgrade the PyTorch version to version 2.0.0 or later for better memory management support. 

In a project, `torch_gc`a function is called by`FaissKBService` a class's `do_add_doc`methods. In the`do_add_doc` method, `torch_gc`it is used to clean up PyTorch's cache memory after adding documents to the knowledge base and updating the vector store. This is because when processing large amounts of data and using PyTorch for vectorization operations, a large memory footprint can be incurred. By calling it`torch_gc`, you can help free up these unused memories to avoid memory overflow errors and ensure the stability and performance of your system. 

**Note**: `torch_gc`When using functions, you need to make sure that PyTorch is installed correctly and that the system supports CUDA or MPS. In addition, if you are using it on macOS and are experiencing related memory cleanup issues, you should consider upgrading your PyTorch version to version 2.0.0 or later. 
## FunctionDef run_async(cor)
**run_async**: The function of this function is to run asynchronous code in a synchronous environment. 

**Parameters**:
- cor: An asynchronous coroutine object that needs to be run.

**Code Description**:
`run_async` Functions are designed to address the need to execute asynchronous coroutines in a synchronous code environment. It first tries to get the current event loop, and if there is no one currently, it creates a new one. This function takes an asynchronous coroutine object as a parameter and runs the coroutine in an event loop that it gets or creates until the coroutine is executed. In this way, asynchronous operations can be easily performed even in synchronous code. 

In the function implementation, the first step is to `asyncio.get_event_loop()`get the current thread's event loop by trying to get it. If the current thread doesn't have an event loop running, this step may throw an exception. To handle this situation, the function uses`try-except` a structure to catch the exception and create`asyncio.new_event_loop()` a new event loop by creating it. Finally,`loop.run_until_complete(cor)` run the incoming coroutine object with `cor`a run and return the execution result. 

**Note**:
- When using this function, you need to make sure that the argument you pass in`cor` is an asynchronous coroutine object. 
- If this function is called in an event loop that is already running, it may result in an error. Therefore, it's best to use this function when making sure there are no running event loops.
- After the function execution ends, the event loop stops, but it doesn't close. If you need to continue using the event loop later in your code, you may need to manually manage the lifecycle of the event loop.

**Example output**:
Suppose you have an asynchronous function`async_function` with a return value of ,`"Hello, Async World!"` the `run_async(async_function())`return value used might be as follows:
```
"Hello, Async World!"
```
## FunctionDef iter_over_async(ait, loop)
**iter_over_async**: Encapsulates an asynchronous generator as a synchronous generator. 

**Parameters**:
- **ait**: An asynchronous generator that needs to be encapsulated. 
- **loop**: An optional parameter that specifies an event loop. If it is not provided, it tries to get the event loop of the current thread, and if it fails, a new event loop is created. 

**Code Description**:
`iter_over_async` Functions are primarily used to convert the iterative process of an asynchronous generator to a synchronous process for use in environments that don't support asynchronous iteration. First,`ait.__aiter__()` get an asynchronous iterator by calling it. Then define an asynchronous function`get_next` that tries to `await ait.__anext__()`get the next element by getting it, and if it succeeds, returns`(False, obj)`, where `obj`is the acquired element; If the iteration ends, the exception is caught`StopAsyncIteration`, and the return `(True, None)`indicates the end of the iteration. 

If no event loop is provided at the time of invocation (`loop`the argument is`None`), the function attempts to get the current thread's event loop and if it fails, creates a new one. 

Next, the function enters an infinite loop, and each time the`loop.run_until_complete(get_next())` function is executed synchronously`get_next`, it gets a signal for the next element or the end of the iteration. If the iteration ends, the loop breaks; Otherwise, yield returns the currently obtained elements, enabling the asynchronous generator to output elements one by one synchronously. 

In a project, `iter_over_async`a function is `server/model_workers/xinghuo.py/XingHuoWorker/do_chat`called by a method that is used to process the response data for an asynchronous request. In the`do_chat` method, the `iter_over_async`data stream returned from the asynchronous API request is processed synchronously by the function, so that the asynchronously obtained data can be processed step by step in a synchronous function, improving the readability and ease of use of the code. 

**Note**:
- When using this function, you need to make sure that the arguments you pass in`ait` are an asynchronous generator. 
- If you use this function in an asynchronous environment, you should pay attention to the management of event loops to avoid conflicts in event loops.

**Example output**:
Let's say you have an asynchronous generator `async_gen`that returns a number each time asynchronously, and`iter_over_async` when used to iterate, the possible output is:
```python
for num in iter_over_async(async_gen):
    print(num)
```
Output:
```
1
2
3
...
```
### FunctionDef get_next
**Function of get_next function**: Get the next object asynchronously. 

**Parameters**: This function does not accept any external parameters. 

**Code Description**: `get_next` is an asynchronous function designed to get the next element from an asynchronous iterator. The function first tries to `ait.__anext__()` get the next element by calling asynchronously. If successful, the function will return a tuple, with the first element being  , `False`indicating that the end of the iterator has not been reached, and the second element being the acquired object. If you encounter an exception when trying to get the next element `StopAsyncIteration` , indicating that the iterator has no more elements to return, the function will return a tuple, the first element is `True` , indicating that the end of the iterator has been reached, and the second element is `None` . 

**Note**: When using this function, you need to make sure that `ait` is an asynchronous iterator object and has been initialized correctly. Also, because this is an asynchronous function, you need to use `await` a keyword or in another asynchronous context when calling it. 

**Example output**:
- When there are also elements in the iterator, the possible return value is `(False, obj)` , where  is the `obj` next object taken from the iterator. 
- When there are no more elements in the iterator, the return value is `(True, None)` . 
***
## FunctionDef MakeFastAPIOffline(app, static_dir, static_url, docs_url, redoc_url)
**MakeFastAPIOffline**: The function of this function is to provide offline documentation support for FastAPI applications, making them independent of the CDN to load the Swagger UI and ReDoc documentation pages. 

**Parameters**:
- `app`: FastAPI object, which needs to be modified to support offline documentation.
- `static_dir`: The path of the static file directory, which defaults to the "static" folder in the directory where the current file is located.
- `static_url`: The URL path of the static file of the service, which is /static-offline-docs by default.
- `docs_url`: The URL path of the Swagger UI documentation, which defaults to "/docs".
- `redoc_url`: The URL path of the ReDoc document, which is /redoc by default.

**Code Description**:
This function first imports the required classes and functions from the FastAPI and starlette modules. It then defines an internal function`remove_route` that removes the specified route from the FastAPI application. Next, the function `app.mount`uses methods to mount static file directories so that the static files required for the Swagger UI and ReDoc can be served from the local server instead of being loaded through an external CDN. 

If the `docs_url`parameter is not None, the function removes the original Swagger UI route and adds a new route that returns a custom Swagger UI HTML page with all resource URLs modified to point to the local static file directory. 

Similarly, if the `redoc_url`parameter is not None, the function removes the original ReDoc route and adds a new route that returns a custom ReDoc HTML page with the URL of the resource also modified to point to the local static file directory. 

This function implements an offline document system that does not depend on external CDNs by modifying the routing and static file configuration of the FastAPI application.

**Application in the project**:
In the project, `MakeFastAPIOffline`functions are used in the creation of different FastAPI applications to ensure that these applications can still provide complete API documentation without an external network connection. For example, in`create_app` functions such as`create_controller_app` , `create_model_worker_app`, and`create_openai_api_app` , the `MakeFastAPIOffline`functions that implement offline documents are called. This means that multiple components in the project need to provide API documentation in an offline environment so that developers and users can access and understand how to use the API even without an internet connection. 

**Note**:
- Make sure that `static_dir`the directory that the parameter points to contains all the static files needed for the Swagger UI and ReDoc, including JavaScript, CSS files, icons, etc. 
- When modifying`docs_url` and `redoc_url`parameter changes, you need to ensure that these URLs do not conflict with existing routes in your application. 

**Output example**: Since this function has no return value, there is no output example. The main function of this function is to modify the incoming FastAPI application object and add offline document support. 
### FunctionDef remove_route(url)
**remove_route**: The function of this function is to remove the specified route from the app. 

**Parameters**:
- **url**: The URL of the route to be removed, of type as a string. 

**Code Description**:
`remove_route` The function accepts a parameter `url`with the intent of removing a route that matches a given from the list of routes in the FastAPI app `url` . The function first initializes a variable called  , `index` which stores the index location of the route that needs to be removed. By traversing`app.routes`, which is a list of all routes in the app, the function compares whether the properties of each route object `path` (i.e., the URL path of the route) are the same as the given  one `url` (the comparison here ignores case). Once a matching route is found, its index is assigned to `index` and  the loop is bounced. 

If `index` is an integer (meaning a matching route was found), the function `pop` `app.routes` removes the route object for that index from the list via the method. In this way, the specified route is successfully removed from the app. 

**Note**:
- When using this function, make sure that the parameters you pass in `url` actually correspond to a valid route in your app. If a non-existent route URL is passed in, the function will do nothing. 
- Since this function directly modifies  , `app.routes`use it with caution to avoid accidentally removing routes that shouldn't be removed. 
- Make sure that the`app` object has been properly initialized and contains a list of routes before calling this function. This function assumes that `app` is a global variable and that a route has already been added `app.routes` to . 
***
### FunctionDef custom_swagger_ui_html(request)
**Functionality of the custom_swagger_ui_html function**: Generate and return a custom Swagger UI HTML response. 

**Parameters**:
- `request`: Request object to obtain the information of the current request.

**Code Description**:
`custom_swagger_ui_html`A function is an asynchronous function that takes an`Request` object as an argument and returns an `HTMLResponse`object. The main purpose of this function is to generate a custom Swagger UI page for displaying API documentation. 

The function is `request.scope`first fetched from`root_path`, which is the root path of the current app. It then constructs a favicon's URL, which is the address of the small icon that appears on the page label. 

Next, the function calls `get_swagger_ui_html`the function, passing in several parameters to customize the Swagger UI page
- `openapi_url`: This is the URL of the OpenAPI specification file, which is used to parse the Swagger UI and display the API documentation.
- `title`: The title of the page, here by `app.title`getting the title of the app, with "-Swagger UI" appended. 
- `oauth2_redirect_url`: OAuth2 redirect URL for the OAuth2 authentication process.
- `swagger_js_url`And`swagger_css_url`: The URLs of the JavaScript and CSS files of the Swagger UI, respectively, for the styling and functionality of the page. 
- `swagger_favicon_url`: The URL of the small icon that appears on the page label.

**Note**:
- Make sure it`request.scope` is included`root_path`, otherwise it may result in path resolution errors. 
- `get_swagger_ui_html`Functions need to be imported externally, make sure they are imported correctly before using them.
- This function relies on the application instance of FastAPI`app` and related configuration variables (e.g`openapi_url``swagger_ui_oauth2_redirect_url`., , etc.), please ensure that these variables are set correctly before calling the function. 

**Example output**:
Assuming that the title of the application is "My API" and the root path is "/api", the HTMLResponse content that this function may return is as follows (the actual content will contain the full HTML structure, only the key information is shown here):

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <title>My API - Swagger UI</title>
    <link rel="icon" type="image/png" href="/api/static/favicon.png" sizes="32x32" />
    <link href="/api/static/swagger-ui.css" rel="stylesheet" />
</head>
<body>
    <div id="swagger-ui"></div>
    <script src="/api/static/swagger-ui-bundle.js"> </script>
</body>
</html>
```

This HTML page will load the Swagger UI, automatically parse `openapi_url`the pointing OpenAPI specification file, and display the API documentation. 
***
### FunctionDef swagger_ui_redirect
**swagger_ui_redirect**: The function of this function is to redirect to the OAuth2 authentication page of the Swagger UI. 

****Arguments: This function has no arguments. 

**Code Description**: `swagger_ui_redirect`The function is an asynchronous function that returns an HTMLResponse object. This function calls `get_swagger_ui_oauth2_redirect_html`a method, which is responsible for generating and returning an HTML page containing the information required for OAuth2 authentication. This page is typically used in API documentation to allow users to authenticate via OAuth2 in order to be able to authenticate when testing the API. Since this is an asynchronous function, it can be used in FastAPI or other web frameworks that support asynchronous operations to improve the efficiency of handling requests. 

**Note**: When using this function, you need to ensure that the `get_swagger_ui_oauth2_redirect_html`method is available and that OAuth2 authentication is configured correctly. Also, since it returns HTMLResponse, the route that calls this function should be configured to return HTML content. 

**Example output**:
```html
<!DOCTYPE html>
<html>
<head>
    <title>OAuth2 Redirect</title>
</head>
<body>
    <script>
        // JavaScript code to handle OAuth2 redirection logic
    </script>
</body>
</html>
```
The above output example shows a possible HTML page structure, and the actual content will vary depending on `get_swagger_ui_oauth2_redirect_html`the implementation of the method. This page contains all the information and logic needed to handle OAuth2 authentication redirects. 
***
### FunctionDef redoc_html(request)
**redoc_html**: The function of this function is to generate and return an HTML response for the ReDoc page that displays the API documentation. 

**Parameters**:
- `request`: Request object to obtain the information of the current request.

**Code Description**:
`redoc_html`A function is an asynchronous function that takes a FastAPI `Request`object as a parameter. The main purpose of this function is to construct and return an HTML response to a ReDoc page that is used to display OpenAPI documentation. 

The function is `request.scope`first fetched from`root_path`, which is the root path of the current app. It then constructs the URL of the favicon icon, which is made by concatenating the root path, the static file path, and the favicon file name. 

Next, the function calls `get_redoc_html`the function, passing in the following parameters:
- `openapi_url`: The URL of the OpenAPI document, which is created by concatenating the root path and the path of the OpenAPI document.
- `title`: Page title, here is the title of the app plus "-ReDoc".
- `redoc_js_url`: The URL of the ReDoc JavaScript library, which is formed by concatenating the root path and the static file path with the file name of the ReDoc library.
- `with_google_fonts`: A boolean value that indicates whether the font is loaded from the Google Fonts service. Set here`False` to mean that fonts are not loaded from Google. 
- `redoc_favicon_url`:The URL of the favicon icon.

Finally, the `get_redoc_html`function returns an `HTMLResponse`object containing the HTML code for the ReDoc page that is used to display the API documentation. 

**Note**:
- Make sure that the root path of your app, the path to the OpenAPI documentation, and the path to the static file are set correctly before calling this function.
- This function depends on `get_redoc_html`the function to ensure that this function is available and returns`HTMLResponse` the object correctly. 

**Example output**:
Since this function returns an `HTMLResponse`object, its exact content depends on `get_redoc_html`the implementation of the function and the parameters passed in. In general, the returned HTML response will contain a full ReDoc page that allows the user to view and interact with the API documentation through a browser. 
***
## FunctionDef list_embed_models
**list_embed_models**: The function of this function is to get a list of embedded model names for the configuration. 

****Arguments: This function has no arguments. 

**Code Description**:  The function retrieves the names of all configured embedding models `list_embed_models` by accessing  the key `MODEL_PATH` in the  global variable `"embed_model"` and returns those names as a list. This function is mainly used in the project to retrieve the available local embedding models, so that when text vectorization is required, the appropriate model can be selected for operation. 

This function is called in different parts of the project to ensure that the correct and valid model is used when performing text embedding or other operations related to embedding the model. For example, in the `embed_texts` and `aembed_texts` functions, `list_embed_models` verify that the incoming  is `embed_model` a valid, configured local model by calling . If so, then the model will be used for vectorization of the text; If not, an alternative route will be tried or an error message will be returned. In addition, in the `knowledge_base_page` function, this function is used to provide a list of embedded models for users to choose which model to use for text embedding when creating or updating a knowledge base. 

**Note**: When using this function, you need to make sure that `MODEL_PATH` the Global Variable is configured correctly and contains  a `"embed_model"` key, whose value should be a dictionary that contains all available embedding model names. 

**Example output**: Suppose `MODEL_PATH["embed_model"]` there are two models `"model1"` and  , `"model2"`then the call `list_embed_models()` will return `["model1", "model2"]`. 
## FunctionDef list_config_llm_models
**list_config_llm_models**: The function of this function is to get the different types of configured large language models (LLMs). 

****Arguments: This function has no arguments. 

**Code Description**: Functions `list_config_llm_models` are designed to retrieve and return different types of large language model (LLM) configuration information from the system configuration. It first `FSCHAT_MODEL_WORKERS` copies all working model configurations from the global variables and removes the default configuration, if one exists. The function then constructs and returns a dictionary that contains three types of model configurations: local (`local`), online (`online`), and working model (`worker`). The model configurations under each type are also organized in a dictionary format, where the key is the model name and the value is the corresponding configuration information. 

In a project,`list_config_llm_models` a function is called by multiple objects, indicating that it plays a central role in the project. For example, in `server/llm_api.py/list_config_models` , this function is used to obtain the corresponding model configuration based on the type specified in the request body (such as `local`, ), `online`and further obtain detailed configuration information for each model to construct the response data. In `server/utils.py/list_online_embed_models` , this function is used to get the configuration of the online models and further check whether those models support embedding to filter out the list of models that can be embedded. 

**Note**: When using this function, you need to ensure that the relevant global variables such as `FSCHAT_MODEL_WORKERS` , , `MODEL_PATH` and  contain `ONLINE_LLM_MODEL`valid configuration information. In addition, because the configuration information returned by this function can have a direct impact on the loading and use of the model, you should be careful when modifying the configuration or extending the model type. 

**Example output**:
```python
{
    "local": {
        "llm_model_1": {"config1": "value1", "config2": "value2"},
        "llm_model_2": {"config1": "value1", "config2": "value2"}
    },
    "online": {
        "llm_model_online_1": {"config1": "value1", "config2": "value2"},
        "llm_model_online_2": {"config1": "value1", "config2": "value2"}
    },
    "worker": {
        "llm_model_worker_1": {"config1": "value1", "config2": "value2"},
        "llm_model_worker_2": {"config1": "value1", "config2": "value2"}
    }
}
```
This output example illustrates the configuration information structure that a function might return, which contains three types of model configurations, each of which may contain multiple models and their configuration information.
## FunctionDef get_model_path(model_name, type)
**get_model_path**: The function of this function is to get the path of the model based on the model name and type. 

**Parameters**:
- model_name: String type, specifying the name of the model from which you want to get the path.
- type: String type, optional, default is None, specifies the type of the model.

**Code Description**:
`get_model_path` Functions are mainly used in a project to determine where to store a model file based on the model name provided (`model_name`) and the optional model type (`type`). This function first checks whether a model type is provided and tries to find the corresponding path in the predefined model path dictionary (`MODEL_PATH`). If no type is provided or the provided type is not found in the dictionary, the function iterates through all the values in the dictionary to build a dictionary with all the paths. 

Next, the function tries to get the path string corresponding to the model name from the built path dictionary. If a path string is found, the function makes several decisions:
1. If the path string points directly to a directory, the absolute path to that directory is returned.
2. If it is not a direct directory path, the function will try to check if there `MODEL_ROOT_PATH`is a corresponding directory as the root directory, combined with the model name or path string. The function tries several combinations to locate the directory:
   - Use the model name as the subdirectory name.
   - Use the full path string as the subdirectory path.
   - The last part of the path string split is used as the subdirectory name.

If none of the above steps can locate an existing directory, the function will eventually return the original path string.

In the project,`get_model_path` functions are used in different scenarios, such as in a `load_embeddings`method, to get the storage path of the model based on the model name, which is used to load the embedded model. In the`get_model_worker_config` function, it is used to determine the model path when getting the model work configuration. 

**Note**:
- Ensure that`MODEL_PATH` the sum `MODEL_ROOT_PATH`variables are properly configured according to the needs of the project so that the function resolves and returns the model path correctly. 
- Depending on how the model is stored and configured, the path returned by the function may be an absolute path or relative to a root.

**Example output**:
- Assuming `MODEL_ROOT_PATH`that it is`/models` included`MODEL_PATH`, the call `{"type1": {"modelA": "path/to/modelA"}}`may be returned`get_model_path("modelA", "type1")``"/models/path/to/modelA"`. 
## FunctionDef get_model_worker_config(model_name)
**get_model_worker_config**: The function of this function is to load the specified model work configuration item. 

**Parameters**:
- model_name: String type, optional, default is None, specifies the name of the model to load the configuration.

**Code Description**: Functions `get_model_worker_config` are mainly used to load and merge configuration items for a specified model. Functions are first imported from`configs.model_config` the AND`configs.server_config` module`ONLINE_LLM_MODEL`, `MODEL_PATH`and `FSCHAT_MODEL_WORKERS`configured. Then, the function `FSCHAT_MODEL_WORKERS`creates a new configuration dictionary based on the "default" configuration item in .`config` Next, the function tries to update`config` the dictionary by first using`model_name` Find `ONLINE_LLM_MODEL`and Update Config in, and then `FSCHAT_MODEL_WORKERS`Find and Update Config in. 

If `model_name`it `ONLINE_LLM_MODEL`exists, the function will be`config` set `"online_api"`to True in and attempt to `model_workers`dynamically load the specified`provider` class from`config` the module into `"worker_class"`the . If there is an exception during the loading process, the function will log an error message. 

For local models, if`model_name` they `MODEL_PATH["llm_model"]`exist in the , the function will call the`get_model_path` function to get the model path and update`config` the model in `"model_path"`it. If the path exists and is a directory,`config` it will also be set `"model_path_exists"`to True. Finally, the function uses the `llm_device`function to determine and set the type of device on which the model runs. 

**Note**:
- Make sure that the relevant configuration files`model_config` and `server_config`settings are set up correctly and that the required model name and default configuration are included before calling this function. 
- The function depends on `model_workers`the class defined in the module and `provider`will log an error message if the specified does not exist. 
- This function handles the configuration loading of both the online and local models, ensuring that the`MODEL_PATH` correct model path information is included. 

**Example output**:
Assuming that `FSCHAT_MODEL_WORKERS`there is a default configuration and a configuration named "example_model", and`ONLINE_LLM_MODEL` a configuration named "example_model" is also included, the call`get_model_worker_config("example_model")` might return a dictionary like this:
```python
{
    "default_key": "default_value",  # From FSCHAT_MODEL_WORKERS["default"]
    "online_api": True,  # Because model_name exists in ONLINE_LLM_MODEL
    "worker_class": <class 'server.model_workers.ExampleProvider'>,  # Dynamically loaded provider class
    "model_path": "/path/to/example_model",  #Model path obtained by get_model_path
    "model_path_exists": True,  # If the path exists and is a directory
    "device": "cuda", # The device type determined by the llm_device function
    ... # Other possible configuration items
}
```
This output example shows how a function can merge configurations from different sources and dynamically adjust the configuration content based on the model name.
## FunctionDef get_all_model_worker_configs
**get_all_model_worker_configs**: The function of this function is to get a dictionary of all model working configuration items. 

****Arguments: This function does not accept any arguments. 

**Code Description**: `get_all_model_worker_configs` The function is primarily used to collect and return details of all model work configuration items. The function first creates an empty dictionary`result` that stores the results. By `FSCHAT_MODEL_WORKERS.keys()`getting a collection of all the model names`model_names`, and then iterating through those model names. For each model name, if the name is not "default", the function is called`get_model_worker_config` to get the configuration of that model and add it to`result` the dictionary. Here, the `get_model_worker_config`function is used to load the working CIs for the specified model, and it dynamically merges the default and model-specific CIs based on the model name. Eventually, the function returns a dictionary containing all model configurations`result`. 

**Note**:
- Make sure that `FSCHAT_MODEL_WORKERS`it has been initialized correctly and that the configuration information for all available models is included. `FSCHAT_MODEL_WORKERS`is a key global variable that stores the default configuration as well as per-model-specific configuration items. 
- This function relies on `get_model_worker_config`the function to get the configuration for each model. Therefore, make sure that `get_model_worker_config`the function is executed correctly and that the relevant configuration files and modules are set up correctly. 

**Example output**:
Suppose `FSCHAT_MODEL_WORKERS`a model configuration named "model1" and "model2" is included, and the "default" configuration is excluded, the call`get_all_model_worker_configs()` may return a dictionary like this:
```python
{
    "model1": {
        "default_key": "default_value",
        "online_api": True,
        "worker_class": <class 'server.model_workers.Model1Provider'>,
        "model_path": "/path/to/model1",
        "model_path_exists": True,
        "device": "cuda",
        ...
    },
    "model2": {
        "default_key": "default_value",
        "online_api": False,
        "worker_class": <class 'server.model_workers.Model2Provider'>,
        "model_path": "/path/to/model2",
        "model_path_exists": True,
        "device": "cpu",
        ...
    }
}
```
This output example shows how configuration items can be dynamically collected and merged for each model, providing detailed configuration information for subsequent model deployment and use.
## FunctionDef fschat_controller_address
**fschat_controller_address**: The function of this function is to get the address of the Fastchat controller. 

****Arguments: This function has no arguments. 

**Code Description**: `fschat_controller_address` The function first imports the configuration from the configuration file`FSCHAT_CONTROLLER`, which contains the host address and port number of the Fastchat controller. The function checks if the host address is`"0.0.0.0"`, and if so, replaces it with`"127.0.0.1"`, because in the local environment, `"0.0.0.0"`it means listening on all available network interfaces, and it needs to be specified as when it is actually `"127.0.0.1"`accessed. The function then formats the processed host address and port number into a full URL string and returns that string. 

In a project,`fschat_controller_address` it is called in multiple places, including but not limited to`list_running_models``stop_llm_model``change_llm_model``set_httpx_config``get_httpx_client` `get_server_configs`functions`run_model_worker` such as , ,`run_openai_api` These call points are mainly used to obtain the address of the Fastchat controller in order to make network requests or to configure network request clients. For example, in a`list_running_models` function, use `fschat_controller_address`Get Controller Address to request a list of loaded models; In the `set_httpx_config`function, use this address to configure the proxy settings for the HTTP client to ensure that requests to the Fastchat controller are not intercepted by the proxy. 

**Note**: When using this function, you need to ensure that`configs.server_config` the configuration in is `FSCHAT_CONTROLLER`correct, including a valid host address and port number, to guarantee a successful connection to the Fastchat controller. 

**Output example**: Assuming that the host address configured by the Fastchat controller is and`"0.0.0.0"` the port number is`8080`, then the string returned by the function is`"http://127.0.0.1:8080"`. If the host address is already a specific IP address or domain name, for example`"192.168.1.100"`, the returned string is`"http://192.168.1.100:8080"`. 
## FunctionDef fschat_model_worker_address(model_name)
**fschat_model_worker_address**: The function of this function is to get the address of a specified model worker. 

**Parameters**:
- model_name: String type, specifying the name of the model to get the address, defaults to the first element in the LLM_MODELS list.

**Code Description**: The `fschat_model_worker_address` function first calls `get_model_worker_config`the function and passes in the model name to obtain the configuration information of the model. If the configuration information is successfully obtained, the function will extract`host` the sum`port` fields from the configuration dictionary. If`host` the value of the field is`"0.0.0.0"`, replace it with because `"127.0.0.1"`in a local environment, it `"0.0.0.0"`means listening to all available addresses, whereas in actual use it usually needs to be specified as a local loopback address`"127.0.0.1"`. Finally, the function combines`host` and `port`combines into a full URL address, formatted`"http://{host}:{port}"`, and returns that address. If you can't get the configuration information for the model, the function will return an empty string. 

**Note**:
- Ensure that the `get_model_worker_config`model's configuration information, including the host address and port number of the model service, is correctly loaded through the function before calling this function. 
- This function is primarily used for communication between internal services, and ensuring that the correct configuration of the service address is critical to the proper functioning of the service.

**Example output**:
The call`fschat_model_worker_address("example_model")`, assuming`example_model` that `host`in the configuration`"0.0.0.0"`, `port`is , it `8080`is possible to return:
```
"http://127.0.0.1:8080"
```
## FunctionDef fschat_openai_api_address
**fschat_openai_api_address**: The function of this function is to get the full address of the FastChat OpenAI API. 

****Arguments: This function does not accept any arguments. 

**Code Description**: `fschat_openai_api_address` The function first imports a configuration dictionary from the project's configuration file `FSCHAT_OPENAI_API` . It then extracts the and information from this configuration `host` `port` to construct and return a formatted URL string that points to the service address of the FastChat OpenAI API. If the value in the configuration `host` is  , `"0.0.0.0"`it will be replaced with  , `"127.0.0.1"`which is a generic IP address that points to the local host. Eventually, the URL returned by the function is in the format of `"http://{host}:{port}/v1"` , where `{host}`  and are `{port}` replaced with the actual host address and port number, respectively. 

In a project,`fschat_openai_api_address` functions are called by multiple objects, including `get_ChatOpenAI`, ,`get_OpenAI``set_httpx_config` , and `get_httpx_client` so on, and these call points are mainly used to configure and initialize services related to the OpenAI API. For example, in the `get_ChatOpenAI` and `get_OpenAI` functions,`fschat_openai_api_address` the return value is used as `ChatOpenAI` a parameter to specify the base URL of the OpenAI API when creating  an instance of and `OpenAI` . This shows that `fschat_openai_api_address` functions play a key role in connecting and configuring OpenAI API services in the project. 

**Note**: When using this function, make sure that  the `FSCHAT_OPENAI_API` configuration dictionary is set correctly in the project's configuration file, including valid `host` and `port` values. In addition, considering the different network environments, if the API service is deployed on a remote server, the value may need to be adjusted accordingly `host` . 

**Example output**: Assuming `FSCHAT_OPENAI_API` that  is `host` `"127.0.0.1"`, is in the`port`  configuration `"5000"`, then the return value of the function will be `"http://127.0.0.1:5000/v1"` . 
## FunctionDef api_address
**api_address**: The function of this function is to generate and return the address of the API server. 

****Arguments: This function has no arguments. 

**Code Description**: `api_address` The function first imports a configuration dictionary from the project's configuration file `API_SERVER` . It then extracts the and values from this dictionary `host` `port` . If `host` the value of  is , `"0.0.0.0"`it is replaced with `"127.0.0.1"` , because in many cases it `"0.0.0.0"` means listening on all network interfaces, and you need to specify it as `"127.0.0.1"` or the actual IP address when you actually access it. Finally, the function `host`  combines and into `port` a string in the format of  , `"http://{host}:{port}"`and returns this string. 

In a project,`api_address` functions are called by multiple objects, including `get_server_configs` functions,`dump_server_info` functions`ApiRequest`,  and `AsyncApiRequest` constructors of and classes. These calls indicate that `api_address` the function is used to provide the address of the API server so that other parts of the code can use that address to make network requests or to display the address of the API server in logs and configuration information. 

**Note**: When using this function, you need to make sure that `configs.server_config` the dictionary  is configured correctly in the file `API_SERVER` , including `host`  the `port` two key-value pairs. In addition, if the address or port of the API server changes during the running of the project, the configuration file needs to be updated to ensure that `api_address` the address returned by the function is up to date. 

**Example output**: Assuming the API server's `host` configuration is `"127.0.0.1"``port`  , `"8080"`then the `api_address` function will return a string`"http://127.0.0.1:8080"`. 
## FunctionDef webui_address
**webui_address**: The function of this function is to get the address of the web user interface (UI) server. 

****Arguments: This function does not accept any arguments. 

**Code Description**: The `webui_address`function is responsible for reading the hostname and port number of the Web UI server from the configuration file and formatting them into a full URL address. First, the function`configs.server_config` imports a `WEBUI_SERVER`dictionary from the module, which contains`host` `port`two keywords, and the hostname and port number of the server, respectively. The function then `f"http://{host}:{port}"`combines these two values into a full URL address by formatting the string and returns that address. 

In a project, `webui_address`a function is called by`startup.py` a function in a module`dump_server_info`. In the `dump_server_info`function, `webui_address`the return value is used to print the address information of the Web UI server, which is useful for verifying the configuration and troubleshooting after the server is started. In particular`args.webui`, if the parameter is true, it indicates that the user wants to obtain the address information of the Web UI server, and the address obtained through the function will be printed`webui_address`. 

**Note**: When using this function, you need to make sure that`configs.server_config` the dictionary in the module `WEBUI_SERVER`is properly configured, including valid`host` and `port`values, otherwise the function will return an invalid address. 

**Example output**: Assuming `WEBUI_SERVER`that in the dictionary`host` is ,`localhost` `port`is `8080`, then the return value of the function will be`"http://localhost:8080"`. This return value can be used directly in a web browser to access the web UI server. 
## FunctionDef get_prompt_template(type, name)
**get_prompt_template**: This function is used to load template content of the specified type and name from the configuration. 

**Parameters**:
- type: String, specifies the type of the template, and the optional values include "llm_chat", "agent_chat", "knowledge_base_chat", and "search_engine_chat", which represent different chat modes or functions.
- name: String, which specifies the name of the template and is used to further determine the template from the specified type.

**Code Description**:
`get_prompt_template`The function `configs`first imports `prompt_config`the configuration from the module and then reloads it with`importlib.reload` the method `prompt_config`to ensure that the most up-to-date configuration information is obtained. The function`type` `prompt_config.PROMPT_TEMPLATES`indexes the corresponding type of template dictionary through parameters, and then uses the parameters to`name` obtain the specific template content from the dictionary. If the specified`name` does not exist in the dictionary, it is returned`None`. 

In a project, `get_prompt_template`functions are called in multiple places to support template loading requirements in different scenarios. For example, the `server/api.py/mount_app_routes/get_server_prompt_template`function is used to dynamically load and return the corresponding template content based on the type and name parameters provided in the API request. In and `server/chat/agent_chat.py/agent_chat/agent_chat_iterator`other similar chat processing functions, it is used to load prompt templates in a specific chat mode to construct the content of the conversation that interacts with the user. 

**Note**:
- When using `get_prompt_template`functions, you need to make sure that`type` `name`the values of the sum parameters are correct and that the corresponding templates have been`prompt_config` defined in . 
- Because the function relies on an external configuration file`prompt_config`, you may need to restart the service or dynamically reload the configuration after modifying the configuration file to ensure that the changes take effect. 

**Example output**:
`prompt_config.PROMPT_TEMPLATES`The following is included in the hypothesis:
```python
PROMPT_TEMPLATES = {
    "llm_chat": {
        "default": "Hello, how can I help you?"
    }
}
```
The call `get_prompt_template(type="llm_chat", name="default")`will return a string`"Hello, how can I help you?"`.
## FunctionDef set_httpx_config(timeout, proxy)
**set_httpx_config**: This function is used to set the default timeout time and proxy configuration for the httpx library. 

**Parameters**:
- timeout: floating-point number, specifies the default timeout period for httpx requests. If not provided, the HTTPX_DEFAULT_TIMEOUT is used as the default.
- Proxy: Can be a string or dictionary type that specifies proxy settings for httpx requests. If it is not provided, the proxy is not used.

**Code Description**:
This function mainly does the following:
1. Modify the default timeout configuration of the httpx library, including connection timeout, read timeout, and write timeout, and set them to`timeout` the values specified by the function parameters. 
2. Depending on `proxy`the type of parameter (string or dictionary), a system-level proxy is set up at the process scope. If`proxy` it is a string, set the same proxy address for the http, https, and all protocols. If `proxy`it is a dictionary, set the proxy address of the corresponding protocol based on the keys (http, https, all) in the dictionary. 
3. Update the settings in the system environment variable`NO_PROXY` to ensure that certain host addresses (such as localhost and 127.0.0.1) and`fschat_controller_address` addresses obtained through the ,`fschat_model_worker_address` and `fschat_openai_api_address`functions do not use proxies. 
4. Rewriting the `urllib.request.getproxies`function to return the currently configured proxy settings ensures that the above proxy configuration is also applied when using the urllib library for network requests. 

**Note**:
- Before using this function, make sure that the variables are set correctly`HTTPX_DEFAULT_TIMEOUT` so that they can be used when you don't specify a timeout period. 
- Proxy settings are especially important in environments that require proxy access to external network resources, but note that for access to local or intranet resources, direct connectivity should be`NO_PROXY` ensured via environment variables to avoid unnecessary proxy delays. 
- This function is called by multiple modules, including controllers, model workers, OpenAI API services, and API servers, to uniformly configure the behavior of network requests when the project starts.

**Example output**:
This function has no return value, and its main purpose is to configure the environment and the httpx library, so it does not have a direct output example. 
### FunctionDef _get_proxies
**_get_proxies**: The function of this function is to get the proxy settings. 

****Arguments: This function has no arguments. 

**Code Description**: `_get_proxies` A function is a simple functional function designed to get proxy settings from the scope in which it resides. In the current code snippet, the`_get_proxies` function returns a variable named  directly `proxies` . This means that the actual effect of this function and the return value are entirely dependent on the `proxies` current value or state of the variable. Since the code snippet does not provide `proxies` the definition or initialization process for the variable, we cannot directly determine `proxies` the exact content or format of the variable. Typically, a proxy setup is used to configure network requests to go through a specific proxy server for purposes such as network anonymity, bypassing geo-restrictions, or improving request efficiency. 

**Note**: When using this function, you need to make sure that the variable has been correctly defined and initialized in the scope where the function is called `proxies` . `proxies` The variable should be a dictionary type that contains the details of the proxy configuration, such as the address and port of the proxy server. If `proxies` the  variable is not defined or initialized correctly, calling this function may result in an error or exception. 

**Example output**: Assuming  that is `proxies` defined as  , `{"http": "http://10.10.1.10:3128", "https": "https://10.10.1.11:1080"}`then calling  the `_get_proxies` function will return a dictionary like this:
```python
{
    "http": "http://10.10.1.10:3128",
    "https": "https://10.10.1.11:1080"
}
```
This return value shows a typical proxy configuration with a setup to use different proxy servers for HTTP and HTTPS requests.
***
## FunctionDef detect_device
**detect_device**: The function of this function is to detect and return the types of devices that are currently available for the environment. 

****Arguments: This function has no arguments. 

**Code Description**: The `detect_device` function first tries to import `torch` the library, and then uses the `torch.cuda.is_available()` method to check if a CUDA device (usually an NVIDIA GPU) is available. If available, the function returns a string`"cuda"`. If the CUDA device is not available, the function then checks to see if it can be accelerated using Apple's Metal Performance Shaders (MPS), which is achieved via `torch.backends.mps.is_available()` the method  . If MPS is available, the function returns a string`"mps"`. If neither of the above two acceleration methods is available, the function will return `"cpu"`, indicating that the calculation can only be performed using the central processing unit. This function ensures robustness by catching all exceptions to ensure that at least a minimum of is returned in any case`"cpu"`. 

In a project,`detect_device` functions are called by `llm_device`  and `embedding_device` function. These two functions are used to determine the devices used for language models (LLMs) and embedded computing. They try to specify a compute device by passing in device parameters, and if the passed device parameters are not `"cuda"` either,`"mps"` or  if no `"cpu"` device parameters are provided, they will call  the `detect_device` function to automatically detect and select an available device. This design enables users to automatically select the optimal computing device without manual configuration in different hardware environments, improving the versatility and ease of use of the code. 

**Note**: When using this function, you need to make sure that  the `torch` library is installed correctly, and depending on your hardware configuration (whether you have an NVIDIA GPU or Apple hardware that supports MPS),`torch` the version of the library should support CUDA or MPS. 

**Example output**: 
- If an NVIDIA GPU is detected, the function will return `"cuda"`. 
- If an MPS-enabled device is detected, the function will return `"mps"`. 
- If neither of the above is available, the function will return `"cpu"`. 
## FunctionDef llm_device(device)
**llm_device**: The function of this function is to determine and return the type of device used for Language model calculations. 

**Parameters**:
- `device`: A string parameter that specifies a computing device. It defaults to `None` . 

**Code Description**: The `llm_device` function first checks whether the incoming `device` argument has been specified. If not specified (i.e., , ), `None`the global variable is used `LLM_DEVICE` as the device type. Next, the function checks `device` if `"cuda"` is one of , `"mps"` or `"cpu"` . If not, the function will call  the `detect_device` function to automatically detect the types of devices available in the current environment. `detect_device` Functions can detect CUDA devices (such as NVIDIA GPUs), Apple's Metal Performance Shaders (MPS) acceleration or fallback to the CPU, ensuring that a usable computing device is found in any environment. Ultimately, the`llm_device` function returns the determined device type. 

**Note**:
- Before calling this function, make sure that you have installed a CUDA or MPS-enabled library version according to your hardware configuration `torch` so that the device type is detected correctly. 
- This function relies on a global variable `LLM_DEVICE`to ensure that it is set correctly before use. 
- When a device is automatically detected, if neither the CUDA device nor the MPS device is detected, the function will use the CPU by default.

**Example output**:
- If is specified `device` and `"cuda"` the environment supports it, the function will return `"cuda"` . 
- If not specified and `device`the auto-detect result is available for the MPS device, the function returns `"mps"` . 
- If neither `device` CUDA nor MPS is specified, the function will return `"cpu"` . 

The use cases of this function in a project include determining the type of computing device in the model's working configuration, and displaying the currently used computing device in the server startup information. This ensures that the system automatically selects the optimal computing device for different hardware environments, thus increasing the versatility and ease of use of the code.
## FunctionDef embedding_device(device)
**embedding_device**: The function of this function is to determine and return the type of device used to embed the calculation. 

**Parameters**:
- device: A string parameter that specifies a computing device. The default value is None.

**Code Description**: The `embedding_device` function first checks whether the incoming `device` argument has been specified. If it is not specified (i.e. None), the `EMBEDDING_DEVICE` value of the global variable is used. Next, the function checks `device` if `"cuda"` is one of , `"mps"` or `"cpu"` . If not, or `EMBEDDING_DEVICE` if a valid device type is not provided, the function will call  the `detect_device` function to automatically detect the device types available in the current environment. `detect_device` Functions can detect CUDA devices (typically NVIDIA GPUs), Apple's Metal Performance Shaders (MPS) acceleration, or fallback to using the CPU. Ultimately, the`embedding_device` function returns the determined device type. 

**Note**: When using this function, you need to ensure that the relevant hardware and software (such as NVIDIA GPUs or Apple hardware that supports MPS, as well as the corresponding supporting libraries) are ready. In addition,`EMBEDDING_DEVICE` global variables need to be set correctly outside of the function in order to provide default values when the device type is not explicitly specified. 

**Example output**: 
- If there is an NVIDIA GPU available in the environment, and the incoming `device` parameter is `"cuda"` or unspecified and `EMBEDDING_DEVICE` is , `"cuda"`the function will return `"cuda"` . 
- If the environment supports MPS acceleration and the incoming `device` parameter is `"mps"` or unspecified and `EMBEDDING_DEVICE` is , `"mps"`the function will return `"mps"` . 
- If neither of the above two cases is satisfied, the function will return `"cpu"`, indicating that the CPU is used for the calculation. 
## FunctionDef run_in_thread_pool(func, params)
**run_in_thread_pool**: The function of this function is to run tasks in batches in the thread pool and return the results of the run in the form of a generator. 

**Parameters**:
- `func`: A function that needs to be executed in the thread pool that should accept a keyword argument.
- `params`: A list of dictionaries, each representing `func`a keyword argument passed to. The default is an empty list. 

**Code Description**:
`run_in_thread_pool`Functions are mainly used to execute multiple tasks in a concurrent environment to improve the execution efficiency of a program. It accepts a callable object`func` and a list of arguments`params`. `params`Each element in is a dictionary that represents the keyword argument that needs to be passed to`func`. Internally, the function first creates a thread pool`ThreadPoolExecutor`, then traverses the `params`list to create a thread task for each parameter dictionary, and executes it using`pool.submit(func, **kwargs)` the commit to the thread pool. Here `**kwargs`is the keyword argument expansion syntax in Python, which is used to expand the dictionary into keyword arguments. Once all tasks are submitted, the function uses methods`as_completed` to wait for all tasks to complete and `yield`returns the results of each task via a generator statement. 

In the project, `run_in_thread_pool`it is used in different scenarios, such as file parsing, file saving and other operations, which need to process multiple files or data, and each operation can be executed independently and in parallel, thus greatly improving the processing efficiency. For example, in a`_parse_files_in_thread` function, it is used to concurrently save the uploaded file to a specified directory and process the file contents; In a`_save_files_in_thread` function, it is used to concurrently save files to a knowledge base directory; In a `files2docs_in_thread`function, it is used to concurrently convert a disk file into a document object. These call scenarios show that `run_in_thread_pool`multi-threading can be effectively used to improve the execution efficiency of programs when dealing with IO-intensive or CPU-intensive tasks. 

**Note**:
- When used`run_in_thread_pool`, you need to ensure that the operations you pass to`func` are thread-safe to avoid data races or other concurrency issues. 
- The task function `func`should be designed to accept keyword arguments so that it`params` matches correctly with the dictionary in . 
- Since `run_in_thread_pool`a generator is returned, calling this function requires iteration to get the results of all tasks. 
## FunctionDef get_httpx_client(use_async, proxies, timeout)
**get_httpx_client**: The function of this function is to obtain the configured httpx client instance and use it to execute HTTP requests. 

**Parameters**:
- `use_async`: Boolean type, defaults`False`. Specifies whether the returned httpx client supports asynchronous operations. 
- `proxies`: String or dictionary type, defaults`None`. Used to set up proxies. 
- `timeout`: floating-point number, defaults`HTTPX_DEFAULT_TIMEOUT`. Set the request timeout period. 
- `**kwargs`: Receives additional keyword arguments that will be passed directly to the httpx client instance.

**Code Description**:
This function first defines a default proxy configuration that bypasses proxy settings for local addresses. It then takes a series of specific addresses (such as the Fastchat controller address, model worker address, and OpenAI API address) and adds these addresses to the proxy configuration, ensuring that requests from those addresses don't go through the proxy. Next, the function reads the proxy settings from the system environment variables and incorporates them into the default proxy configuration. If the user provides`proxies` the parameters, the function incorporates the user-specified proxy settings into the default proxy configuration. 

In addition, functions can be used to `use_async`return synchronous or asynchronous httpx client instances via parameter selection. When constructing an httpx client instance, the timeout, proxy configuration, and any additional keyword parameters are passed to the client constructor. 

If verbose logging is enabled`log_verbose``True`, the function logs the configuration parameters of the client instance. 

**Note**:
- When using this function, you need to pay attention to `proxies`the format of the parameter, which can be a string or a dictionary. If it is a string, it will be interpreted as the proxy address of all requests; In the case of dictionaries, different proxies can be specified for different protocols. 
- Make sure that the proxy settings in the environment variable are correct, otherwise requests for the httpx client instance may be affected.
- When an asynchronous HTTP request needs to be executed, the parameter should be`use_async` set to `True`get an httpx client instance that supports asynchronous operations. 

**Example output**:
Since this function returns an instance of an httpx client, the output example depends on how the instance is used to make HTTP requests. For example, if you use a synchronous client to make a GET request, you might use something like this:
```python
with get_httpx_client() as client:
    response = client.get('https://www.example.com')
    print(response.text)
```
If you are using an asynchronous client to make a GET request, you might use something like this:
```python
async with get_httpx_client(use_async=True) as client:
    response = await client.get('https://www.example.com')
    print(response.text)
```
## FunctionDef get_server_configs
**get_server_configs**: The function of this function is to get the configuration information of the server and return it in dictionary form. 

****Arguments: This function does not accept any arguments. 

**Code description**: `get_server_configs` The function first imports the configuration items of the server from multiple configuration files, including knowledge base, search engine, vector search type, tile size, overlap size, score threshold, top K value of vector search, top K value of search engine, Chinese title enhancement flag, text splitter dictionary, text splitter name, large language model, history length, temperature parameter, and prompt template. In addition, the function also calls and`fschat_controller_address``fschat_openai_api_address` `api_address`three functions to obtain the Fastchat controller address, the FastChat OpenAI API address, and the API server address, respectively, and store these address information in a dictionary called`_custom` Dictionary. Finally, by merging`locals()` the local variables and `_custom`dictionaries, the function constructs and returns a dictionary containing all the configuration information. It is important to note that the function excludes`_` local variables that begin with the dictionary when returning the dictionary. 

**Note**: When using this function, you need to make sure that the configuration items in the relevant configuration file are set correctly. In addition, because the configuration information returned by the function may contain sensitive data, such as API addresses, etc., caution should be exercised when providing this information to the frontend or other services. 

**Example output**:
```python
{
    "DEFAULT_KNOWLEDGE_BASE": "example_kb",
    "DEFAULT_SEARCH_ENGINE": "google",
    "DEFAULT_VS_TYPE": "example_vs_type",
    "CHUNK_SIZE": 512,
    "OVERLAP_SIZE": 50,
    "SCORE_THRESHOLD": 0.5,
    "VECTOR_SEARCH_TOP_K": 10,
    "SEARCH_ENGINE_TOP_K": 5,
    "ZH_TITLE_ENHANCE": True,
    "TEXT_SPLITTER_NAME": "example_splitter",
    "LLM_MODELS": ["model1", "model2"],
    "HISTORY_LEN": 10,
    "TEMPERATURE": 0.7,
    "PROMPT_TEMPLATES": {"template1": "Example template."},
    "controller_address": "http://127.0.0.1:8080",
    "openai_api_address": "http://127.0.0.1:5000/v1",
    "api_address": "http://127.0.0.1:8080"
}
```
This example shows a dictionary of configuration information that a function might return, including various default configuration items, model parameters, prompt templates, and three key API addresses. What is actually returned will vary depending on the settings in the project profile.
## FunctionDef list_online_embed_models
**list_online_embed_models**: The function of this function is to list the names of all online models that support embedding functionality. 

****Arguments: This function has no arguments. 

**Code Description**: `list_online_embed_models` The function first `list_config_llm_models` obtains information about all configured online large language models (LLMs) from the function. It then iterates through the configuration information of these models, checking whether each model has a specified provider and whether the corresponding class for that provider exists in `model_workers` . If so, further check if the class supports embedding (by calling `can_embedding` the method). Only when these conditions are met will the name of the model be added to the return list. This means that the returned list contains models that are online models that can be embedded. 

**Note**: When using this function, you need to make sure that  the `model_workers` appropriate model provider classes have been defined in and that they implement `can_embedding` methods. In addition, because this function relies on `list_config_llm_models` the configuration information provided by the function, you need to ensure that the relevant configuration information is accurate and up-to-date. 

**Example output**:
```python
["llm_model_online_1", "llm_model_online_2"]
```
This output example shows a list of online model names that a function might return, and what will actually be returned will vary depending on the configured online models and whether they support embedding capabilities.

In a project,`list_online_embed_models` functions are called in several places, including but not limited to `embed_texts` the  and `aembed_texts` functions, which are used to handle vectorization operations on text. In addition, it is called by `load_kb_embeddings` the  method, which is used to load the embedding vector of the knowledge base. This shows that `list_online_embed_models` functions play an important role in the project, helping other components determine which online models can be used for embedding operations. 
## FunctionDef load_local_embeddings(model, device)
**load_local_embeddings**: The function of this function is to load and return an embedding vector object for the specified model from the cache. 

**Parameters**:
- `model`: String type, specifying the name of the embedding model to be loaded. If not provided, the default embedding model specified in the configuration file will be used.
- `device`: String type, specifying the computing device. If it is not provided, the `embedding_device`appropriate device will be automatically detected and selected via the function. 

**Code Description**: 
`load_local_embeddings`The function first checks if `model`the arguments are provided, and if not, uses the default embedding model name in the configuration file. The function then calls`embeddings_pool` the `load_embeddings`method, passing in the model name and device type, to load and return the embedding vector object. This process involves retrieving the embedding vector object from the cache, and if it does not exist in the cache, creating and loading a new embedding vector object. This loading process is thread-safe, avoiding race conditions in a multi-threaded environment. 

**Note**:
- When using this function, you need to make sure that the `model`embedding model corresponding to the parameters is configured correctly and that the supported device types (such as CUDA, MPS, or CPU) match the runtime environment. 
- This function relies on `embedding_device`a function to automatically detect or specify a computing device, so you need to ensure that the relevant hardware and software environments are ready. 
- When using this function in a multi-threaded environment, you can call it with confidence because of the thread-safe mechanisms implemented internally.

**Example output**: 
The call `load_local_embeddings(model="text-embedding-ada-002", device="cuda")`may return an embedding vector object that has been loaded with the specified OpenAI embedding model, ready to be computed on the CUDA device. 

In a project, `load_local_embeddings`functions are called in several places, including but not limited to`embed_texts` and `aembed_texts`functions, which are used to vectorize text; and the `ESKBService`vector search feature used to load the embedding model to support Elasticsearch during the initialization of the class. In addition, it is used by`worker` functions to add, search, and delete text in the Faiss cache. These invocation scenarios show that functions `load_local_embeddings`are the core function of handling local embedding vector loading, and support various application scenarios such as text vectorization and text retrieval. 
## FunctionDef get_temp_dir(id)
**get_temp_dir**: This function is used to create a temporary directory and return the path and folder name of the directory. 

**Parameters**:
- **id**: Optional, string type. If provided, the function will attempt to create or find a subdirectory corresponding to the underlying temporary directory. 

**Code Description**:
`get_temp_dir` The function first imports the path () to the underlying temporary directory from the configuration file`BASE_TEMP_DIR`. If a parameter is provided during the call`id`, and the `id`corresponding directory already exists in the basic temporary directory, the function will directly return the path and name of the directory without creating a new temporary directory. If no parameters are provided`id`, or the `id`corresponding directory does not exist, the function will use `tempfile.mkdtemp`the method to create a new temporary directory in the basic temporary directory and return the path and name of the new directory. 

In a project, `get_temp_dir`functions are `upload_temp_docs`called by functions to create or find a temporary directory to store files before they are uploaded and processed. In this process, if the previous temporary directory ID() is provided`prev_id`, the directory will be attempted to be reused, otherwise a new temporary directory will be created. This can effectively manage temporary files, avoid the repeated creation of unnecessary temporary directories, and also facilitate subsequent file processing and vectorization operations. 

**Note**:
- If you are using this function in a multi-threaded or multi-process environment, ensure that you have an appropriate synchronization mechanism for operations on temporary directories, such as reading and writing files, to avoid data contention or file corruption.
- The cleanup of temporary directories needs to be managed by the caller, and the function will not automatically delete the created temporary directories.

**Example output**:
The call `get_temp_dir()`might return a tuple of the form of:
- (`"/tmp/base_temp_dir/abc123"`, `"abc123"`), where `"/tmp/base_temp_dir/abc123"`is the full path to the temporary directory and`"abc123"` is the name of the directory. 
