## FunctionDef agent_chat(query, history, stream, model_name, temperature, max_tokens, prompt_name)
**agent_chat**: This function is used to handle asynchronous chat conversations with agents. 

**Parameters**:
- `query`: Query string entered by the user, required parameter.
- `history`: A list of historical conversations, each element of which is an`History` object. 
- `stream`: Whether to return data as a streaming output, defaults`False`. 
- `model_name`: The name of the LLM model used, which defaults to the`LLM_MODELS` first model in the list. 
- `temperature`: LLM sampling temperature, used to adjust the randomness of the generated text, the default value is determined by`TEMPERATURE` a constant, and the value can range from 0.0 to 1.0. 
- `max_tokens`: Limit the number of tokens generated by LLMs, which is the `None`maximum value of the model that can be used by default. 
- `prompt_name`: The name of the prompt template used, default is "default".

**Code Description**:
`agent_chat`A function is an asynchronous function that is primarily responsible for handling the user's chat conversation with an agent. It first converts the incoming list of historical conversations`history` into `History`a list of objects. Then, an asynchronous iterator is defined`agent_chat_iterator` to generate chat responses. In , `agent_chat_iterator`based on the parameters and configurations passed in, the corresponding LLM models and tools are initialized, historical conversation recordings are processed, and replies are generated based on the user's query. 

If `stream`the parameter is set to , `True`the function returns data as a streaming output, which is suitable for scenarios where the chat content needs to be updated in real time. In streaming output mode, the function generates different JSON data blocks based on different states (such as tool call start, completion, error, etc.), and`yield` returns them to the caller asynchronously through statements. 

In non-streaming output mode, the function collects all generated chat replies and returns them in a JSON format in the end.

**Note**:
- When using `agent_chat`functions, you need to make sure that the parameters you pass in`history` are formatted correctly, i.e., each element should be an`History` object or a data structure that can be converted into`History` an object. 
- `stream`The setting of parameters will affect the return mode of the function, and the appropriate mode can be selected according to the actual application scenario.
- Functions rely on configured LLM models and prompt templates to ensure that these dependencies are properly configured before being called.

**Example output**:
In non-streaming output mode, assuming that the user's query gets a series of chat responses, the function may return JSON data in the following format:
```json
{
  "answer": "这是聊天过程中生成的回复文本。",
  "final_answer": "这是最终的回复文本。"
}
```
In streaming output mode, the function returns data block by block, and each piece of data might look like this:
```json
{
  "tools": [
    "工具名称: 天气查询",
    "工具状态: 调用成功",
    "工具输入: 北京今天天气",
    "工具输出: 北京今天多云，10-14摄氏度"
  ]
}
```
Or when you get a final reply:
```json
{
  "final_answer": "这是最终的回复文本。"
}
```
### FunctionDef agent_chat_iterator(query, history, model_name, prompt_name)
**agent_chat_iterator**: The function of this function is to iteratively generate a response to an agent chat asynchronously. 

**Parameters**:
- `query`: String type, user's query or input.
- `history`: An optional `List[History]`type that represents conversation history. 
- `model_name`: String type, which defaults to the `LLM_MODELS`first model in the list and is used to specify the language model to use. 
- `prompt_name`: String type, which specifies the name of the prompt template.

**Code Description**:
`agent_chat_iterator`A function is an asynchronous generator that handles the logic of an agent's chat. First, the function`max_tokens` checks if it is an integer and less than or equal to 0, and if it is, sets it to`None`. Next, `get_ChatOpenAI`a chat model instance is initialized with a function, and `get_kb_details`a list of knowledge bases is obtained through the function to store it in the database of the model container. If it `Agent_MODEL`exists, another instance of the chat model is initialized with that model and stored in the model container; Otherwise, use the previously initialized model. 

The function uses `get_prompt_template`the function to obtain the specified prompt template and uses the`CustomPromptTemplate` class to create a custom prompt template instance. Then, `CustomOutputParser`create an output parser instance using the class, and depending on the model name, decide to`initialize_glm3_agent` initialize the GLM3 proxy executor with a function, or use`LLMSingleActionAgent` and `AgentExecutor`create a proxy executor. 

In an asynchronous loop, the function attempts to create a task, wraps `wrap_done`the call of the proxy executor with the function, and is notified via a callback when it is completed. If parameters are set`stream`, the function asynchronously iterates over the output of the callback processor and generates different response data depending on the state, and finally produces the output in JSON format. If no parameters are set`stream`, all output data is collected and a JSON object containing the answer and the final answer is generated at the end. 

**Note**:
- When using this function, you need to make sure that the `history`parameters provided are properly formatted and that each history item should be an`History` instance of the class. 
- `model_name`and `prompt_name`parameters, you should choose the appropriate model and prompt template according to the actual needs. 
- There are several asynchronous operations and custom classes used inside the function, such as`CustomAsyncIteratorCallbackHandler` , `CustomPromptTemplate`and `CustomOutputParser`, and you need to ensure that these components are implemented and configured correctly. 
- This function is designed to enable real-time or asynchronous chat interactions with the front-end, so when integrating into a chat system, you should consider its asynchronous nature and how it handles external callbacks.
***
