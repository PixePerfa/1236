## FunctionDef knowledge_base_chat(query, knowledge_base_name, top_k, score_threshold, history, stream, model_name, temperature, max_tokens, prompt_name, request)
**knowledge_base_chat**: This function is used to handle the user's interactive conversation with the knowledge base. 

**Parameters**:
- `query`: The user's input query, of type String.
- `knowledge_base_name`: The name of the knowledge base, of type String.
- `top_k`: A matching vector number, of type an integer.
- `score_threshold`: The relevance threshold of the knowledge base matches the value range from 0 to 1, and the type is floating-point.
- `history`: A list of historical conversations, each element of which is an`History` object. 
- `stream`: Whether to output as a stream, type Boolean.
- `model_name`: The name of the LLM model, of type as a string.
- `temperature`: The LLM sampling temperature, of type floating-point number.
- `max_tokens`: Limit the number of tokens generated by LLMs, and the type is Integer or None.
- `prompt_name`: The name of the prompt template used, of type String.
- `request`: The current request object, of type`Request`. 

**Code Description**:
The function first `KBServiceFactory.get_service_by_name`obtains the knowledge base service instance with the specified name through the method. If no corresponding knowledge base is found, a 404 status code response is returned. The incoming historical conversation data is then converted into`History` a list of objects. Next, an asynchronous generator is defined `knowledge_base_chat_iterator`to handle the logic of knowledge base queries and LLM model generation of answers. In this generator, the values are first adjusted according `max_tokens`to the conditions, then an instance of the LLM model is created, and a document search is performed. If reranker is enabled, the search results are reordered. Build context based on search results and generate input hints for LLM models. Finally, the LLM model is used to generate the responses, and depending on`stream` the parameters, decide whether to output all the results in a streaming or all at once. 

**Note**:
- When calling this function, you need to make sure that the incoming knowledge base name already exists in the system, otherwise a 404 error will be returned.
- `history`parameter allows an empty list to be passed in, indicating that there are no historical dialogs.
- `stream`The parameter controls the output mode, and when set to True, will stream the answers and document information; Otherwise, everything will be returned at once.
- There are multiple asynchronous operations used inside the function, so keywords are needed when called`await`. 

**Example output**:
`knowledge_base_chat`An example of a JSON format that might be returned by calling a function:
```json
{
  "answer": "这是根据您的查询生成的回答。",
  "docs": [
    "出处 [1] [文档名称](文档链接) \n\n文档内容\n\n",
    "<span style='color:red'>未找到相关文档,该回答为大模型自身能力解答！</span>"
  ]
}
```
If streaming output is enabled, each generated response fragment and document information will be sent individually as a separate JSON object.
### FunctionDef knowledge_base_chat_iterator(query, top_k, history, model_name, prompt_name)
**knowledge_base_chat_iterator**: The function of this function is to iteratively generate knowledgebase-based chat answers asynchronously. 

**Parameters**:
- `query`: String type, the content of the user's query.
- `top_k`: Integer, specifying the number of most relevant documents to be returned.
- `history`: An optional list of histories, each of which is an`History` object. 
- `model_name`: String type, `model_name`defaults, specifies the name of the model to use. 
- `prompt_name`: String type, defaults`prompt_name`, specifies the name of the prompt template to use. 

**Code Description**:
`knowledge_base_chat_iterator`A function is an asynchronous generator that handles a user's query request and asynchronously generates a chat answer based on the knowledge base content. First, the function`max_tokens` checks the validity and adjusts its value as needed. Next, `get_ChatOpenAI`a chat model instance is initialized with a function that is configured with a model name, temperature, maximum number of tokens, and callback functions. 

Functions run `run_in_threadpool`functions asynchronously `search_docs`to search for relevant documents in the knowledge base based on the user's query. If the reordering feature () is enabled`USE_RERANKER`, the search results are reordered using`LangchainReranker` classes to improve the relevance of the results. 

Based on the number of documents searched, the function selects the corresponding prompt template. If no relevant document is found, use the "empty" template; Otherwise, the specified template is used`prompt_name`. Then, convert the history and the user's query requests into a chat prompt template. 

Create `LLMChain`a chat chain with a class and `wrap_done`wrap the asynchronous task with a function so that the callback can be handled when the task is completed. The function also generates information about the source document, including the source and content of the document. 

Finally, depending on whether streaming () is enabled or not`stream`, the function generates the chat answer asynchronously in different ways. If streaming is enabled, use server-sent-events to send the token of the answer one by one; Otherwise, all tokens will be spliced and returned at once. 

**Note**:
- When using this function, you need to make sure that the provided`model_name` ones and `prompt_name`those in the system are configured and valid. 
- When you enable the reordering feature, you need to ensure that `LangchainReranker`the classes are configured correctly, including the model path and device type. 
- The asynchronous nature of functions requires the caller to`async` make calls with the and `await`keyword to ensure the correct execution of the asynchronous operation. 
- When processing a large number of query requests, reasonable configuration `top_k`and reordering of parameters can effectively improve processing efficiency and answer quality. 
***
