## FunctionDef completion(query, stream, echo, model_name, temperature, max_tokens, prompt_name)
**completion**: This function is used to process user input, generating and returning text that is completed by the LLM model. 

**Parameters**:
- `query`: Text entered by the user.
- `stream`: Whether or not to output the result in a stream.
- `echo`: Whether or not the input text is echoed in the output result.
- `model_name`: The name of the LLM model used.
- `temperature`: The sampling temperature of the LLM model, which is used to control the randomness of the generated text.
- `max_tokens`: Limit the number of tokens generated by the LLM model.
- `prompt_name`: The name of the prompt template used.

**Code Description**:
This function first defines an asynchronous generator `completion_iterator`that is responsible for the actual text generation logic. It `get_OpenAI`initializes an LLM model with a function and configures the model according to the provided parameters. It then uses`get_prompt_template` a function to get the specified prompt template and passes the user input`query` to the LLM model for processing. Depending on `stream`the value of the parameter, this function can return the generated tokens one by one in a streaming manner, or wait for all tokens to be generated and return the results at one time. Finally, using a`EventSourceResponse` wrapper `completion_iterator`generator, the results are returned in an HTTP response format that is appropriate for streaming output. 

In a project, `completion`the function`/other/completion` `server/api.py`is registered as a handler for a POST request in the file via a route. This indicates that it is designed to handle text completion requests from clients, and clients can obtain the completed text generated by the LLM model based on user input by sending a POST request to this route and providing the corresponding parameters in the request body. 

**Note**:
- Ensure that the `model_name`LLM model corresponding to the parameter is properly configured and available. 
- `temperature`The parameters should be between 0.0 and 1.0 to control the randomness of the generated text.
- If `max_tokens`it is set to negative or 0, there will be no limit on the number of tokens. 

**Example output**:
If `stream`the parameter is`False`, and the user enters "What's the weather like today", an example of a possible return value is:
```
"今天天气晴朗，适合外出。"
```
If the `stream`parameter is`True`, it is possible to return each word or phrase in the above text on a case-by-case basis. 
### FunctionDef completion_iterator(query, model_name, prompt_name, echo)
**completion_iterator**: The function of this function is to iteratively generate complete text based on a given query asynchronously. 

**Parameters**:
- `query`: String type, the user's query input.
- `model_name`: String type, which defaults to the first model in the LLM_MODELS list, specifies the name of the language model to use.
- `prompt_name`: String type, specifying the name of the prompt template to use.
- `echo`: Boolean type, indicating whether the input is echoed.

**Code Description**:
`completion_iterator`A function is an asynchronous generator that generates text based on the user's query input. First, the function checks if the`max_tokens` parameter is an integer and less than or equal to 0, and if it is, sets it to None. Next, `get_OpenAI`a configured OpenAI model instance is initialized through a function, which includes parameters such as model name, temperature, maximum number of tokens, list of callback functions, and whether to echo inputs. Then, use the`get_prompt_template` function to load the prompt template with the specified type and name, and`PromptTemplate.from_template` create an instance with the method`PromptTemplate`. After that, create an `LLMChain`instance that passes in the prompt template and language model as parameters. 

The function then creates an asynchronous task,`asyncio.create_task` wraps the method's call`chain.acall` with a method, and `wrap_done`associates it with a callback function through the function to notify you when the task is completed. Depending on`stream` the value of the variable, the function will generate text in different ways. If`stream` true, each `callback.aiter()`generated token is iterated asynchronously and the response is streamed using server-sent-events. If `stream`false, all generated tokens are accumulated into a string, and the entire answer is generated at once. 

Finally, the function waits for the previously created asynchronous task to complete, ensuring that all generated text has been processed.

**Note**:
- When using this function, you need to make sure that the `query`argument is correct, as it directly affects the content of the generated text. 
- `model_name`and `prompt_name`parameters, you should choose the appropriate model and prompt template according to your needs to get the best text generation effect. 
- When using the streaming feature, you should consider how the client handles the streaming data to ensure a user experience.
- This function depends on `get_OpenAI`functions such as and`get_prompt_template`, so you should make sure that the relevant configurations and templates are set up correctly before using them. 
***
